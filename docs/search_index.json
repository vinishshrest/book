[["ipw-and-aipw.html", "3 IPW and AIPW", " 3 IPW and AIPW The target is to estimate the average treatment effect (ATE): \\[\\begin{equation} \\label{eq:ATE} ATE = E[Y_i(1) - Y_i(0)] \\tag{3.1} \\end{equation}\\] Note that using the following two assumptions: \\(W_i \\perp \\{Y_i(0), \\; Y_i(1)\\}\\) (independence assumption) \\(Y_i(W) = Y_i\\) (SUTVA) the ATE estimate \\(\\hat{\\tau}\\) can be written as the difference-in-means estimator: \\[\\begin{equation} \\label{eq:ATE_estimator} \\hat{\\tau} = \\frac{1}{N_T} \\sum_{W_i = 1} Y_i - \\frac{1}{N_C} \\sum_{i \\in W_i = 0} Y_i \\end{equation}\\] where \\(N_T\\) and \\(N_C\\) are the number of treated and control units, respectively. In the previous lecture, we disscussed randomized control trial as an ideal approach to estimate ATE. In a randomized controlled trial each unit has an equal probability of receiving the treatment. This means the following: \\[\\begin{equation} P(W_i = 1 \\; | \\; Y_i(0), \\; Y_i(1), \\; n_T) = \\frac{n_T}{n}, \\; \\; i = \\{1, ...., n\\} \\tag{3.2} \\end{equation}\\] In equation (3.2), \\(n_T\\) refers to the number of units that receives the treatment.1 In an easy to understand set-up, if a researcher wants \\(P(W_i = 1) = 0.5\\) (unit is equally likely to be treated or untreated), a coin flip can feasibly be used as a mechanism to assign treatment.2 Although randomized controlled trials (RCTs) are often considered the gold standard in causal inference, they cannot always be used due to ethical, moral, and monetary reasons. Returning to the example we used in the previous chapter, it is not ethical to demarcate who can attend the tutoring session versus who cannot. In real-world scenarios, tutoring sessions are typically voluntary. Students who regularly attend these sessions may have different baseline (pre-treatment) characteristics compared to those who do not attend. These differences can introduce biases that complicate causal inference in observational studies. To proceed further in observational setting (without using RCTs), we require more knowledge about the treatment assignment. In other words, we need to understand which variables determine who attends the tutoring sessions. This information is crucial for identifying potential confounders and for applying methods that can help estimate causal effects in observational settings. In causal inference, confounders are variables that are associated with both the treatment and the outcome. They can introduce bias in the estimation of the causal effect of the treatment on the outcome by providing alternative explanations for any observed relationships. For example, say you are trying to evaluate the efficacy of a new drug on blood pressure level. If smokers are more likey to get treated and if they tend to have higher blood pressure to begin with, the treatment effects are likely to be understated. This brings us to the unconfoundedness assumption. Unconfoundedness: The treatment assignment is as good as random once we control for \\(X\\)s. \\[\\begin{equation} \\{W_i \\perp \\{Y_i(0), \\; Y_i(1)\\} | X_i \\} \\; for \\; all \\; x \\in \\chi. \\tag{3.3} \\end{equation}\\] As with the tutoring example, the independence assumption (discussed in the previous chapter) is highly unlikely to hold in observational settings. Let’s consider the following scenarios: Out of the ten states that are yet to expand Medicaid, eight fall in South. Medicaid expansion is not random. Cigarette taxes are higher in states with higher anti-smoking sentiments. Infrastructure development, such as construction of roads, schools, hopitals, are demand-driven. The list goes on .. However, if we manage to observe all the \\(X\\)s (covariates) that influence the treatment, we can invoke unconfoundedness for causal inference. Although it is generally recommended to assign half of the sample to the treatment group and the other half to the control group, this is not a strict requirement.↩︎ Of course, this is quicky going to be inefficient as the sample size increases. In general, treatment assignment is determinted by a statistical process via a software. For example, if a researcher wants about one-third of the sample treated then a bernoulli trial with the probability of success of 0.33 can be used.↩︎ "],["a-simple-example.html", "3.1 A simple example", " 3.1 A simple example Say, you are interested in evaluating the effect of tutoring program initiated following the first exam on grades at an introductory level course. For simplicity, the possible grades are A and B. However, students who received B on their first exam are more likely to attend the tutoring session. In other words, \\(P(W_i = 1 | Y_{iFE} = A) &lt; P(W_i = 1 | Y_{iFE} = B)\\) (\\(Y_{iFE}\\) is read as unit \\(i&#39;s\\) grade in the first exam). In this case, the treatment assignment is correlated with the past grade, which can predict the grade on the second exam. In other words, if you did well in the first exam, you are likely to perform well in the second exam and so on. Hence, using equation (2) to estimate effects of the tutoring program will result in biased estimate. Since we know that the probability of treatment is influenced by the grade on the first exam, we can estimate the conditional average treatment effect (CATE) and average them using weights to form an estimate of ATE. Let’s take a look at the data. # function to report grade breakdown by the first exam grade (A and B) grade_mat &lt;- function(grade_vec){ grade &lt;- matrix(0, nrow =2, ncol = 3) grade[, 1] &lt;- c(&quot;Treat&quot;, &quot;Control&quot;) grade[ ,2] &lt;- c(grade_vec[1], grade_vec[2]) grade[ ,3] &lt;- c(grade_vec[3], grade_vec[4]) return(grade) } # Y_iFS == A grade &lt;- grade_mat(c(5, 9, 2, 4)) colnames(grade) &lt;- c(&quot; &quot;, &quot;A (2nd Exam)&quot;, &quot;B (2nd Exam)&quot;) # Se grade %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) %&gt;% add_header_above(c(&quot;Table 1.&quot; = 1, &quot;Grade in the 2nd exam | 1st exam = A&quot; = 2)) Table 1. Grade in the 2nd exam | 1st exam = A A (2nd Exam) B (2nd Exam) Treat 5 2 Control 9 4 grade &lt;- grade_mat(c(15, 1, 5, 4)) colnames(grade) &lt;- c(&quot; &quot;, &quot;A (2nd Exam)&quot;, &quot;B (2nd Exam)&quot;) grade %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) %&gt;% add_header_above(c(&quot;Table 2.&quot; = 1, &quot;Grade in the 2nd exam | 1st exam = B&quot; = 2)) Table 2. Grade in the 2nd exam | 1st exam = B A (2nd Exam) B (2nd Exam) Treat 15 5 Control 1 4 \\(~\\) \\(~\\) Estimation \\(\\hat{\\tau}_{FE=A} = \\frac{5}{7} - \\frac{9}{13} = 2.1 \\; pp\\) \\(\\hat{\\tau}_{FE=B} = \\frac{15}{20} - \\frac{1}{5} = 55 \\; pp\\) \\(\\hat{\\tau}_{AGG} = \\frac{20}{45} \\hat{\\tau}_{FE=A} - \\frac{25}{45} \\hat{\\tau}_{FE=B} = 31.48 \\; pp\\). The first two are CATEs for the group that recived A and B in the first exam. The assumption is that once conditioned on the grade in the first exam, treatment (who attends vs. who doesn’t) is random. This allows valid estimation of within group causal effects, which are then averaged to form ATE using appropriate weights on the third line. This simple example using the discrete feature space (grade in the first exam can be A or B) provides intuition that if variables influencing the treatment assignment are observed then ATE estimate can be uncovered by taking weighted average of CATE estimates (these are also group-wise ATE).3 In this case, CATEs are different across the two sub-groups. Sometimes the core interest of analysis can be uncovering the heterogeneous treatment effects, which motivates estimation and inference on CATEs across two or more sub-groups.↩︎ "],["propensity-score.html", "3.2 Propensity score", " 3.2 Propensity score Previously we discussed the setting of a discrete feature in which case we estimate group-wise ATEs and use the weighted average to obtain an overall ATE estimate. When there are many features (covariates), this approach is prone to the curse of dimensionality.4 Moreover, if features are continuous, we won’t be able to estimate ATE at each value of \\(x \\in \\chi\\) due to lack of enough sample size. Instead of estimating group-wise ATE and averaging them, we would want to use a more indirect approach. This is when propensity score comes in. The implicit assumption is that we have collected enough features (discrete, continuous, interaction terms, higher degree polynomials) to back unconfoundedness. This again means that the treatment assignment is as good as random after controlling for \\(X_i\\). More formally, this us back to equation (3.3). But in actuality we are not interested in splitting groups to estimate group-wise treatment effects in the case when covariates are continuous and there are many characteristics determining the treatment assignment. Propensity score: \\(e(x)\\). The probability of being treated given a set of covariates \\(X\\)s. \\[\\begin{equation} e(x) = P(W_i = 1 | X_i = x) \\tag{3.4} \\end{equation}\\] The key property of the propensity score is that it balances units in the treatment and control groups. If unconfoundedness assumption holds, we can write the following: \\[\\begin{equation} W_i \\perp \\{Y_i(0), \\; Y_i(1)\\} | \\; e(X_i) \\tag{3.5} \\end{equation}\\] What equation (3.5) says is that instead of controlling for \\(X\\) one can control for the probability of treatment \\((e(X))\\) to establish the desired property that the treatment is as good as random. The propensity scores are mainly used for balancing purposes. One straight-forward implication of equation (3.5) is that if we partition observations into groups with similar propensity score then we can estimate group-wise treatment effects and aggregate them to form an estimate for ATE. This can be done using the propensity score stratification method. The argument here is that when units with similar propensity scores are compared, the covariates are approximately balanced, mimicking a randomized experiment. As the number of covariates increases the domain space shrinks quite rapidly making it infeasible to estimate ATE within the given domain due to thinning out data.↩︎ "],["estimation-of-propensity-score.html", "3.3 Estimation of propensity score", " 3.3 Estimation of propensity score Propensity scores can be estimated using various statistical or machine learning models. We will first estimate propensity score using a logistic regression model, where the treatment assignment \\(W\\) is regressed on the covariates \\(X\\). Next, we will estimate propensity score using random forest model built within the GRF framework in Athey et al.  Logistic Regression Using a linear regression framework to predict probabilities when the outcome is binary \\(\\{0, \\; 1\\}\\) falls short since the predicted values can go beyond 0 and 1. Many models contain values within the range of 0 and 1, which can be used to model a binary response. The logistic regression uses a logistic function given as: \\[\\begin{equation} p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p}} \\tag{3.6} \\end{equation}\\] It is easy to see that \\(lim_{a \\rightarrow - \\inf}[\\frac{e^a}{1+e^a}] = 0\\) and \\(lim_{a \\rightarrow \\inf}[\\frac{e^a}{1+e^a}] = 1\\). Equation @ref{eq:logit} can be transformed using the logit transformation given as: \\[\\begin{equation} g(X) = ln[\\frac{p(X)}{1-p(X)}] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p \\end{equation}\\] We want to fit a logistic regression in order to predict the probability. For now, we will use simulated data. # helper packages library(dplyr) # data wrangling library(ggplot2) # plots library(rsample) # data splitting library(tidyr) # for reshaping, pivot_wider # Modeling package library(caret) # for logistic regression modeling # Model interpretability library(vip) set.seed(194) # for replicability # Generate simulated Data n &lt;- 2000 # number of obsevations p &lt;- 10 # number of covariates X &lt;- matrix(rnorm(n * p), n, p) # data matrix true_effect &lt;- 2.56 W &lt;- rbinom(n, 1, 0.1 + 0.4 * (X[, 1] &gt; 0) + 0.2 * (X[, 2] &gt; 0)) prob &lt;- 0.1 + 0.4 * (X[, 1] &gt; 0) + 0.2 * (X[, 2] &gt; 0) # oracle propensity score Y &lt;- true_effect * W + X[, 2] + pmax(X[, 1], 0) + rnorm(n) #plot(X[, 1], X[, 2], col = as.factor(W)) dat &lt;- data.frame(cbind(W, Y, X)) colnames(dat) &lt;- c(&quot;W&quot;, &quot;Y&quot;, paste0(&quot;X&quot;, seq(1, 10))) dat &lt;- dat %&gt;% mutate(W = as.factor(W)) # create 70% training and 30% test data churn_split &lt;- initial_split(dat, prop = 0.7) dat_train &lt;- training(churn_split) dat_test &lt;- testing(churn_split) # dimension of training and testing data print(dim(dat_train)) ## [1] 1400 12 print(dim(dat_test)) ## [1] 600 12 # let&#39;s compare two different models # using X1 as the predictor cv_model1 &lt;- train( W ~ X1 + X2 + X3 + X4, data = dat_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # misses out on X1 cv_model2 &lt;- train( W ~ X2 + X3 + X4, data = dat_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # print the sample performance measures sum_performance &lt;- summary( resamples( list( model1 &lt;- cv_model1, model2 &lt;- cv_model2 ) ) ) sum_performance$statistics$Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Model1 0.6043165 0.6446429 0.6678571 0.6706292 0.6964286 0.7285714 0 ## Model2 0.5642857 0.5785714 0.5892392 0.5935621 0.6160714 0.6285714 0 # use the confusion matrix # predict class threshold &lt;- 0.5 pred_prob &lt;- predict(cv_model1, dat_train, type = &quot;prob&quot;) pred_class_manual &lt;- rep(0, 1400) pred_class_manual[pred_prob[, 2] &gt;= 0.5] &lt;- 1 pred_class &lt;- predict(cv_model1, dat_train) # print the confusion matrix confusionMatrix( data = relevel(pred_class, ref = &quot;1&quot;), # predictions reference = relevel(dat_train$W, ref = &quot;1&quot;) # reference or the true value ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 270 151 ## 0 299 680 ## ## Accuracy : 0.6786 ## 95% CI : (0.6534, 0.703) ## No Information Rate : 0.5936 ## P-Value [Acc &gt; NIR] : 3.164e-11 ## ## Kappa : 0.3053 ## ## Mcnemar&#39;s Test P-Value : 4.219e-12 ## ## Sensitivity : 0.4745 ## Specificity : 0.8183 ## Pos Pred Value : 0.6413 ## Neg Pred Value : 0.6946 ## Prevalence : 0.4064 ## Detection Rate : 0.1929 ## Detection Prevalence : 0.3007 ## Balanced Accuracy : 0.6464 ## ## &#39;Positive&#39; Class : 1 ## # if predict all yes still get an accuracy of 0.5936 table(dat_train$W) %&gt;% prop.table() ## ## 0 1 ## 0.5935714 0.4064286 Looking at the confusion matrix, the values on the downward diagonal ([1, 1] and [2, 2] in matrix) are correctly idenfified by the model, while the upward diagonal values ([2, 1] and [1, 2]) are incorrectly classified. If all of the observations were assigned the value of 0, the accuracy would still be 0.5936%. This is termed as the no information rate. The model performs quite well in predicting True Negatives (classify as 0, when the value is actually 0). However, it does not perform so well in classifying the True Positives – more than 50% of the positive cases are classified as negative. Next, two measures of importance are sensitivity and specificity. The sensitivity measure tracks the true positive rate from the model, while the specificity measure tracks the true negative rate. \\(sensitivity = \\frac{True \\; positives}{True \\; positives + False \\; negatives} = 0.8790\\). \\(specificity = \\frac{True \\; negatives}{True \\; negatives \\; + \\; False \\; positives} = \\frac{604}{604 + 85} = 0.8766\\). How are the observations classified? A threshold value is used to transform the raw prediction of probabilities into classification such that \\(P(Y_{i} &gt; p_{threshold})=1.\\) The implicit \\(p_{threshold}\\) used is 0.5. Varying the threshold from 0 to 1, one can calculate the relationship between the False Positive Rate (the prediction is positive when in actual the outcome is negative) and True Positive Rate at each threshold value. If the threshold value \\((p_{threshold})\\) is 1, then all observations are classified as 0, which means that the False Positive Rate is 0 but so is the True Positive Rate. Similarly, if the threshold is 0, then both True and False positive rates are 1. This gives the Receiver Operating Characteristic (ROC). library(ROCR) # compute probabilities m1_prob &lt;- predict(cv_model1, dat_train, type = &quot;prob&quot;)[, 2] m2_prob &lt;- predict(cv_model2, dat_train, type = &quot;prob&quot;)[, 2] # AUC metrics perf1 &lt;- prediction(m1_prob, dat_train$W) %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) perf2 &lt;- prediction(m2_prob, dat_train$W) %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) # plot ROC curves plot(perf1, col = &quot;red&quot;) plot(perf2, add = TRUE, col = &quot;green&quot;) legend(0.8, 0.2, legend = c(&quot;cv_model1&quot;, &quot;cv_model2&quot;), lty = c(1,1), col = c(&quot;red&quot;, &quot;green&quot;), cex = 0.6) Figure 3.1: ROC The figure above plots the ROC for two models that we tested using cross-validation. The cv_model2 produces a diagonal line, which means that this model is as good as a random guess. Next, cv_model1 performs a whole lot better since a large gains in True positive rate can be achieved with a relatively small increase in False positive rate at the start. The ROC curve pertaining to cv_model1 helps pick a threshold to balance the sensitivity (True Positive Rate) and specificity (1 - False Positive Rate). The histogram of the estimated propensity scores using the logistic regression is as: hist(pred_prob[, 2], main = &quot;Histogram of P(W=1|X) \\n using Logistic Regression&quot;, xlab = &quot;probabilities&quot;) Now, let’s take a look at the confusion matrix using the test data. pred_class_test &lt;- predict(cv_model1, dat_test) # print the confusion matrix this time for the test sample confusionMatrix( data = relevel(pred_class_test, ref = &quot;1&quot;), # classification from the prediction reference = relevel(dat_test$W, ref = &quot;1&quot;) # ground truth ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 94 69 ## 0 131 306 ## ## Accuracy : 0.6667 ## 95% CI : (0.6274, 0.7043) ## No Information Rate : 0.625 ## P-Value [Acc &gt; NIR] : 0.01882 ## ## Kappa : 0.2474 ## ## Mcnemar&#39;s Test P-Value : 1.608e-05 ## ## Sensitivity : 0.4178 ## Specificity : 0.8160 ## Pos Pred Value : 0.5767 ## Neg Pred Value : 0.7002 ## Prevalence : 0.3750 ## Detection Rate : 0.1567 ## Detection Prevalence : 0.2717 ## Balanced Accuracy : 0.6169 ## ## &#39;Positive&#39; Class : 1 ## The measures of accuracy, sensitivity, and specificity are similar for both the training and testing sample. "],["using-cross-fitting-to-predict-propensity-score.html", "3.4 Using cross-fitting to predict propensity score", " 3.4 Using cross-fitting to predict propensity score Here, we will be using 10-fold cross-folding to predict propensity score. fun_probit_predict &lt;- function(predictfold){ # @Arg predictfold: number of the fold to avoid for model traning # but used for prediction cv_model1 &lt;- train( W ~ X1 + X2 + X3 + X4, data = dat[-predictfold, ], method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) predict_logit &lt;- predict(cv_model1, dat[predictfold, ], type = &quot;prob&quot;) return(predict_logit[, 2]) } ############################## # # cross-fitting # ############################## k &lt;- 10 # number of folds len &lt;- nrow(dat) ind &lt;- sample(1:len, replace = FALSE, size = len) fold &lt;- cut(1:len, breaks = k, labels = FALSE) # create 10 folds fold &lt;- fold[ind] # randomly allocate the folds by ind # container to store the predicted values store &lt;- c() true_index &lt;- c() # do the cross-fitting and store for(i in 1:k){ # which(fold == i) is used as an index, if 8th observation receives the 1st fold for the first time, # then the 1st prediction value corresponds to the 8th obs store_new &lt;- fun_probit_predict(predictfold = which(fold == i)) store_new &lt;- as.numeric(as.character(store_new)) true_index_new &lt;- which(fold == i) store &lt;- c(store, store_new) true_index &lt;- c(true_index, true_index_new) } # create a dataframe with index that maps the predictions with the actual data store &lt;- data.frame(pscore = store, index = true_index) # sort by index store &lt;- store[order(store[, 2]), ] # propensity score dat &lt;- dat %&gt;% mutate(pscore = store$pscore) # histogram of propensity score hist(dat$pscore, main = &quot;propensity score \\n from cross-fitting&quot;) "],["propensity-score-stratification.html", "3.5 Propensity score stratification", " 3.5 Propensity score stratification Propensity scores are super important as they can be used in various different approaches to enchance the validity of causal inference in observational settings. These include but are not limited to inverse probability weighting, matching estimates, weight adjustments in regression (for better balancing), trimming, and propensity score stratification. These methods will be discussed in detail as we move on with the course. First, let’s take a look at propensity score stratification to get a gist of how propensity scores contribute in comparing treatment units with control units. The simple idea is given by the cliché that we want to compare oranges with oranges and not apples. To bring focus back into our context, it simply means that it is no good comparing a treated unit with an extremely high probability of receiving the treatment with a control unit with super low probability of receiving the treatment. But what if (yes, what if) we compare units with similar treatment probabilities? Let’s run a quick thought experiment. We run the logistic regression and estimate the propensity score. Say, we have two units, each from the treatment and control group, with the propensity score of 0.6. The assumption here is, conditional on the similar propensity score, the treatment assignment is random. This follows from the unconfoundedness assumption: \\(Y_i^{0}, \\; Y_i^{1} \\; \\perp \\; W_i \\; | X_i\\). Propensity score stratification divides the estimates of propensity scores into several segments and estimates the ATE within each segment. Finally, these segment-specific ATE estimates are averaged to obtain the overall estimate of ATE. Steps for ATE estimation using propensity score stratification Order observations according to their estimated propensity score. \\(\\hat{e}(X)_{i1}, \\; \\hat{e}(X)_{i2}, ... \\; \\hat{e}(X)_{iN}\\) Form \\(J\\) strata of equal size and take the simple difference in mean between the treated and control units within each strata. These are \\(\\hat{\\tau}_j\\) for \\(j = \\{1, \\; 2, \\; ..., \\; N\\}\\). Form the ATE, \\(\\hat{\\tau}_{Strat} = \\frac{1}{J} \\sum_{j = 1}^{J} \\hat{\\tau}_j\\) Here, \\(\\hat{\\tau}_{Strat}\\) is consistent for \\(\\tau\\), meaning that \\(\\hat{\\tau}_{Strat} \\rightarrow_p \\tau\\) given that \\(\\hat{e}(x)\\) is consistent for \\(e(x)\\) and the number of strata grows appropriately with \\(N\\). However, one needs to set the number of strata, which can be a bit ad-hoc. Demo of propensity score stratification # order data by the propensity score: low to high dat &lt;- dat[order(dat$pscore), ] # cut to form ventiles strata &lt;- cut(dat$pscore, breaks = quantile(dat$pscore, seq(0, 1, 0.05)), labels = 1:20, include.lowest = TRUE) dat &lt;- dat %&gt;% mutate(strata = strata) # compare across strata dat_sum &lt;- dat %&gt;% group_by(W, strata) %&gt;% summarize(mean_Y = mean(Y)) %&gt;% pivot_wider(names_from = W, values_from = mean_Y) ## `summarise()` has grouped output by &#39;W&#39;. You can override using the `.groups` argument. colnames(dat_sum) &lt;- c(&quot;strata&quot;, &quot;mean_control&quot;, &quot;mean_treat&quot;) dat_sum &lt;- dat_sum %&gt;% mutate(diff = mean_treat - mean_control) print(paste(&quot;ATE Estimation from propensity score stratification is: &quot;, mean(dat_sum$diff), sep = &quot;&quot;)) ## [1] &quot;ATE Estimation from propensity score stratification is: 2.56135397640408&quot; print(paste(&quot;raw difference is :&quot;, mean(dat$Y[dat$W == 1]) - mean(dat$Y[dat$W == 0]), sep = &quot;&quot;)) ## [1] &quot;raw difference is :3.10098842187519&quot; print(paste(&quot;And the true treatment effect is :&quot;, true_effect, sep = &quot;&quot;)) ## [1] &quot;And the true treatment effect is :2.56&quot; We see that the estimate from stratification gets closer to the true effect compared to the mean difference estimator. Looks like given that we know and observe what variables determine the treatment assignment, propensity score stratification approach performs well in estimating the ATE. "],["inverse-probability-weighting-ipw.html", "3.6 Inverse Probability Weighting (IPW)", " 3.6 Inverse Probability Weighting (IPW) A more natural way to exploit the condition of unconfoundedness is to weight observations by their propensity score, which is known as the inverse probability weighting. As before \\(\\hat{e}(x)\\) is defined as an estimated propensity score. \\[\\begin{equation} \\hat{\\tau}_{IPW} = \\frac{1}{N}\\sum_{i = 1}^{N} \\Bigg(\\frac{Y_i . W_i}{\\hat{e}(X_i)} - \\frac{Y_i . (1-W_i)}{1 - \\hat{e}(X_i)}\\Bigg) \\tag{3.7} \\end{equation}\\] Intuitively, observations with high propensity score within the treated group are weighted down, while observations with higher propensity score in the control group are weighted more. In this way, propensity score is used to balance the differences in covariates across the treatment and control groups. Note that the validity of \\(\\hat{\\tau}\\) still hinges on the unconfoundedness assumption. Any inference that you make is only good if your assumption holds. Limitation of IPW Estimate. One way to analyze the accuracy of \\(\\hat{\\tau}_{IPW}\\) is to compare it with the oracle IPW estimate, \\(\\hat{\\tau}_{IPW}^{*}\\). The oracle estimate is obtained from the known propensity score. Briefly, comparison between \\(\\hat{\\tau}_{IPW}^{*}\\) and \\(\\hat{\\tau}_{AGG}\\) suggests that the oracle IPW under-performs \\(\\hat{\\tau}_{AGG}\\). In other words, the variance of the oracle estimate is larger than that of \\(\\hat{\\tau}_{AGG}\\). Algorithmically, we can form score as: \\((\\frac{Y_i \\times W_i}{\\hat{e}(X_i)} - \\frac{Y_i \\times (1-W_i)}{1 - \\hat{e}(X_i)})\\) The mean of it results to \\(\\hat{\\tau}\\) and the standard error of the estimate is simply \\(\\frac{\\hat{\\sigma}_{score}}{\\sqrt{N}}\\). Estimating IPW. In the example below we will simulate a dataset where the treatment assignment is made to be correlated with the outcome. This means that the independence assumption does not hold. However, since this is a simulated data, we know exactly what covariates influence the treatment assignment. Hence, we can invoke the unconfoundedness assumption. We estimate the propensity score using random forest based on honest splitting. For this, we use GRF package from . Note that \\(e(x)\\) is estimated via cross-fitting. The data is divided into \\(K\\)-folds. For each fold \\(k\\), model building is administered using \\(-k\\) folds. Using Step 2, predictions are generated for units in the \\(k^{th}\\) fold. Steps 2 and 3 are repeated until all \\(K\\) folds are exhausted. Estimation The following example uses 10 fold cross-fitting. ################################# # Author: VS # Last Revised: Jan 16, 2024 # Keywords: IPW, AIPW, GRF # # # ################################# set.seed(194) # Generate Data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) prob &lt;- 1 / (1 + exp(- (X[, 1] + rnorm(n)))) W &lt;- rbinom(n, 1, prob) Y &lt;- 2.56 * W + X[, 2] + pmax(X[, 1], 0) + rnorm(n) plot(X[, 1], X[, 2], col = as.factor(W)) #paste0(&quot;average treatment effect is: &quot;, round(mean(pmax(X[, 1], 0)), 3)) ################################# ################################# # # Inverse Probability Weighting # ################################# ################################# # use the random forest to get the propensity score dat &lt;- data.frame(W, X) n_features &lt;- length(setdiff(names(dat), &quot;W&quot;)) # A. ranger (probability tree) rf1_ranger &lt;- ranger( W ~ ., data = dat, mtry = min(ceiling(sqrt(n_features) + 20), n_features), num.trees = 2000, probability = TRUE ) # OOB predictions from ranger p.ranger &lt;- rf1_ranger$predictions[, 1] # B. probability tree using GRF # cross-fitting index K &lt;- 10 ind &lt;- sample(1:length(W), replace = FALSE, size = length(W)) folds &lt;- cut(1:length(W), breaks = K, labels = FALSE) index &lt;- matrix(0, nrow = length(ind) / K, ncol = K) for(f in 1:K){ index[, f] &lt;- ind[which(folds == f)] } # Build RF using GRF P(W = 1 | X) fun.rf.grf &lt;- function(X, W, predictkfold){ rf_grf &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) p.grf &lt;- predict(rf_grf, predictkfold)$predictions return(p.grf) } # storing predict.mat &lt;- matrix(0, nrow = nrow(index), ncol = K) tauk &lt;- rep(0, K) tauk_oracle &lt;- rep(0, K) weighttau &lt;- rep(0, K) score &lt;- list() score_oracle &lt;- list() # for each fold i use other folds for estimation for(i in seq(1:K)){ predict.mat[, i] &lt;- fun.rf.grf(X = X[c(index[, -i]), ], W = W[index[, -i]], predictkfold = X[c(index[, i]), ]) # fold-specific treatment effect score[[i]] &lt;- ((W[index[, i]] * Y[index[, i]]) / (predict.mat[, i])) - (((1 - W[index[, i]]) * Y[index[, i]]) / (1 - predict.mat[, i])) tauk[i] &lt;- mean(score[[i]]) } # ipw using oracle propensity score and propensity score estimated from grf alpha &lt;- 0.05 # 5 percent level of significance #ipw.ranger &lt;- mean(((W * Y) / (p.ranger)) - (((1 - W) * Y) / (1 - p.ranger))) ipw.grf &lt;- mean(unlist(score)) score_oracle &lt;- ((W * Y) / (prob)) - ((1 - W) * Y / (1 - prob)) ipw.oracle &lt;- mean(score_oracle) sd.ipw &lt;- sd(unlist(score)) sd.oracle &lt;- sd(score_oracle) ll &lt;- ipw.grf - (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ul &lt;- ipw.grf + (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ll_oracle &lt;- ipw.oracle - (sd.oracle / sqrt(length(score_oracle))) * qnorm(1 - alpha/2) ul_oracle &lt;- ipw.oracle + (sd.oracle / sqrt(length(score_oracle))) * qnorm(1 - alpha/2) result.ipw &lt;- c(&quot;IPW estimate&quot; = round(ipw.grf, 3), &quot;se&quot; = round(sd.ipw / (sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll, 3), &quot;upper bound&quot; = round(ul, 3)) result.oracle.ipw &lt;- c(&quot;IPW Oracle estimate&quot; = round(ipw.oracle, 3), &quot;se&quot; = round(sd.oracle / (sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll_oracle, 3), &quot;upper bound&quot; = round(ul_oracle, 3)) print(result.ipw) ## IPW estimate se lower bound upper bound ## 2.593 0.103 2.391 2.796 print(result.oracle.ipw) ## IPW Oracle estimate se lower bound upper bound ## 2.611 0.133 2.351 2.872 What? Despite having true propensity score, the Oracle IPW underperforms in accuracy compared to the IPW estimate with unknown propensity score. Why is it so? "],["aipw-and-estimation.html", "3.7 AIPW and Estimation", " 3.7 AIPW and Estimation Augmented Inverse Probability Weighting (AIPW) provides a robust way to estimate ATE by alleviating the limitation of IPW estimate. Following the IPW approach, estimation of ATE is given in equation (6). The other approach to estimate \\(\\tau\\) is to think of it from the conditional response approach. Write \\(\\mu_{w}(x) = E[Y_i| \\; X_i = x, W_i = w]\\). Then: \\(\\tau(x) = E[Y_i| \\; X_i = x, W_i = 1] - E[Y_i| \\; X_i = x, W_i = 0]\\) This is the regression outcome approach, where \\(\\tau = E[\\mu_{1}(x) - \\mu_{0}(x)]\\). The consistent estimator can be formed by using: \\(\\hat{\\tau}(x) = N^{-1} \\sum_{i = 1}^{N} \\mu_{1}(X_i) - \\mu_{0}(X_i)\\). AIPW approach combines both IPW approach as well as regression outcome approach to estimate \\(\\tau\\). \\(\\hat{\\tau}_{AIPW} = \\frac{1}{N} \\sum_{i = 1}^{N} (\\mu_{1}(X_i) - \\mu_{0}(X_i) + \\frac{(Y_i - \\hat{\\mu}_1(X_i)). W_i}{\\hat{e}(X_i)} - \\frac{(Y_i - \\hat{\\mu}_0(X_i)). (1-W_i)}{1 - \\hat{e}(X_i)})\\) ML approach using cross-fitting is used to estimate both \\(\\hat{e}(x)\\) and \\(\\hat{\\mu}_{w}(x)\\). Following the cross-fitting structure, we can formally write the estimate for \\(\\tau\\) as: \\(\\hat{\\tau}_{AIPW} = \\lowerbracket{\\frac{1}{N} \\sum_{i = 1}^{N} (\\mu_{1}^{-k(i)}(X_i) - \\mu_{0}^{-k(i)}(X_i)}_{consistent \\; estimate \\; of \\; \\tau} + \\frac{(Y_i - \\hat{\\mu}_1^{-k(i)}(X_i)). W_i}{\\hat{e}^{-k(i)}(X_i)} - \\frac{(Y_i - \\hat{\\mu}_0^{-k(i)}(X_i)). (1-W_i)}{1 - \\hat{e}^{-k(i)}(X_i)})\\) The AIPW approach can be thought of estimating ATE taking the difference across conditional responses. Next, the residuals are adjusted using weights given by the propensity score. There are two attractive features of AIPW estimate. First, $_{AIPW} is consistent as long as \\(\\hat{e}(x)\\) or \\(\\hat{\\mu}_{w}(x)\\) is consistent. This is because \\(E[(Y_i - \\hat{\\mu}_{W_i}(X_i)) \\approx 0\\). Second, \\(\\hat{\\tau}_{AIPW}\\) is a good approximation to oracle \\(\\hat{\\tau}_{AIPW}^{*}\\) as long as \\(\\hat{\\mu}(.)\\) and \\(\\hat{e}(.)\\) are reasonably accurate. If one estimate is highly accurate, then it can compensate lack of accuracy on the other estimate. If both \\(\\hat{\\mu}(.)\\) and \\(\\hat{e}(.)\\) are \\(\\sqrt{n}\\)-consistent5, then the following holds. \\(\\sqrt{n}(\\hat{\\tau}_{AIPW} - \\hat{\\tau}_{AIPW}^{*}) \\rightarrow_p 0\\). ####################### # # Augmented IPW (aipw) # ####################### #n_features2 &lt;- length(setdiff(names(dat2), &quot;Y&quot;)) # ranger #funrf_ranger &lt;- function(dat){ # rf2 &lt;- ranger( # Y ~ ., # data = dat, # mtry = min(ceiling(sqrt(n_features) + 20), n_features), # respect.unordered.factors = &quot;order&quot;, # seed = 123, # num.trees = 2000 # ) # return(rf2) #} # storing predict.mat2a &lt;- matrix(0, nrow = nrow(index), ncol = K) predict.mat2b &lt;- predict.mat2a aipwK &lt;- rep(0, K) weightaipK &lt;- rep(nrow(index) / length(index), K) for(i in seq(1:K)){ # E(Y | X, W = 1) using cross-fitting predict.mat2a[, i] &lt;- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], predictkfold = cbind(X[c(index[, i]), ], 1)) # E(Y | X, W = 0) using cross-fitting predict.mat2b[, i] &lt;- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], predictkfold = cbind(X[c(index[, i]), ], 0)) noise &lt;- ((W[index[, i]] * (Y[index[, i]] - predict.mat2a[, i])) / (predict.mat[, i])) - (((1 - W[index[, i]]) * (Y[index[, i]] - predict.mat2b)) / (1 - predict.mat[, i])) score[[i]] &lt;- predict.mat2a[, i] - predict.mat2b[, i] + noise aipwK[i] &lt;- mean(score[[i]]) } aipw.grf &lt;- weighted.mean(aipwK, weights = weightaipK) sd.aipw &lt;- sd(unlist(score)) ll &lt;- aipw.grf - (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ul &lt;- aipw.grf + (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) result.aipw &lt;- c(&quot;AIPW Est.&quot; = round(aipw.grf, 3), &quot;se&quot; = round(sd.aipw/(sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll, 3), &quot;upper bound&quot; = round(ul, 3)) ###################### # grf ###################### # Train a causal forest tau.forest &lt;- causal_forest(X, Y, W) # Estimate the conditional average treatment effect on the full sample (CATE). grf_ate &lt;- average_treatment_effect(tau.forest, target.sample = &quot;all&quot;) grf_att &lt;- average_treatment_effect(tau.forest, target.sample = &quot;treated&quot;) ## PRINT ALL #print(paste0(&quot;average treatment effect is: &quot;, round(mean(pmax(X[, 1], 0)), 3))) print(paste(&quot;treatment effects according to naive estimator:&quot;, round(mean(Y[which(W == 1)]) - mean(Y[which( W == 0)]), 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to naive estimator: 2.92&quot; print(paste(&quot;treatment effects according to IPW using&quot;, K, &quot;fold cross-fittin:&quot;, round(ipw.grf, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to IPW using 10 fold cross-fittin: 2.593&quot; print(paste(&quot;treatment effects according to IPW oracle:&quot;, round(ipw.oracle, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to IPW oracle: 2.611&quot; print(paste(&quot;treatment effects according to AIPW using&quot;, K, &quot;fold cross-fitting:&quot;, round(aipw.grf, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to AIPW using 10 fold cross-fitting: 2.627&quot; print(paste(&quot;treatment effects according to GRF:&quot;, round(grf_ate[[1]], 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to GRF: 2.542&quot; print(result.ipw) ## IPW estimate se lower bound upper bound ## 2.593 0.103 2.391 2.796 print(result.aipw) ## AIPW Est. se lower bound upper bound ## 2.627 0.035 2.559 2.695 print(grf_ate) ## estimate std.err ## 2.54233590 0.04855607 This means that \\(\\hat{\\mu}(.)\\) converges to \\(\\hat{\\mu}\\) at the ↩︎ "],["assessing-balance.html", "3.8 Assessing Balance", " 3.8 Assessing Balance ########################################## # # # Assessing Balance # # ########################################## XX &lt;- X[c(index), ] YY &lt;- Y[c(index)] WW &lt;- W[c(index)] e.hat &lt;- c(predict.mat) # unadjusted means.treat &lt;- apply(XX[WW == 1, ], 2, mean) means.control &lt;- apply(XX[WW == 0, ], 2, mean) abs.mean.diff &lt;- abs(means.treat - means.control) var.treat &lt;- apply(XX[WW == 1, ], 2, var) var.control &lt;- apply(XX[WW == 0, ], 2, var) std &lt;- sqrt(var.treat + var.control) # adjusted means.treat.adj &lt;- apply(XX*WW / e.hat, 2, mean) means.control.adj &lt;- apply(XX*(1 - WW) / (1 - e.hat), 2, mean) abs.mean.diff.adj &lt;- abs(means.treat.adj - means.control.adj) var.treat.adj &lt;- apply(XX * WW / e.hat, 2, var) var.control.adj &lt;- apply(XX * (1 - WW) / (1 - e.hat), 2, var) std.adj &lt;- sqrt(var.treat.adj + var.control.adj) # plot unadjusted and adjusted differences par(oma=c(0,4,0,0)) plot(-2, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, xlim=c(-.01, 1.01), ylim=c(0, ncol(XX)+1), main=&quot;&quot;) axis(side=1, at=c(-1, 0, 1), las=1) lines(abs.mean.diff / std, seq(1, ncol(XX)), type=&quot;p&quot;, col=&quot;blue&quot;, pch=19) lines(abs.mean.diff.adj / std.adj, seq(1, ncol(XX)), type=&quot;p&quot;, col=&quot;orange&quot;, pch=19) legend(&quot;topright&quot;, c(&quot;Unadjusted&quot;, &quot;Adjusted&quot;), col=c(&quot;blue&quot;, &quot;orange&quot;), pch=19) abline(v = seq(0, 1, by=.25), lty = 2, col = &quot;grey&quot;, lwd=.5) abline(h = 1:ncol(XX), lty = 2, col = &quot;grey&quot;, lwd=.5) mtext(paste0(&quot;X&quot;, seq(1, ncol(XX))), side=2, cex=0.7, at=1:ncol(XX), padj=.4, adj=1, col=&quot;black&quot;, las=1, line=.3) abline(v = 0) hist(e.hat, breaks = 100, freq = FALSE) "],["cross-fitting.html", "3.9 Cross-fitting", " 3.9 Cross-fitting What is cross-fitting? Divide the data into K folds randomly. Train the model using \\(-k\\) folds (all folds except the \\(k^{th}\\) one). Generate a fit of fold k on the model trained using \\(-k\\) folds Repeat steps 2 and 3 to generate fit for all \\(K\\) number of folds. This is illustrated using the figure below. The data is randomly divided into 5 folds (segments). This is an example of a five-fold cross-fitting. In the first round, the blue segments are used for model building, while responses are constructed for observations in the green segment of the data. Next, we move into the second round and so on; again the blue segments are used for model building and responses are constructed for the green segment. In this way, each observation is used for model building. # cross-fitting illustration colorcode &lt;- diag(5) # this creates a coding colorcode &lt;- c(colorcode) # Create data for the boxes boxes &lt;- data.frame( x = rep(seq(2, 10, 2), 5), y = rep(seq(5, 1, by = -1), each = 5), label = rep(paste(&quot;fold&quot;, seq(1, 5), sep = &quot; &quot;), 5), colorcode = colorcode ) boxes &lt;- boxes %&gt;% mutate(fill = ifelse(colorcode == 1, &quot;lightgreen&quot;, &quot;lightblue&quot;)) %&gt;% select(-c(colorcode)) # Create the plot ggplot() + geom_rect(data = boxes, aes(xmin = x , xmax = x + 2, ymin = y - 0.3, ymax = y + 0.5, fill = fill), color = &quot;black&quot;, alpha = 0.5) + xlim(0, 14) + ylim(-1, 6) + theme_void() + scale_fill_identity() + annotate(&quot;text&quot;, x = c(seq(3, 11, 2), rep(0.5, 5)), y = c(rep(0.3, 5), seq(5, 1, -1)), label = c(paste(&quot;fold&quot;, seq(1, 5, 1), sep = &quot; &quot;), paste(&quot;round&quot;, seq(1, 5, 1), sep = &quot; &quot;)), color = rep(c(&quot;red&quot;, &quot;black&quot;), each = 5) ) What does it do? Simply put, cross-fitting assures that the same observations are not used for modeling building as well as to estimate the response (e.g., predictions). In this way, we would want to alleviate concerns of over-fitting. "]]
