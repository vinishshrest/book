[["index.html", "Causal Inference 1 Work in Progress", " Causal Inference Vinish Shrestha 2025-01-13 1 Work in Progress This is a work in progress. Causal inference, like everything else, is ever-evolving. It is a topic that spans across disciplines and has established deep roots in machine learning literature. For the most part, we are trying to figure out whether X causes Y. Isn’t that essentially what science does – cause and effect? "],["causal-inference-an-introduction.html", "2 Causal Inference: An Introduction", " 2 Causal Inference: An Introduction “Correlation is not causality” is one of the most frequently used lines in social science. In a lab experiment, a researcher can perform controlled experiments to determine whether A causes B by controlling for confounders. However, the complexities and interrelations of human behavior create a setting starkly different from the controlled environment of a lab, making things much more convoluted. Causal inference, therefore, can be seen as a process to determine whether A causes B in both lab settings and out-of-lab scenarios. A simple example. Say, we are interested in evaluating the effects of a tutoring program on exam scores for an introductory course. To begin, in this simple example, we assume that the treatment is (completely) randomly assigned. The class is randomly divided into two groups: one group receives the treatment (treatment group) and the other group does not receive the treatment (control group). Proper randomization means that each individual has an equal probability of receiving the treatment or not receiving it. This approach with an arbitrarily high probability ensures balance in both observed and unobserved factors as the sample size grows such that any differences in outcomes between the treatment and control groups can be attributed to the treatment itself, rather than to pre-existing differences between the groups.1 Balance here is defined as an instance when all pre-treatment covariates between the treatment and control groups are similar. If this is attained then it increases confidence that the treatment and the control units are comparable. Set up. We use \\(W\\) to denote the treatment status such that \\(W_i \\in \\{0, \\; 1\\}\\), \\(Y_i\\) is the exam score following the treatment assignment, and \\(X_i\\) are the covariates (e.g., gender, race). The subscript \\(i\\) indicates an individual or unit of observation. Of course, balance is not guranteed and in such cases one should think hard whether differences in covariates matter, and if they do, adjustment should be applied.↩︎ "],["potential-outcome-framework-neyman-rubin-causal-model.html", "2.1 Potential Outcome Framework: Neyman-Rubin Causal Model", " 2.1 Potential Outcome Framework: Neyman-Rubin Causal Model We are going to use the potential outcome framework to describe the impacts of the treatment following the Neyman-Rubin causal model (Splawa-Neyman, Dabrowska, and Speed 1923 [1990]; Rubin 1974). Define \\(Y_i(0)\\) and \\(Y_i(1)\\) as the potential outcomes for an individual \\(i\\) in the case of treatment and without treatment, respectively. The potential outcomes are not realized yet. As such it is wrong to say that \\(Y_{i}(0) = Y_i\\). Let’s spend some time discussing various formats of the potential outcome in relation to what is observed versus what is not. \\([Y_{i}(0)|W_i = 0].\\) Here, the expression in the bracket is read as the outcome of an unit \\(i\\) in the no-treatment state conditional upon \\(i\\) actually not receiving the treatment. This is an observed outcome. \\([Y_{i}(0)|W_i = 1].\\) Here, the expression is asking for what the outcome of an unit \\(i\\) who received the treatment \\((W_i = 1)\\) would be in absence of the treatment. This is not observed and is termed as the counterfactual. The same goes with the potential outcome \\(Y_{i}(1)\\) – the outcome if \\(i\\) were to be treated. The observed variable, \\(Y_i\\), can be written as a function of the potential outcomes as follows: \\[\\begin{equation} Y_i = W \\times Y_i(1) + (1-W)\\times Y_i(0) \\end{equation}\\] The fundamental problem is that one cannot observe both \\(Y_i(0)\\) and \\(Y_i(1)\\) at the same time. As such, the causal inference through the lens of Neyman-Rubin causal framework can be seen as the missing data problem. If one has the data for \\(Y_i(1)\\) then the \\(Y_i(0)\\) counterpart is missing and vice-versa. Much of causal inference is finding ways to deal with the missing-data problem. The independence assumption allows us to proceed further with causality. Formally, a complete random assignment of treatment means: \\(W_i \\perp Y_i(0), Y_i(1)\\). This states that the treatment assignment is independent of potential outcomes. Quite literally, this means that the treatment assignment is not related to the potential outcome. In other words, the treatment assignment is completely random (probability of being treated is 0.5 in the case of binary treatment). The independence assumption also states that the treatment assignment is independent of any covariates \\(X_i\\). In our particular example, this means that the probability of receiving the treatment is the same for different groups defined by these covariates, such as gender and race. Specifically, females are equally likely to get treated compared to males, and Blacks are equally likely to be treated compared to Whites. Within both the treatment and control groups, it is highly likely that the proportion of Blacks and Whites, as well as males and females, will be similar – an attribute known as balance. The independence assumption is one of the necessary assumptions to proceed further but it is not sufficient. Additional two assumptions are needed to proceed ahead: overlap and Stable Unit Treatment Value Assumption (SUTVA). The overlap assumption states that observations in both the treatment and control groups fall within the common support. For instance, this assumption is violated if the treatment group consist of all females and the control group consist of all males as one would not be able to attain balance in covariates. The independence and overlap assumption together constitute a property known as stong ignorability of assignment, which is necessary for the identification of the treatment effect. The SUTVA assumption is the no interference assumption defining that the treatment status of one unit should not affect the potential outcome for other units. In our example, tutoring treatment for a unit in the treatment group should not change the potential outcome for other units. This assumption breaks down if there is a spillover effect, for example, if the a student in the treatment group helps her friend in the control group. References "],["average-treatment-effect-ate.html", "2.2 Average treatment effect (ATE)", " 2.2 Average treatment effect (ATE) Our target is to estimate the effects of the treatment. For a brief moment, let’s assume the presence of a parallel universe that includes Alia, Ryan, Shrey, Samaira, and Rakshya in the course. In one universe (actual universe) the treatment for these individuals are randomly allocated: \\(W_{Alia} = 1\\), \\(W_{Ryan} = 0\\), \\(W_{Shrey} = 0\\), \\(W_{Samaira} = 1\\), \\(W_{Rakshya} = 0\\). In the other (parallel) universe, everything is similar to the actual universe except that the treatment status is exactly opposite. In this case, individual specific treatment effect can be estimated by taking the difference in individual specific outcomes across two universes. For example, the treatment effect for Alia is \\(Y_{Alia}(1) - Y_{Alia}(0).\\) This is feasible since a perfect counterfactual is available for all the units given the parallel universe. The average of such individual treatment effect gives the average treatment effect, ATE. The target is to estimate average treatment effect (ATE), which is defined as: \\[\\begin{equation} ATE = E(Y_i(1)) - E(Y_i(0)) \\end{equation}\\] \\(Y_i(1)\\) denotes the outcome for an unit \\(i\\) in presence of treatment, whereas \\(Y_i(0)\\) is the realization for the same unit \\(i\\) in absence of the treatment. As we know, it is impossible to measure the unit \\(i\\) in two different states (with and without treatment). A major difficulty is that one cannot observe units simultaneously with and without treatment in reality. This means that the perfect counterfactual does not really exist. This again emphasizes causal inference as a missing data problem – when estimating the treatment effect of an unit \\(i\\), \\(Y_i(0)\\) is not observed if \\(Y_i(1)\\) is and vice-versa. This unfortunately does not allow us to estimate individualized treatment effect. The best we can do (as of yet) is use the independence assumption as well as overlap assumption together and evalute ATE. Note that the independence condition, $ W_i Y_i(0), ; Y_i(1)$, gives: \\(E(Y_i(0)|W_i = 1) = E(Y_i(0)|W_i = 0)\\). The term, \\(E(Y_i(0))\\), in ATE equation is replaced by \\(E(Y_i|W_i = 0)\\). In the case of a pure randomized experiment, the ATE is given as: \\[\\begin{equation} ATE = E(Y_i | W_i = 1) - E(Y_i | W_i = 0) \\end{equation}\\] ATE evaluates treatment effect for the whole population by comparing the treated units to the control units. "],["rct.html", "2.3 RCT", " 2.3 RCT Randomized Controlled Trials (RCTs) are the cornerstone of causal inference and are often referred to as the gold standard. The quality of non-experimental studies is frequently assessed by comparing how closely the observational setting approximates an RCT. In an RCT, the Average Treatment Effect (ATE) is identified through the randomization of treatment assignment. This process ensures that the treatment and control groups are comparable, making RCTs a straightforward yet immensely powerful tool for establishing causal relationships. In a simple RCT setting the treatment is binary – the units are either assigned to the treatment group \\((W_i = 0)\\) or the control group \\((W_i = 1)\\). The implicit assumption in this design is that each unit has an equal probability of being treated. The treatment assignment for the RCT setting can be attained using a Bernoulli process, where each unit has an independent probability \\(\\pi\\) of receiving treatment. Specifically, each unit is assigned to the treatment group with probability \\(\\pi\\) and to the control group with probability \\(1 - \\pi\\). # a bernoulli process of treatment assignment library(ggplot2) fun_treat_assign &lt;- function(N, prob, treat.type){ treatment &lt;- rbinom(N, size = 1, p = prob) dat &lt;- data.frame(treatment = treatment, type = treat.type) return(dat) } # p = 0.5 for each unit dat1 &lt;- fun_treat_assign(N = 10000, prob = 0.5, treat.type = &quot;p = 0.5&quot;) # p = 0.3 for each unit dat2 &lt;- fun_treat_assign(N = 10000, prob = 0.2, treat.type = &quot;p = 0.2&quot;) dat.assign &lt;- rbind(dat1, dat2) # plot ggplot(dat.assign, aes(x = treatment)) + geom_histogram(fill= &quot;skyblue&quot;, color = &quot;black&quot;) + facet_wrap(~ type) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. A practical example of this is an unbiased coin toss used to determine treatment assignment. In this case, a head could correspond to the treatment group (e.g., \\(W_i = 1\\)), and a tail to the control group (e.g., \\(W_i = 0\\)). This method exemplifies Bernoulli-randomization, where the assignment is determined by a random process, ensuring that each unit has an equal probability of being assigned to either group. In an RCT setting, the difference-in-means estimator is given as: \\[\\begin{equation} \\hat{\\tau} = \\frac{1}{N_t}\\sum_{W_i =1} Y_i - \\frac{1}{N_c}\\sum_{W_i =0} Y_i \\end{equation}\\] The difference-in-mean estimator is unbiased and consistent for the average treatment effect. "],["average-treatment-effect-on-the-treated-att.html", "2.4 Average treatment effect on the treated (ATT)", " 2.4 Average treatment effect on the treated (ATT) The average treatment effect on the treated is concerned with the evaluation of treatment effects for only those units that are treated. Formally, it is defined as: \\[\\begin{equation} ATT = E(Y_i(1) - Y_i(0) | W_i = 1) \\end{equation}\\] ATE is only concerned with a subset of the population who received the treatment, \\(E[.|W_i = 1]\\). Here, ATT is comparing outcomes among the treated units in presence of the treatment versus what the outcomes would have been in absence of the treatment only for units receiving the treatment. Hence, only one segment of the counterfactual (potential outcome) is required. In our example, the counterfactual for Alia and Samaira would allow estimation of ATT, whereas ATE requires counterfactual for everyone. The independence condition states that on average the potential outcomes for the treated group in absence of the treatment would be similar to the average outcome for the control group, i.e. \\(E(Y_i(0)|W_i = 1) = E(Y_i(0)|W_i = 0)\\). This allows re-writing ATT as the following: \\[\\begin{align} ATT = E\\{E(Y_i | W_i = 1) - E(Y_i | W_i = 0) | W_i = 1\\} \\\\ ATT = E(Y_i | W_i = 1) - E(Y_i | W_i = 0) \\end{align}\\] The second line follows from the independence assumption which allows this: \\(E\\{E(Y_i | W_i = 0) | W_i = 1\\} = E(Y_i | W_i = 0).\\) Under the independence assumption this means that we can estimate ATT by substrating the averages of exam score across the treated and control units. In purely randomized experiments, if there is a perfect case of compliance, then the ATT will be similar to ATE. "],["an-estimation-example.html", "2.5 An estimation example", " 2.5 An estimation example # create a function to estimate the treatment effects fun_ATE &lt;- function(tau, N){ # @arg tau: true treatment effect # @arg N: sample size # Return tau_hat: estimate of the treatment effect gender &lt;- rbinom(N, size = 1, p = 0.5) race &lt;- rbinom(N, size = 1, p = 0.5) W &lt;- rbinom(N, size = 1, p = 0.5) Y &lt;- 50 + tau * W + gender * 5 + race * 10 + rnorm(n = N, mean = 5, sd = 5) tau_hat &lt;- mean(Y[which(W == 1)]) - mean(Y[which(W == 0)]) return(tau_hat) } # print treatment effect print(paste(&quot;the ATE estimate is: &quot;, fun_ATE(tau = 10, N = 2000))) ## [1] &quot;the ATE estimate is: 9.59413839628461&quot; # run 2000 replications to get a distribution of tau_hats reps &lt;- 2000 tau.hats &lt;- rep(0, reps) for(i in 1:reps){ tau.hats[i] &lt;- fun_ATE(tau = 10, N = 2000) } # histogram of tau hats hist(tau.hats, breaks = 30) # obtaining the standard error print(paste(&quot;the mean of tau hats : &quot;, mean(tau.hats))) ## [1] &quot;the mean of tau hats : 10.0009346263204&quot; print(paste(&quot;the standard error of tau hats : &quot;, sd(tau.hats))) ## [1] &quot;the standard error of tau hats : 0.345532965269282&quot; "],["unconfoundedness-assumption.html", "2.6 Unconfoundedness assumption", " 2.6 Unconfoundedness assumption Most of the time the treatment assignment may not be fully random but can be driven by some selective covariates. Referring to the tutoring example, it may be unethical to disallow someone in the control group who wants to attend the tutoring sessions. As such, tutoring sessions may be voluntarily held, where students can select whether to attend the session. Say, females and Blacks are more likely to attend the tutoring session and both of these variables are also likely to yield higher potential outcome. This means that females and Blacks are more likely to have higher exam score in absence of the treatment compared to males and Whites. It is easy to see that the treatment assignment is correlated with the potential outcomes and the independence assumption is violated. This is true in many cases of observational settings and even in randomized experiments. We require adjustments before being able to estimate treatment effects in such cases. If we understand the treatment mechanism fairly well then we can still proceed further to estimate the treatment effects. For example, suppose the treatment assignment is (voluntarily) more tilted towards females than males and Blacks than Whites. In this case, we would want to invoke unconfoundedness (conditional independence) assumption.2 Formally, this states that \\(Y_i(0), \\; Y_i(1) \\perp W_i | X_i\\). This means that conditional upon the covariates the treatment assignment is random. To estimate ATT one would want to first estimate ATT within each strata: \\(i)\\) female-Black, \\(ii)\\) female-White, \\(iii)\\) male-Black, and \\(iv)\\) male-White, and take a weighted average of the strata-specific ATEs by using the proportion of the sample in the given strata as weights. The conditional independence assumption means that within each strata treatment assignment is random. The following code first estimates the strata specific ATTs and then summarizes them using the weighted average. Note that the true treatment effect is 10. fun_ATE2 &lt;- function(N, tau){ # @arg tau: true treatment effect # @arg N: sample size # Return tau_hat: estimate of the treatment effect using conditional randomness assumption # Return tau_hat2: estimate of the treatment effect wrongly using unconditional independence assumption # Return reg_tau: estimate from conditioning using regression but from a misspecified model # create pseudo data gender &lt;- rbinom(N, size = 1, p = 0.5) race &lt;- rbinom(N, size = 1, p = 0.5) W &lt;- rbinom(N, size = 1, p = 0.2 + 0.4 * (gender &gt; 0) + 0.2 * (race &gt; 0)) Y &lt;- 40 + 10 * W + gender * 2 + race * 5 + 25 * race * gender + rnorm(n = N, mean = 5, sd = 5) # female-Blacks tau_hat1 &lt;- mean(Y[which(W == 1 &amp; gender == 1 &amp; race == 1)]) - mean(Y[which(W == 0 &amp; gender == 1 &amp; race == 1)]) w1 &lt;- sum(gender == 1 &amp; race == 1) / N # female-Whites tau_hat2 &lt;- mean(Y[which(W == 1 &amp; gender == 1 &amp; race == 0)]) - mean(Y[which(W == 0 &amp; gender == 1 &amp; race == 0)]) w2 &lt;- sum(gender == 1 &amp; race == 0) / N # male-Blacks tau_hat3 &lt;- mean(Y[which(W == 1 &amp; gender == 0 &amp; race == 1)]) - mean(Y[which(W == 0 &amp; gender == 0 &amp; race == 1)]) w3 &lt;- sum(gender == 0 &amp; race == 1) / N # male-Whites tau_hat4 &lt;- mean(Y[which(W == 1 &amp; gender == 0 &amp; race == 0)]) - mean(Y[which(W == 0 &amp; gender == 0 &amp; race == 0)]) w4 &lt;- sum(gender == 0 &amp; race == 0) / N tau_hat &lt;- tau_hat1 * w1 + tau_hat2 * w2 + tau_hat3 * w3 + tau_hat4 * w4 tau_hat2 &lt;- mean(Y[W == 1]) - mean(Y[W == 0]) # a mis-specified regression model reg &lt;- lm(Y ~ W + gender) reg_tau &lt;- coefficients(reg)[[2]] return(list(table(gender[W == 1]), table(race[W == 1]), tau_hat, tau_hat2, reg_tau)) } ATE2_results &lt;- fun_ATE2(N = 20000, tau = 10) print(paste(c(&quot;treated males: &quot;, &quot;treated females: &quot;) , ATE2_results[[1]])) ## [1] &quot;treated males: 2971&quot; &quot;treated females: 7092&quot; print(paste(c(&quot;treated Whites: &quot;, &quot;treated Blacks: &quot;) , ATE2_results[[2]])) ## [1] &quot;treated Whites: 4054&quot; &quot;treated Blacks: 6009&quot; print(paste(&quot;ATE conditioned on Xs is :&quot;, ATE2_results[[3]])) ## [1] &quot;ATE conditioned on Xs is : 10.0473051532008&quot; print(paste(&quot;ATE not conditioned on Xs is :&quot;, ATE2_results[[4]])) ## [1] &quot;ATE not conditioned on Xs is : 19.3709518303825&quot; # get tau_hats from replications store &lt;- rep(0, reps) store2 &lt;- store store.reg &lt;- store for(i in 1:reps){ ATE.results &lt;- fun_ATE2(N = 20000, tau = 10) store[i] &lt;- ATE.results[[3]] store2[i] &lt;- ATE.results[[4]] store.reg[i] &lt;- ATE.results[[5]] } # histogram of tau_hat conditioned hist(store, main = &quot;tau hats conditioned&quot;) print(paste(&quot;The standard error from the conditioned approach is:&quot;, sd(store))) ## [1] &quot;The standard error from the conditioned approach is: 0.0794243135722255&quot; hist(store2, main = &quot;tau hats not conditioned&quot;) The ATT estimate is much closer to the true parameter, 10, when conditioned upon the covariates as compared to an unconditional approach (where the distribution of ATT estimate is centered around 19.3). This example highlights the importance of conditioning on \\(X\\)s when evaluating the treatment effects if the treatment assignment is correlated with the potential outcomes. In this case, Blacks and females are more likely to have higher scores in general even without the treatment and both of these subgroups are also more likely to be treated. Treatment is not only non-random but is systematically correlated with the outcomes. Since we have the perfect information on the treatment assignment mechanism, after conditioning for the covariates the treatment assignment is essentially random. In other words, within Black vs. White race groups, for example, the treatment assignment is randomly allocated. This allows estimation of ATE for each subgroup or strata. After estimating ATE for each strata, the ATEs are averaged using the sample size of the strata as weights. One problem with the approach highlighted above is that in complex settings, with many determinants (multi-dimensionality) of treatment or in presence of continuous covariates, the sub-space required for the analyses highlighted above will be thinned out too soon. As an alternative, regression framework has been rigorously used as a tool-kit to control for covariates. While there are benefits of using a regression framework, it is by nomeans a panacea. This is especially true if the regression models are misspecified. Below we will use a misspecified version of the regression model to see if we can recover the treatment estimate close to the true value. print(paste(&quot;ATE estimated from misspecified regression model:&quot;, ATE2_results[[5]])) ## [1] &quot;ATE estimated from misspecified regression model: 14.3394045839167&quot; print(paste(&quot;The standard error is:&quot;, sd(store.reg))) ## [1] &quot;The standard error is: 0.179320398608073&quot; hist(store.reg, main = &quot;Treatment effects using regression&quot;) The terms unconfoundedness and conditional independence (\\(heart\\; disease \\perp age \\; | \\; cholestrol\\)) are used interchangebly in causal inference literature. Conditional independence is a broader term that relates to the general field of probability and statistics, whereas unconfoundedness is more specific to causal inference. Unconfoundedness implies a specific kind of conditional independence, specific to causal inference.↩︎ "],["discussion.html", "2.7 Discussion", " 2.7 Discussion In our discussion, we explored the causal effect through the lens of the potential outcome framework, emphasizing the crucial assumptions needed for its accurate identification. We particularly focused on the independence assumption and the unconfoundedness assumption, alongside the Stable Unit Treatment Value Assumption (SUTVA) and the overlap assumption. These assumptions play a pivotal role in ensuring valid causal inference, with the conditional independence assumption being highly effective in randomized controlled trials. However, in observational studies where randomization is not possible, the conditional independence assumption can be quite stringent and challenging to meet as there might be unobserved variables driving the treatment assignment. Consequently, it is essential to leverage alternative methodologies designed for observational settings. Exploring approaches such as propensity score matching, instrumental variables, regression discontinuity design, and difference-in-differences can help overcome these challenges and improve the robustness of causal effect estimation when randomization is not feasible. By employing these methods, we can better navigate the complexities of observational data and draw more reliable causal inferences. "],["reference.html", "2.8 Reference", " 2.8 Reference "],["ipw-and-aipw.html", "3 IPW and AIPW", " 3 IPW and AIPW The target is to estimate the average treatment effect (ATE): \\[\\begin{equation} \\label{eq:ATE} ATE = E[Y_i(1) - Y_i(0)] \\tag{3.1} \\end{equation}\\] Note that using the following two assumptions: \\(W_i \\perp \\{Y_i(0), \\; Y_i(1)\\}\\) (independence assumption) \\(Y_i(W) = Y_i\\) (SUTVA) the ATE estimate \\(\\hat{\\tau}\\) can be written as the difference-in-means estimator: \\[\\begin{equation} \\label{eq:ATE_estimator} \\hat{\\tau} = \\frac{1}{N_T} \\sum_{W_i = 1} Y_i - \\frac{1}{N_C} \\sum_{i \\in W_i = 0} Y_i \\end{equation}\\] where \\(N_T\\) and \\(N_C\\) are the number of treated and control units, respectively. In the previous lecture, we disscussed randomized control trial as an ideal approach to estimate ATE. In a randomized controlled trial each unit has an equal probability of receiving the treatment. This means the following: \\[\\begin{equation} P(W_i = 1 \\; | \\; Y_i(0), \\; Y_i(1), \\; n_T) = \\frac{n_T}{n}, \\; \\; i = \\{1, ...., n\\} \\tag{3.2} \\end{equation}\\] In equation (3.2), \\(n_T\\) refers to the number of units that receives the treatment.3 In an easy to understand set-up, if a researcher wants \\(P(W_i = 1) = 0.5\\) (unit is equally likely to be treated or untreated), a coin flip can feasibly be used as a mechanism to assign treatment.4 Although randomized controlled trials (RCTs) are often considered the gold standard in causal inference, they cannot always be used due to ethical, moral, and monetary reasons. Returning to the example we used in the previous chapter, it is not ethical to demarcate who can attend the tutoring session versus who cannot. In real-world scenarios, tutoring sessions are typically voluntary. Students who regularly attend these sessions may have different baseline (pre-treatment) characteristics compared to those who do not attend. These differences can introduce biases that complicate causal inference in observational studies. To proceed further in observational setting (without using RCTs), we require more knowledge about the treatment assignment. In other words, we need to understand which variables determine who attends the tutoring sessions. This information is crucial for identifying potential confounders and for applying methods that can help estimate causal effects in observational settings. In causal inference, confounders are variables that are associated with both the treatment and the outcome. They can introduce bias in the estimation of the causal effect of the treatment on the outcome by providing alternative explanations for any observed relationships. For example, say you are trying to evaluate the efficacy of a new drug on blood pressure level. If smokers are more likey to get treated and if they tend to have higher blood pressure to begin with, the treatment effects are likely to be understated. This brings us to the unconfoundedness assumption. Unconfoundedness: The treatment assignment is as good as random once we control for \\(X\\)s. \\[\\begin{equation} \\{W_i \\perp \\{Y_i(0), \\; Y_i(1)\\} | X_i \\} \\; for \\; all \\; x \\in \\chi. \\tag{3.3} \\end{equation}\\] As with the tutoring example, the independence assumption (discussed in the previous chapter) is highly unlikely to hold in observational settings. Let’s consider the following scenarios: Out of the ten states that are yet to expand Medicaid, eight fall in South. Medicaid expansion is not random. Cigarette taxes are higher in states with higher anti-smoking sentiments. Infrastructure development, such as construction of roads, schools, hopitals, are demand-driven. The list goes on .. However, if we manage to observe all the \\(X\\)s (covariates) that influence the treatment, we can invoke unconfoundedness for causal inference. Although it is generally recommended to assign half of the sample to the treatment group and the other half to the control group, this is not a strict requirement.↩︎ Of course, this is quicky going to be inefficient as the sample size increases. In general, treatment assignment is determinted by a statistical process via a software. For example, if a researcher wants about one-third of the sample treated then a bernoulli trial with the probability of success of 0.33 can be used.↩︎ "],["a-simple-example.html", "3.1 A simple example", " 3.1 A simple example Say, you are interested in evaluating the effect of tutoring program initiated following the first exam on grades at an introductory level course. For simplicity, the possible grades are A and B. However, students who received B on their first exam are more likely to attend the tutoring session. In other words, \\(P(W_i = 1 | Y_{iFE} = A) &lt; P(W_i = 1 | Y_{iFE} = B)\\) (\\(Y_{iFE}\\) is read as unit \\(i&#39;s\\) grade in the first exam). In this case, the treatment assignment is correlated with the past grade, which can predict the grade on the second exam. In other words, if you did well in the first exam, you are likely to perform well in the second exam and so on. Hence, using equation (2) to estimate effects of the tutoring program will result in biased estimate. Since we know that the probability of treatment is influenced by the grade on the first exam, we can estimate the conditional average treatment effect (CATE) and average them using weights to form an estimate of ATE. Let’s take a look at the data. # function to report grade breakdown by the first exam grade (A and B) grade_mat &lt;- function(grade_vec){ grade &lt;- matrix(0, nrow =2, ncol = 3) grade[, 1] &lt;- c(&quot;Treat&quot;, &quot;Control&quot;) grade[ ,2] &lt;- c(grade_vec[1], grade_vec[2]) grade[ ,3] &lt;- c(grade_vec[3], grade_vec[4]) return(grade) } # Y_iFS == A grade &lt;- grade_mat(c(5, 9, 2, 4)) colnames(grade) &lt;- c(&quot; &quot;, &quot;A (2nd Exam)&quot;, &quot;B (2nd Exam)&quot;) # Se grade %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) %&gt;% add_header_above(c(&quot;Table 1.&quot; = 1, &quot;Grade in the 2nd exam | 1st exam = A&quot; = 2)) Table 1. Grade in the 2nd exam | 1st exam = A A (2nd Exam) B (2nd Exam) Treat 5 2 Control 9 4 grade &lt;- grade_mat(c(15, 1, 5, 4)) colnames(grade) &lt;- c(&quot; &quot;, &quot;A (2nd Exam)&quot;, &quot;B (2nd Exam)&quot;) grade %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) %&gt;% add_header_above(c(&quot;Table 2.&quot; = 1, &quot;Grade in the 2nd exam | 1st exam = B&quot; = 2)) Table 2. Grade in the 2nd exam | 1st exam = B A (2nd Exam) B (2nd Exam) Treat 15 5 Control 1 4 \\(~\\) \\(~\\) Estimation \\(\\hat{\\tau}_{FE=A} = \\frac{5}{7} - \\frac{9}{13} = 2.1 \\; pp\\) \\(\\hat{\\tau}_{FE=B} = \\frac{15}{20} - \\frac{1}{5} = 55 \\; pp\\) \\(\\hat{\\tau}_{AGG} = \\frac{20}{45} \\hat{\\tau}_{FE=A} - \\frac{25}{45} \\hat{\\tau}_{FE=B} = 31.48 \\; pp\\). The first two are CATEs for the group that recived A and B in the first exam. The assumption is that once conditioned on the grade in the first exam, treatment (who attends vs. who doesn’t) is random. This allows valid estimation of within group causal effects, which are then averaged to form ATE using appropriate weights on the third line. This simple example using the discrete feature space (grade in the first exam can be A or B) provides intuition that if variables influencing the treatment assignment are observed then ATE estimate can be uncovered by taking weighted average of CATE estimates (these are also group-wise ATE).5 In this case, CATEs are different across the two sub-groups. Sometimes the core interest of analysis can be uncovering the heterogeneous treatment effects, which motivates estimation and inference on CATEs across two or more sub-groups.↩︎ "],["aggregated-estimator.html", "3.2 Aggregated Estimator", " 3.2 Aggregated Estimator As we saw in Section 1, aggregated estimator can be used in when we know what exact variables are determining the treatment assignment. We will consider the discrete case, when a discrete covariate determines the probability of treatment for simplicity. Note that students who received B on their first exam are more likely to attend the tutoring session in our simple example. So grade in the first exam (A or B for simplicity) determines the treatment here. Setup. \\(Y_i\\) is the outcome variable; final exam score. \\(W_i\\) is the treatment variable; whether a student attended the tutoring session. \\(X_i\\) is the covariate; a student’s grade on the first exam. We want to evaluate the effects of attending a tutoring program on a student’s exam score. What we would want to do is to condition on the first exam’s grade, estimate the treatment effects, and aggregate it. The aggregated estimator is given as: \\[\\begin{align} \\hat{\\tau}_{AGG} = \\frac{n_{A}}{n} \\bigg[\\frac{1}{n_{A1}} \\underbrace{\\sum}_{\\substack{i \\in A \\\\ \\textrm{W = 1}}} Y_i - \\frac{1}{n_{A0}} \\underbrace{\\sum}_{\\substack{i \\in A \\\\ \\textrm{W = 0}}} Y_i \\bigg] + \\frac{n_{B}}{n} \\bigg[\\frac{1}{n_{B1}} \\underbrace{\\sum}_{\\substack{i \\in B \\\\ \\textrm{W = 1}}} Y_i - \\frac{1}{n_{B0}} \\underbrace{\\sum}_{\\substack{i \\in B \\\\ \\textrm{W = 0}}} Y_i \\bigg] \\tag{3.4} \\end{align}\\] Note that the first block is the treatment effect for those who received A on their first exam, whereas the second block represents treatment effect for those who received B. After a few steps of simple algebra, equation @ref{eq:AGG0} can be written as: \\[\\begin{equation} = \\frac{1}{n} \\bigg[ \\frac{1}{\\frac{n_{A1}}{n_{A}}} \\underbrace{\\sum}_{\\substack{i \\in A}} Y_i \\times W_i - \\frac{1}{\\frac{n_{A0}}{n_{A}}} \\underbrace{\\sum}_{\\substack{i \\in A}} Y_i \\times (1 - W_i) \\bigg] + \\\\ \\frac{1}{n} \\bigg[ \\frac{1}{\\frac{n_{B1}}{n_{B}}} \\underbrace{\\sum}_{\\substack{i \\in B}} Y_i \\times W_i - \\frac{1}{\\frac{n_{B0}}{n_{B}}} \\underbrace{\\sum}_{\\substack{i \\in B}} Y_i \\times (1 - W_i) \\bigg] \\tag{3.5} \\end{equation}\\] Equation (3.5) looks super complicated, buts its not. Let’s breakdown the components of it: \\(\\frac{n_{A1}}{n_A}:\\) Represents the fraction of treated individuals who received \\(A\\) on the first exam. \\(\\frac{n_{A0}}{n_A}:\\) Represents the fraction of untreated individuals who received \\(A\\) on the first exam. \\(\\frac{n_{B1}}{n_B}:\\) Represents the fraction of treated individuals who received \\(B\\) on the first exam. \\(\\frac{n_{B0}}{n_B}:\\) Represents the fraction of untreated individuals who received \\(B\\) on the first exam. Note that \\(\\frac{n_{A1}}{n_A} = e(X = A)\\) and \\(\\frac{n_{A0}}{n_A} = 1 - e(X = A)\\) and the same goes with the segment composed of those who received B on their first exam. Equation (3.5) can be further simplified as: \\[\\begin{equation} \\hat{\\tau}_{AGG} = \\frac{1}{n}\\bigg[ \\sum \\frac{Y_i \\times W_i}{e(X)} - \\sum \\frac{Y_i \\times (1-W_i)}{1 - e(X)} \\bigg] \\tag{3.6} \\end{equation}\\] Equation @ref{eq:AGG2} takes the form of an inverse probability-weighted estimator. We’ll find out that Aggregate Estimator \\((\\hat{\\tau}_{AGG})\\) is a special case of Inverse Probability Weighting later on in this section. n &lt;- 2000 p &lt;- 10 true_effect &lt;- 10 set.seed &lt;- 12570 agg.means &lt;- replicate(1000, { X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) prob &lt;- 1 / (1 + exp(- (X[, 1] &lt; 1 + rnorm(n)))) W &lt;- rbinom(n, 1, prob) Y &lt;- 60 + true_effect * W + 5 * pmax(X[, 1], 1) + rnorm(n) group &lt;- ifelse( X[, 1] &lt; 1, &quot;A&quot;, &quot;B&quot; ) # estimates for those receiving A and B on the first exam att.A &lt;- mean(Y[group == &quot;A&quot; &amp; W == 1]) - mean(Y[group == &quot;A&quot; &amp; W == 0]) att.B &lt;- mean(Y[group == &quot;B&quot; &amp; W == 1]) - mean(Y[group == &quot;B&quot; &amp; W == 0]) prop.A &lt;- mean(group == &quot;A&quot;) prop.B &lt;- mean(group == &quot;B&quot;) prop.A * att.A + prop.B * att.B } ) # plot(X[, 1], X[, 2], col = as.factor(W)) hist(agg.means, freq = F, main = &quot;&quot;, col= rgb(0, 0, 1, 1/8), xlab = &quot;ATT estimates&quot;, las = 1) abline(v = true_effect, lwd = 3, lty = 2) legend(&quot;topright&quot;, &quot;True effect&quot;, lwd = 3, lty = 2, bty = &quot;n&quot;) paste(&quot;mean of AGG estimates: &quot;, round(mean(agg.means), 3)) ## [1] &quot;mean of AGG estimates: 9.956&quot; paste(&quot;standard error of AGG estimates: &quot;, round(sd(agg.means), 3)) ## [1] &quot;standard error of AGG estimates: 0.062&quot; Let’s compare this to a naive estimator. set.seed &lt;- 12570 naive.means &lt;- replicate(1000, { X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) prob &lt;- 1 / (1 + exp(- (X[, 1] &lt; 1 + rnorm(n)))) W &lt;- rbinom(n, 1, prob) Y &lt;- 60 + true_effect * W + 5 * pmax(X[, 1], 1) + rnorm(n) # naive estimates that does not take voluntary selection into account mean(Y[ W == 1]) - mean(Y[ W == 0]) } ) # plot(X[, 1], X[, 2], col = as.factor(W)) hist(naive.means, freq = F, main = &quot;&quot;, col= rgb(0, 0, 1, 1/8), xlab = &quot;ATT naive estimates&quot;, las = 1) abline(v = true_effect, lwd = 3, lty = 2) legend(&quot;topright&quot;, &quot;True effect&quot;, lwd = 3, lty = 2, bty = &quot;n&quot;) paste(&quot;mean of naive estimates: &quot;, round(mean(naive.means), 3)) ## [1] &quot;mean of naive estimates: 9.761&quot; paste(&quot;standard error of naive estimates: &quot;, round(sd(naive.means), 3)) ## [1] &quot;standard error of naive estimates: 0.084&quot; "],["propensity-score.html", "3.3 Propensity score", " 3.3 Propensity score Previously we discussed the setting of a discrete feature in which case we estimate group-wise ATEs and use the weighted average to obtain an overall ATE estimate. When there are many features (covariates), this approach is prone to the curse of dimensionality.6 Moreover, if features are continuous, we won’t be able to estimate ATE at each value of \\(x \\in \\chi\\) due to lack of enough sample size. Instead of estimating group-wise ATE and averaging them, we would want to use a more indirect approach. This is when propensity score comes in. The implicit assumption is that we have collected enough features (discrete, continuous, interaction terms, higher degree polynomials) to back unconfoundedness. This again means that the treatment assignment is as good as random after controlling for \\(X_i\\). More formally, this us back to equation (3.3). But in actuality we are not interested in splitting groups to estimate group-wise treatment effects in the case when covariates are continuous and there are many characteristics determining the treatment assignment. Propensity score: \\(e(x)\\). The probability of being treated given a set of covariates \\(X\\)s. \\[\\begin{equation} e(x) = P(W_i = 1 | X_i = x) \\tag{3.7} \\end{equation}\\] The key property of the propensity score is that it balances units in the treatment and control groups. If unconfoundedness assumption holds, we can write the following: \\[\\begin{equation} W_i \\perp \\{Y_i(0), \\; Y_i(1)\\} | \\; e(X_i) \\tag{3.8} \\end{equation}\\] What equation (3.8) says is that instead of controlling for \\(X\\) one can control for the probability of treatment \\((e(X))\\) to establish the desired property that the treatment is as good as random. The propensity scores are mainly used for balancing purposes. One straight-forward implication of equation (3.8) is that if we partition observations into groups with similar propensity score then we can estimate group-wise treatment effects and aggregate them to form an estimate for ATE. This can be done using the propensity score stratification method. The argument here is that when units with similar propensity scores are compared, the covariates are approximately balanced, mimicking a randomized experiment. As the number of covariates increases the domain space shrinks quite rapidly making it infeasible to estimate ATE within the given domain due to thinning out data.↩︎ "],["estimation-of-propensity-score.html", "3.4 Estimation of propensity score", " 3.4 Estimation of propensity score Propensity scores can be estimated using various statistical or machine learning models. We will first estimate propensity score using a logistic regression model, where the treatment assignment \\(W\\) is regressed on the covariates \\(X\\). Next, we will estimate propensity score using random forest model built within the GRF framework in Athey et al.  Logistic Regression Using a linear regression framework to predict probabilities when the outcome is binary \\(\\{0, \\; 1\\}\\) falls short since the predicted values can go beyond 0 and 1. Many models contain values within the range of 0 and 1, which can be used to model a binary response. The logistic regression uses a logistic function given as: \\[\\begin{equation} p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p}} \\tag{3.9} \\end{equation}\\] It is easy to see that \\(lim_{a \\rightarrow - \\inf}[\\frac{e^a}{1+e^a}] = 0\\) and \\(lim_{a \\rightarrow \\inf}[\\frac{e^a}{1+e^a}] = 1\\). Equation @ref{eq:logit} can be transformed using the logit transformation given as: \\[\\begin{equation} g(X) = ln[\\frac{p(X)}{1-p(X)}] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p \\end{equation}\\] We want to fit a logistic regression in order to predict the probability. For now, we will use simulated data. # helper packages library(dplyr) # data wrangling library(ggplot2) # plots library(rsample) # data splitting library(tidyr) # for reshaping, pivot_wider # Modeling package library(caret) # for logistic regression modeling # Model interpretability library(vip) set.seed(194) # for replicability # Generate simulated Data n &lt;- 2000 # number of obsevations p &lt;- 10 # number of covariates X &lt;- matrix(rnorm(n * p), n, p) # data matrix true_effect &lt;- 2.5 W &lt;- rbinom(n, 1, 0.1 + 0.4 * (X[, 1] &gt; 0) + 0.2 * (X[, 2] &gt; 0)) prob &lt;- 0.1 + 0.4 * (X[, 1] &gt; 0) + 0.2 * (X[, 2] &gt; 0) # oracle propensity score Y &lt;- true_effect * W + X[, 2] + pmax(X[, 1], 0) + rnorm(n) #plot(X[, 1], X[, 2], col = as.factor(W)) dat &lt;- data.frame(cbind(W, Y, X)) colnames(dat) &lt;- c(&quot;W&quot;, &quot;Y&quot;, paste0(&quot;X&quot;, seq(1, 10))) dat &lt;- dat %&gt;% mutate(W = as.factor(W)) # create 70% training and 30% test data churn_split &lt;- initial_split(dat, prop = 0.7) dat_train &lt;- training(churn_split) dat_test &lt;- testing(churn_split) # dimension of training and testing data print(dim(dat_train)) ## [1] 1400 12 print(dim(dat_test)) ## [1] 600 12 # let&#39;s compare two different models # using X1 as the predictor cv_model1 &lt;- train( W ~ X1 + X2 + X3 + X4, data = dat_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # misses out on X1 cv_model2 &lt;- train( W ~ X2 + X3 + X4, data = dat_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # print the sample performance measures sum_performance &lt;- summary( resamples( list( model1 &lt;- cv_model1, model2 &lt;- cv_model2 ) ) ) sum_performance$statistics$Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Model1 0.6043165 0.6446429 0.6678571 0.6706292 0.6964286 0.7285714 0 ## Model2 0.5642857 0.5785714 0.5892392 0.5935621 0.6160714 0.6285714 0 # use the confusion matrix # predict class threshold &lt;- 0.5 pred_prob &lt;- predict(cv_model1, dat_train, type = &quot;prob&quot;) pred_class_manual &lt;- rep(0, 1400) pred_class_manual[pred_prob[, 2] &gt;= 0.5] &lt;- 1 pred_class &lt;- predict(cv_model1, dat_train) # print the confusion matrix confusionMatrix( data = relevel(pred_class, ref = &quot;1&quot;), # predictions reference = relevel(dat_train$W, ref = &quot;1&quot;) # reference or the true value ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 270 151 ## 0 299 680 ## ## Accuracy : 0.6786 ## 95% CI : (0.6534, 0.703) ## No Information Rate : 0.5936 ## P-Value [Acc &gt; NIR] : 3.164e-11 ## ## Kappa : 0.3053 ## ## Mcnemar&#39;s Test P-Value : 4.219e-12 ## ## Sensitivity : 0.4745 ## Specificity : 0.8183 ## Pos Pred Value : 0.6413 ## Neg Pred Value : 0.6946 ## Prevalence : 0.4064 ## Detection Rate : 0.1929 ## Detection Prevalence : 0.3007 ## Balanced Accuracy : 0.6464 ## ## &#39;Positive&#39; Class : 1 ## # if predict all yes still get an accuracy of 0.5936 table(dat_train$W) %&gt;% prop.table() ## ## 0 1 ## 0.5935714 0.4064286 Looking at the confusion matrix, the values on the downward diagonal ([1, 1] and [2, 2] in matrix) are correctly idenfified by the model, while the upward diagonal values ([2, 1] and [1, 2]) are incorrectly classified. If all of the observations were assigned the value of 0, the accuracy would still be 0.5936%. This is termed as the no information rate. The model performs quite well in predicting True Negatives (classify as 0, when the value is actually 0). However, it does not perform so well in classifying the True Positives – more than 50% of the positive cases are classified as negative. Next, two measures of importance are sensitivity and specificity. The sensitivity measure tracks the true positive rate from the model, while the specificity measure tracks the true negative rate. \\(sensitivity = \\frac{True \\; positives}{True \\; positives + False \\; negatives} = 0.8790\\). \\(specificity = \\frac{True \\; negatives}{True \\; negatives \\; + \\; False \\; positives} = \\frac{604}{604 + 85} = 0.8766\\). How are the observations classified? A threshold value is used to transform the raw prediction of probabilities into classification such that \\(P(Y_{i} &gt; p_{threshold})=1.\\) The implicit \\(p_{threshold}\\) used is 0.5. Varying the threshold from 0 to 1, one can calculate the relationship between the False Positive Rate (the prediction is positive when in actual the outcome is negative) and True Positive Rate at each threshold value. If the threshold value \\((p_{threshold})\\) is 1, then all observations are classified as 0, which means that the False Positive Rate is 0 but so is the True Positive Rate. Similarly, if the threshold is 0, then both True and False positive rates are 1. This gives the Receiver Operating Characteristic (ROC). library(ROCR) # compute probabilities m1_prob &lt;- predict(cv_model1, dat_train, type = &quot;prob&quot;)[, 2] m2_prob &lt;- predict(cv_model2, dat_train, type = &quot;prob&quot;)[, 2] # AUC metrics perf1 &lt;- prediction(m1_prob, dat_train$W) %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) perf2 &lt;- prediction(m2_prob, dat_train$W) %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) # plot ROC curves plot(perf1, col = &quot;red&quot;) plot(perf2, add = TRUE, col = &quot;green&quot;) legend(0.8, 0.2, legend = c(&quot;cv_model1&quot;, &quot;cv_model2&quot;), lty = c(1,1), col = c(&quot;red&quot;, &quot;green&quot;), cex = 0.6) Figure 3.1: ROC The figure above plots the ROC for two models that we tested using cross-validation. The cv_model2 produces a diagonal line, which means that this model is as good as a random guess. Next, cv_model1 performs a whole lot better since a large gains in True positive rate can be achieved with a relatively small increase in False positive rate at the start. The ROC curve pertaining to cv_model1 helps pick a threshold to balance the sensitivity (True Positive Rate) and specificity (1 - False Positive Rate). The histogram of the estimated propensity scores using the logistic regression is as: hist(pred_prob[, 2], main = &quot;Histogram of P(W=1|X) \\n using Logistic Regression&quot;, xlab = &quot;probabilities&quot;) Now, let’s take a look at the confusion matrix using the test data. pred_class_test &lt;- predict(cv_model1, dat_test) # print the confusion matrix this time for the test sample confusionMatrix( data = relevel(pred_class_test, ref = &quot;1&quot;), # classification from the prediction reference = relevel(dat_test$W, ref = &quot;1&quot;) # ground truth ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 94 69 ## 0 131 306 ## ## Accuracy : 0.6667 ## 95% CI : (0.6274, 0.7043) ## No Information Rate : 0.625 ## P-Value [Acc &gt; NIR] : 0.01882 ## ## Kappa : 0.2474 ## ## Mcnemar&#39;s Test P-Value : 1.608e-05 ## ## Sensitivity : 0.4178 ## Specificity : 0.8160 ## Pos Pred Value : 0.5767 ## Neg Pred Value : 0.7002 ## Prevalence : 0.3750 ## Detection Rate : 0.1567 ## Detection Prevalence : 0.2717 ## Balanced Accuracy : 0.6169 ## ## &#39;Positive&#39; Class : 1 ## The measures of accuracy, sensitivity, and specificity are similar for both the training and testing sample. "],["using-cross-fitting-to-predict-propensity-score.html", "3.5 Using cross-fitting to predict propensity score", " 3.5 Using cross-fitting to predict propensity score Here, we will be using 10-fold cross-folding to predict propensity score. fun_probit_predict &lt;- function(predictfold){ # @Arg predictfold: number of the fold to avoid for model traning # but used for prediction cv_model1 &lt;- train( W ~ X1 + X2 + X3 + X4, data = dat[-predictfold, ], method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) predict_logit &lt;- predict(cv_model1, dat[predictfold, ], type = &quot;prob&quot;) return(predict_logit[, 2]) } ############################## # # cross-fitting # ############################## k &lt;- 10 # number of folds len &lt;- nrow(dat) ind &lt;- sample(1:len, replace = FALSE, size = len) fold &lt;- cut(1:len, breaks = k, labels = FALSE) # create 10 folds fold &lt;- fold[ind] # randomly allocate the folds by ind # container to store the predicted values store &lt;- c() true_index &lt;- c() # do the cross-fitting and store for(i in 1:k){ # which(fold == i) is used as an index, if 8th observation receives the 1st fold for the first time, # then the 1st prediction value corresponds to the 8th obs store_new &lt;- fun_probit_predict(predictfold = which(fold == i)) store_new &lt;- as.numeric(as.character(store_new)) true_index_new &lt;- which(fold == i) store &lt;- c(store, store_new) true_index &lt;- c(true_index, true_index_new) } # create a dataframe with index that maps the predictions with the actual data store &lt;- data.frame(pscore = store, index = true_index) # sort by index store &lt;- store[order(store[, 2]), ] # propensity score dat &lt;- dat %&gt;% mutate(pscore = store$pscore) # histogram of propensity score hist(dat$pscore, main = &quot;propensity score \\n from cross-fitting&quot;) "],["propensity-score-stratification.html", "3.6 Propensity score stratification", " 3.6 Propensity score stratification Propensity scores are super important as they can be used in various different approaches to enchance the validity of causal inference in observational settings. These include but are not limited to inverse probability weighting, matching estimates, weight adjustments in regression (for better balancing), trimming, and propensity score stratification. These methods will be discussed in detail as we move on with the course. First, let’s take a look at propensity score stratification to get a gist of how propensity scores contribute in comparing treatment units with control units. The simple idea is given by the cliché that we want to compare oranges with oranges and not apples. To bring focus back into our context, it simply means that it is no good comparing a treated unit with an extremely high probability of receiving the treatment with a control unit with super low probability of receiving the treatment. But what if (yes, what if) we compare units with similar treatment probabilities? Let’s run a quick thought experiment. We run the logistic regression and estimate the propensity score. Say, we have two units, each from the treatment and control group, with the propensity score of 0.6. The assumption here is, conditional on the similar propensity score, the treatment assignment is random. This follows from the unconfoundedness assumption: \\(Y_i^{0}, \\; Y_i^{1} \\; \\perp \\; W_i \\; | X_i\\). Propensity score stratification divides the estimates of propensity scores into several segments and estimates the ATE within each segment. Finally, these segment-specific ATE estimates are averaged to obtain the overall estimate of ATE. Steps for ATE estimation using propensity score stratification Order observations according to their estimated propensity score. \\(\\hat{e}(X)_{i1}, \\; \\hat{e}(X)_{i2}, ... \\; \\hat{e}(X)_{iN}\\) Form \\(J\\) strata of equal size and take the simple difference in mean between the treated and control units within each strata. These are \\(\\hat{\\tau}_j\\) for \\(j = \\{1, \\; 2, \\; ..., \\; N\\}\\). Form the ATE, \\(\\hat{\\tau}_{Strat} = \\frac{1}{J} \\sum_{j = 1}^{J} \\hat{\\tau}_j\\) Here, \\(\\hat{\\tau}_{Strat}\\) is consistent for \\(\\tau\\), meaning that \\(\\hat{\\tau}_{Strat} \\rightarrow_p \\tau\\) given that \\(\\hat{e}(x)\\) is consistent for \\(e(x)\\) and the number of strata grows appropriately with \\(N\\). However, one needs to set the number of strata, which can be a bit ad-hoc. Demo of propensity score stratification # order data by the propensity score: low to high dat &lt;- dat[order(dat$pscore), ] # cut to form ventiles strata &lt;- cut(dat$pscore, breaks = quantile(dat$pscore, seq(0, 1, 0.05)), labels = 1:20, include.lowest = TRUE) dat &lt;- dat %&gt;% mutate(strata = strata) # compare across strata dat_sum &lt;- dat %&gt;% group_by(W, strata) %&gt;% summarize(mean_Y = mean(Y)) %&gt;% pivot_wider(names_from = W, values_from = mean_Y) ## `summarise()` has grouped output by &#39;W&#39;. You can override using the `.groups` argument. colnames(dat_sum) &lt;- c(&quot;strata&quot;, &quot;mean_control&quot;, &quot;mean_treat&quot;) dat_sum &lt;- dat_sum %&gt;% mutate(diff = mean_treat - mean_control) print(paste(&quot;ATE Estimation from propensity score stratification is: &quot;, mean(dat_sum$diff), sep = &quot;&quot;)) ## [1] &quot;ATE Estimation from propensity score stratification is: 2.50135397640408&quot; print(paste(&quot;raw difference is :&quot;, mean(dat$Y[dat$W == 1]) - mean(dat$Y[dat$W == 0]), sep = &quot;&quot;)) ## [1] &quot;raw difference is :3.04098842187519&quot; print(paste(&quot;And the true treatment effect is :&quot;, true_effect, sep = &quot;&quot;)) ## [1] &quot;And the true treatment effect is :2.5&quot; We see that the estimate from stratification gets closer to the true effect compared to the mean difference estimator. Looks like given that we know and observe what variables determine the treatment assignment, propensity score stratification approach performs well in estimating the ATE. "],["inverse-probability-weighting-ipw.html", "3.7 Inverse Probability Weighting (IPW)", " 3.7 Inverse Probability Weighting (IPW) A more natural way to exploit the condition of unconfoundedness is to weight observations by their propensity score, which is known as the inverse probability weighting. As before \\(\\hat{e}(x)\\) is defined as an estimated propensity score. \\[\\begin{equation} \\hat{\\tau}_{IPW} = \\frac{1}{N}\\sum_{i = 1}^{N} \\Bigg(\\frac{Y_i . W_i}{\\hat{e}(X_i)} - \\frac{Y_i . (1-W_i)}{1 - \\hat{e}(X_i)}\\Bigg) \\tag{3.10} \\end{equation}\\] Intuitively, observations with high propensity score within the treated group are weighted down, while observations with higher propensity score in the control group are weighted more. In this way, propensity score is used to balance the differences in covariates across the treatment and control groups. Note that the validity of \\(\\hat{\\tau}\\) still hinges on the unconfoundedness assumption. Any inference that you make is only good if your assumption holds. Limitation of IPW Estimate. One way to analyze the accuracy of \\(\\hat{\\tau}_{IPW}\\) is to compare it with the oracle IPW estimate, \\(\\hat{\\tau}_{IPW}^{*}\\). The oracle estimate is obtained from the known propensity score. Briefly, comparison between \\(\\hat{\\tau}_{IPW}^{*}\\) and \\(\\hat{\\tau}_{AGG}\\) suggests that the oracle IPW under-performs \\(\\hat{\\tau}_{AGG}\\). In other words, the variance of the oracle estimate is larger than that of \\(\\hat{\\tau}_{AGG}\\). Algorithmically, we can form score as: \\((\\frac{Y_i \\times W_i}{\\hat{e}(X_i)} - \\frac{Y_i \\times (1-W_i)}{1 - \\hat{e}(X_i)})\\) The mean of it results to \\(\\hat{\\tau}\\) and the standard error of the estimate is simply \\(\\frac{\\hat{\\sigma}_{score}}{\\sqrt{N}}\\). Estimating IPW. In the example below we will simulate a dataset where the treatment assignment is made to be correlated with the outcome. This means that the independence assumption does not hold. However, since this is a simulated data, we know exactly what covariates influence the treatment assignment. Hence, we can invoke the unconfoundedness assumption. We estimate the propensity score using random forest based on honest splitting. For this, we use GRF package from . Note that \\(e(x)\\) is estimated via cross-fitting. The data is divided into \\(K\\)-folds. For each fold \\(k\\), model building is administered using \\(-k\\) folds. Using Step 2, predictions are generated for units in the \\(k^{th}\\) fold. Steps 2 and 3 are repeated until all \\(K\\) folds are exhausted. Estimation The following example uses 10 fold cross-fitting. ################################# # Author: VS # Last Revised: Jan 16, 2024 # Keywords: IPW, AIPW, GRF # # # ################################# set.seed(194) # Generate Data n &lt;- 2000 p &lt;- 10 true_effect &lt;- 15 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) prob &lt;- 1 / (1 + exp(- (X[, 1] &gt; 1 + rnorm(n)))) W &lt;- rbinom(n, 1, prob) Y &lt;- 60 + true_effect * W + 5 * pmax(X[, 1], 0) + rnorm(n) plot(X[, 1], X[, 2], col = as.factor(W)) #paste0(&quot;average treatment effect is: &quot;, round(mean(pmax(X[, 1], 0)), 3)) ################################# ################################# # # Inverse Probability Weighting # ################################# ################################# # use the random forest to get the propensity score dat &lt;- data.frame(W, X, Y) n_features &lt;- length(setdiff(names(dat), &quot;W&quot;)) # A. ranger (probability tree) rf1_ranger &lt;- ranger( W ~ ., data = dat, mtry = min(ceiling(sqrt(n_features) + 20), n_features), num.trees = 2000, probability = TRUE ) # OOB predictions from ranger p.ranger &lt;- rf1_ranger$predictions[, 1] # B. probability tree using GRF # cross-fitting index K &lt;- 10 ind &lt;- sample(1:length(W), replace = FALSE, size = length(W)) folds &lt;- cut(1:length(W), breaks = K, labels = FALSE) index &lt;- matrix(0, nrow = length(ind) / K, ncol = K) for(f in 1:K){ index[, f] &lt;- ind[which(folds == f)] } # Build RF using GRF P(W = 1 | X) fun.rf.grf &lt;- function(X, W, predictkfold){ rf_grf &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) p.grf &lt;- predict(rf_grf, predictkfold)$predictions return(p.grf) } # storing predict.mat &lt;- matrix(0, nrow = nrow(index), ncol = K) tauk &lt;- rep(0, K) tauk_oracle &lt;- rep(0, K) weighttau &lt;- rep(0, K) score &lt;- list() score_oracle &lt;- list() # for each fold i use other folds for estimation for(i in seq(1:K)){ predict.mat[, i] &lt;- fun.rf.grf(X = X[c(index[, -i]), ], W = W[index[, -i]], predictkfold = X[c(index[, i]), ]) # fold-specific treatment effect score[[i]] &lt;- ((W[index[, i]] * Y[index[, i]]) / (predict.mat[, i])) - (((1 - W[index[, i]]) * Y[index[, i]]) / (1 - predict.mat[, i])) tauk[i] &lt;- mean(score[[i]]) } # ipw using oracle propensity score and propensity score estimated from grf alpha &lt;- 0.05 # 5 percent level of significance #ipw.ranger &lt;- mean(((W * Y) / (p.ranger)) - (((1 - W) * Y) / (1 - p.ranger))) ipw.grf &lt;- mean(unlist(score)) score_oracle &lt;- ((W * Y) / (prob)) - ((1 - W) * Y / (1 - prob)) ipw.oracle &lt;- mean(score_oracle) sd.ipw &lt;- sd(unlist(score)) sd.oracle &lt;- sd(score_oracle) ll &lt;- ipw.grf - (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ul &lt;- ipw.grf + (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ll_oracle &lt;- ipw.oracle - (sd.oracle / sqrt(length(score_oracle))) * qnorm(1 - alpha/2) ul_oracle &lt;- ipw.oracle + (sd.oracle / sqrt(length(score_oracle))) * qnorm(1 - alpha/2) result.ipw &lt;- c(&quot;IPW estimate&quot; = round(ipw.grf, 3), &quot;se&quot; = round(sd.ipw / (sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll, 3), &quot;upper bound&quot; = round(ul, 3)) result.oracle.ipw &lt;- c(&quot;IPW Oracle estimate&quot; = round(ipw.oracle, 3), &quot;se&quot; = round(sd.oracle / (sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll_oracle, 3), &quot;upper bound&quot; = round(ul_oracle, 3)) print(result.ipw) ## IPW estimate se lower bound upper bound ## 15.508 3.092 9.448 21.568 print(result.oracle.ipw) ## IPW Oracle estimate se lower bound upper bound ## 16.496 3.165 10.294 22.699 What? Despite having true propensity score, the Oracle IPW underperforms in accuracy compared to the IPW estimate with unknown propensity score. Why is it so? "],["comparing-ipw-with-aggregated-estimate.html", "3.8 Comparing IPW with Aggregated Estimate", " 3.8 Comparing IPW with Aggregated Estimate We know that the probablity of the treatment increases with \\(X1\\), as shown below. plot(X[, 1], prob) par(new = T) abline(v = 0, col = &quot;red&quot;, lty = &quot;dashed&quot;) Next, we would want to divide \\(X1\\) into segments such that the probability of treatment remains more or less similar in each segment. Since we generated the data ourselves, we know that the probability of treatment increases for observations with \\(X1 &gt; 0\\). However, lets take 10 segments, which cuts the distribution of \\(X1\\) in decile. quant &lt;- quantile(X[, 1], p = seq(0, 1, 0.1)) group &lt;- cut(X[, 1], quant, include.lowest = TRUE) dat$group &lt;- group Next, we want to estimate the treatment effects for each segment and then aggregate it. dat_sum &lt;- data.frame(dat %&gt;% mutate(status = ifelse(W == 1, &quot;treatment&quot;, &quot;control&quot;)) %&gt;% group_by(group) %&gt;% summarize(num_treat = sum(W), n_group = n())) %&gt;% mutate(prop_treat = num_treat / n_group) %&gt;% dplyr::select(c(group, prop_treat)) dat &lt;- dat %&gt;% merge(dat_sum, by = &quot;group&quot;, all.x = T) %&gt;% mutate(score = Y * ((W / prop_treat) - ((1 - W) / (1 - prop_treat))) ) paste(&quot;aggregated treatment effect is: &quot;, round(mean(dat$score), 3)) ## [1] &quot;aggregated treatment effect is: 15.019&quot; paste(&quot;se of aggregated treatment effect is: &quot;, round(sd(dat$score) / sqrt(length(W)), 3)) ## [1] &quot;se of aggregated treatment effect is: 3.122&quot; Now, let’s estimate the oracle IPW with known propensity score. score_oracle &lt;- ((W * Y) / (prob)) - ((1 - W) * Y / (1 - prob)) ipw.oracle &lt;- mean(score_oracle) se.oracle &lt;- sd(score_oracle) / sqrt(length(score_oracle)) paste(&quot;oracle IPW estimate: &quot;, round(ipw.oracle, 3)) ## [1] &quot;oracle IPW estimate: 16.496&quot; paste(&quot;standard error: &quot;, round(se.oracle, 3)) ## [1] &quot;standard error: 3.165&quot; "],["aipw-and-estimation.html", "3.9 AIPW and Estimation", " 3.9 AIPW and Estimation Augmented Inverse Probability Weighting (AIPW) provides a robust way to estimate ATE by alleviating the limitation of IPW estimate. Following the IPW approach, estimation of ATE is given in equation (6). The other approach to estimate \\(\\tau\\) is to think of it from the conditional response approach. Write \\(\\mu_{w}(x) = E[Y_i| \\; X_i = x, W_i = w]\\). Then: \\(\\tau(x) = E[Y_i| \\; X_i = x, W_i = 1] - E[Y_i| \\; X_i = x, W_i = 0]\\) This is the regression outcome approach, where \\(\\tau = E[\\mu_{1}(x) - \\mu_{0}(x)]\\). The consistent estimator can be formed by using: \\(\\hat{\\tau}(x) = N^{-1} \\sum_{i = 1}^{N} \\mu_{1}(X_i) - \\mu_{0}(X_i)\\). AIPW approach combines both IPW approach as well as regression outcome approach to estimate \\(\\tau\\). \\(\\hat{\\tau}_{AIPW} = \\frac{1}{N} \\sum_{i = 1}^{N} (\\mu_{1}(X_i) - \\mu_{0}(X_i) + \\frac{(Y_i - \\hat{\\mu}_1(X_i)). W_i}{\\hat{e}(X_i)} - \\frac{(Y_i - \\hat{\\mu}_0(X_i)). (1-W_i)}{1 - \\hat{e}(X_i)})\\) ML approach using cross-fitting is used to estimate both \\(\\hat{e}(x)\\) and \\(\\hat{\\mu}_{w}(x)\\). Following the cross-fitting structure, we can formally write the estimate for \\(\\tau\\) as: \\(\\hat{\\tau}_{AIPW} = \\lowerbracket{\\frac{1}{N} \\sum_{i = 1}^{N} (\\mu_{1}^{-k(i)}(X_i) - \\mu_{0}^{-k(i)}(X_i)}_{consistent \\; estimate \\; of \\; \\tau} + \\frac{(Y_i - \\hat{\\mu}_1^{-k(i)}(X_i)). W_i}{\\hat{e}^{-k(i)}(X_i)} - \\frac{(Y_i - \\hat{\\mu}_0^{-k(i)}(X_i)). (1-W_i)}{1 - \\hat{e}^{-k(i)}(X_i)})\\) The AIPW approach can be thought of estimating ATE taking the difference across conditional responses. Next, the residuals are adjusted using weights given by the propensity score. There are two attractive features of AIPW estimate. First, $_{AIPW} is consistent as long as \\(\\hat{e}(x)\\) or \\(\\hat{\\mu}_{w}(x)\\) is consistent. This is because \\(E[(Y_i - \\hat{\\mu}_{W_i}(X_i)) \\approx 0\\). Second, \\(\\hat{\\tau}_{AIPW}\\) is a good approximation to oracle \\(\\hat{\\tau}_{AIPW}^{*}\\) as long as \\(\\hat{\\mu}(.)\\) and \\(\\hat{e}(.)\\) are reasonably accurate. If one estimate is highly accurate, then it can compensate lack of accuracy on the other estimate. If both \\(\\hat{\\mu}(.)\\) and \\(\\hat{e}(.)\\) are \\(\\sqrt{n}\\)-consistent7, then the following holds. \\(\\sqrt{n}(\\hat{\\tau}_{AIPW} - \\hat{\\tau}_{AIPW}^{*}) \\rightarrow_p 0\\). ####################### # # Augmented IPW (aipw) # ####################### #n_features2 &lt;- length(setdiff(names(dat2), &quot;Y&quot;)) # ranger #funrf_ranger &lt;- function(dat){ # rf2 &lt;- ranger( # Y ~ ., # data = dat, # mtry = min(ceiling(sqrt(n_features) + 20), n_features), # respect.unordered.factors = &quot;order&quot;, # seed = 123, # num.trees = 2000 # ) # return(rf2) #} # storing predict.mat2a &lt;- matrix(0, nrow = nrow(index), ncol = K) predict.mat2b &lt;- predict.mat2a aipwK &lt;- rep(0, K) weightaipK &lt;- rep(nrow(index) / length(index), K) for(i in seq(1:K)){ # E(Y | X, W = 1) using cross-fitting predict.mat2a[, i] &lt;- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], predictkfold = cbind(X[c(index[, i]), ], 1)) # E(Y | X, W = 0) using cross-fitting predict.mat2b[, i] &lt;- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], predictkfold = cbind(X[c(index[, i]), ], 0)) noise &lt;- ((W[index[, i]] * (Y[index[, i]] - predict.mat2a[, i])) / (predict.mat[, i])) - (((1 - W[index[, i]]) * (Y[index[, i]] - predict.mat2b)) / (1 - predict.mat[, i])) score[[i]] &lt;- predict.mat2a[, i] - predict.mat2b[, i] + noise aipwK[i] &lt;- mean(score[[i]]) } aipw.grf &lt;- weighted.mean(aipwK, weights = weightaipK) sd.aipw &lt;- sd(unlist(score)) ll &lt;- aipw.grf - (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ul &lt;- aipw.grf + (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) result.aipw &lt;- c(&quot;AIPW Est.&quot; = round(aipw.grf, 3), &quot;se&quot; = round(sd.aipw/(sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll, 3), &quot;upper bound&quot; = round(ul, 3)) ###################### # grf ###################### # Train a causal forest tau.forest &lt;- causal_forest(X, Y, W) # Estimate the conditional average treatment effect on the full sample (CATE). grf_ate &lt;- average_treatment_effect(tau.forest, target.sample = &quot;all&quot;) grf_att &lt;- average_treatment_effect(tau.forest, target.sample = &quot;treated&quot;) ## PRINT ALL #print(paste0(&quot;average treatment effect is: &quot;, round(mean(pmax(X[, 1], 0)), 3))) print(paste(&quot;treatment effects according to naive estimator:&quot;, round(mean(Y[which(W == 1)]) - mean(Y[which( W == 0)]), 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to naive estimator: 15.558&quot; print(paste(&quot;treatment effects according to IPW using&quot;, K, &quot;fold cross-fittin:&quot;, round(ipw.grf, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to IPW using 10 fold cross-fittin: 15.508&quot; print(paste(&quot;treatment effects according to IPW oracle:&quot;, round(ipw.oracle, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to IPW oracle: 16.496&quot; print(paste(&quot;treatment effects according to AIPW using&quot;, K, &quot;fold cross-fitting:&quot;, round(aipw.grf, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to AIPW using 10 fold cross-fitting: 15.037&quot; print(paste(&quot;treatment effects according to GRF:&quot;, round(grf_ate[[1]], 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to GRF: 15.031&quot; print(result.ipw) ## IPW estimate se lower bound upper bound ## 15.508 3.092 9.448 21.568 print(result.aipw) ## AIPW Est. se lower bound upper bound ## 15.037 0.031 14.975 15.098 print(grf_ate) ## estimate std.err ## 15.03144837 0.05209014 This means that \\(\\hat{\\mu}(.)\\) converges to \\(\\hat{\\mu}\\) at the ↩︎ "],["assessing-balance.html", "3.10 Assessing Balance", " 3.10 Assessing Balance ########################################## # # # Assessing Balance # # ########################################## XX &lt;- X[c(index), ] YY &lt;- Y[c(index)] WW &lt;- W[c(index)] e.hat &lt;- c(predict.mat) # unadjusted means.treat &lt;- apply(XX[WW == 1, ], 2, mean) means.control &lt;- apply(XX[WW == 0, ], 2, mean) abs.mean.diff &lt;- abs(means.treat - means.control) var.treat &lt;- apply(XX[WW == 1, ], 2, var) var.control &lt;- apply(XX[WW == 0, ], 2, var) std &lt;- sqrt(var.treat + var.control) # adjusted means.treat.adj &lt;- apply(XX*WW / e.hat, 2, mean) means.control.adj &lt;- apply(XX*(1 - WW) / (1 - e.hat), 2, mean) abs.mean.diff.adj &lt;- abs(means.treat.adj - means.control.adj) var.treat.adj &lt;- apply(XX * WW / e.hat, 2, var) var.control.adj &lt;- apply(XX * (1 - WW) / (1 - e.hat), 2, var) std.adj &lt;- sqrt(var.treat.adj + var.control.adj) # plot unadjusted and adjusted differences par(oma=c(0,4,0,0)) plot(-2, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, xlim=c(-.01, 1.01), ylim=c(0, ncol(XX)+1), main=&quot;&quot;) axis(side=1, at=c(-1, 0, 1), las=1) lines(abs.mean.diff / std, seq(1, ncol(XX)), type=&quot;p&quot;, col=&quot;blue&quot;, pch=19) lines(abs.mean.diff.adj / std.adj, seq(1, ncol(XX)), type=&quot;p&quot;, col=&quot;orange&quot;, pch=19) legend(&quot;topright&quot;, c(&quot;Unadjusted&quot;, &quot;Adjusted&quot;), col=c(&quot;blue&quot;, &quot;orange&quot;), pch=19) abline(v = seq(0, 1, by=.25), lty = 2, col = &quot;grey&quot;, lwd=.5) abline(h = 1:ncol(XX), lty = 2, col = &quot;grey&quot;, lwd=.5) mtext(paste0(&quot;X&quot;, seq(1, ncol(XX))), side=2, cex=0.7, at=1:ncol(XX), padj=.4, adj=1, col=&quot;black&quot;, las=1, line=.3) abline(v = 0) hist(e.hat, breaks = 100, freq = FALSE) "],["cross-fitting.html", "3.11 Cross-fitting", " 3.11 Cross-fitting What is cross-fitting? Divide the data into K folds randomly. Train the model using \\(-k\\) folds (all folds except the \\(k^{th}\\) one). Generate a fit of fold k on the model trained using \\(-k\\) folds Repeat steps 2 and 3 to generate fit for all \\(K\\) number of folds. This is illustrated using the figure below. The data is randomly divided into 5 folds (segments). This is an example of a five-fold cross-fitting. In the first round, the blue segments are used for model building, while responses are constructed for observations in the green segment of the data. Next, we move into the second round and so on; again the blue segments are used for model building and responses are constructed for the green segment. In this way, each observation is used for model building. # cross-fitting illustration colorcode &lt;- diag(5) # this creates a coding colorcode &lt;- c(colorcode) # Create data for the boxes boxes &lt;- data.frame( x = rep(seq(2, 10, 2), 5), y = rep(seq(5, 1, by = -1), each = 5), label = rep(paste(&quot;fold&quot;, seq(1, 5), sep = &quot; &quot;), 5), colorcode = colorcode ) boxes &lt;- boxes %&gt;% mutate(fill = ifelse(colorcode == 1, &quot;lightgreen&quot;, &quot;lightblue&quot;)) %&gt;% dplyr::select(-c(colorcode)) # Create the plot ggplot() + geom_rect(data = boxes, aes(xmin = x , xmax = x + 2, ymin = y - 0.3, ymax = y + 0.5, fill = fill), color = &quot;black&quot;, alpha = 0.5) + xlim(0, 14) + ylim(-1, 6) + theme_void() + scale_fill_identity() + annotate(&quot;text&quot;, x = c(seq(3, 11, 2), rep(0.5, 5)), y = c(rep(0.3, 5), seq(5, 1, -1)), label = c(paste(&quot;fold&quot;, seq(1, 5, 1), sep = &quot; &quot;), paste(&quot;round&quot;, seq(1, 5, 1), sep = &quot; &quot;)), color = rep(c(&quot;red&quot;, &quot;black&quot;), each = 5) ) What does it do? Simply put, cross-fitting assures that the same observations are not used for modeling building as well as to estimate the response (e.g., predictions). In this way, we would want to alleviate concerns of over-fitting. "],["difference-in-differences.html", "4 Difference in Differences ", " 4 Difference in Differences "],["a-quick-introduction.html", "4.1 A Quick Introduction", " 4.1 A Quick Introduction Let’s consider that there are two groups: group \\(i)\\) receives the treatment and group \\(ii)\\) does not receive the treatment. We term group \\(i)\\) as the treatment group and group \\(ii)\\) as the control or untreated group. We want to compare the two groups in the pre-treatment period versus the post-treatment period. While doing so we will be utilizing both within and across variation in outcomes between the two groups. "],["set-up.html", "4.2 Set up", " 4.2 Set up For example, you observe units A and B in periods 1 and 2. Unit A is treated in period 2, whereas unit B does not receive treatment. The variation in treatment between periods 1 and 2 is the within variation in treatment. The comparison in treatment across units A and B is the across variation. Goal. We would want to compare the difference in average outcomes between two periods for unit A with the difference in average outcomes between the two periods pertaining to unit B. "],["an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html", "4.3 An example: Evaluating the impact of Medicaid expansion on uninsured rate", " 4.3 An example: Evaluating the impact of Medicaid expansion on uninsured rate Let’s take a concrete example. Say, we are interested in evaluating the impacts of ACA-Medicaid expansion on insurance outcomes. Following the supreme court decision in 2012 that deemed Medicaid expansion voluntary, 26 states expanded Medicaid in 2014, while the rest did not. In fact, 19 states did not expand Medicaid until 2018. Data for this example comes from one of my projects and can be downloaded from github. We are looking at the county-level data, where our outcome variable is the uninsured rate. We want to evaluate the ATT. What is the effect of ACA-Medicaid expansion reform on uninsured rates? We will draw our attention to the states that expanded Medicaid in 2014 plus the 19 states that did not expand Medicaid until 2018. States that expanded Medicaid between 2014 and 2018 are dropped from the sample. This is to avoid the case of bad comparison, an issue that arises from comparing early treated units with later treated units. We will reflect on this topic later. For now, let’s take a quick look at the data. user = 2 if(user == 1){ source(&quot;/home/user1/Dropbox/Medicaid_South/code/filepath.r&quot;) }else{ source(&quot;/Users/vshrestha/Dropbox/Medicaid_South/code/filepath.r&quot;) } library(pacman) p_load(fixest, dplyr, ggplot2, tidyverse, patchwork, arrow) # load in county level uninsured rate data merged with other variables mort_allcauses &lt;- read_feather( file.path(datapath, &quot;NVSS_data_county_2010to2017_merged_allcauses.feather&quot;)) %&gt;% mutate(treat = ifelse(is.na(treat) == T, &quot;control 3&quot;, treat)) %&gt;% filter(yearexpand == 2014 &amp; age == 0 &amp; race_name == &quot;white&quot;) %&gt;% dplyr::select(&quot;countyfips&quot;, &quot;year&quot;, &quot;state.abb&quot;, &quot;expand&quot;, &quot;yearexpand&quot;, &quot;sahieunins138&quot;, &quot;GovernorisDemocrat1Yes&quot;) %&gt;% filter(duplicated(.)) %&gt;% arrange(countyfips, year) # sort by countyfips and year # the select() function is masked # by other packages, so use dplyr::select() instead # only keep the years 2013 and 2014 for the canonical case dat_canonical &lt;- mort_allcauses %&gt;% filter(year %in% c(2013, 2014)) head(dat_canonical) ## # A tibble: 6 × 7 ## countyfips year state.abb expand yearexpand sahieunins138 GovernorisDemocrat1Yes ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1001 2013 AL 0 2014 39.6 0 ## 2 1001 2014 AL 0 2014 31.9 0 ## 3 1003 2013 AL 0 2014 45.1 0 ## 4 1003 2014 AL 0 2014 43.8 0 ## 5 1005 2013 AL 0 2014 37.3 0 ## 6 1005 2014 AL 0 2014 34 0 dat_canonical %&gt;% tabyl(state.abb, expand) ## state.abb 0 1 ## AL 134 0 ## AR 0 150 ## AZ 0 30 ## CA 0 115 ## CO 0 126 ## CT 0 16 ## DE 0 6 ## FL 134 0 ## GA 318 0 ## HI 0 8 ## IA 0 198 ## ID 88 0 ## IL 0 204 ## KS 210 0 ## KY 0 240 ## MA 0 28 ## MD 0 48 ## ME 32 0 ## MI 0 166 ## MN 0 174 ## MO 230 0 ## MS 163 0 ## NC 200 0 ## ND 0 106 ## NE 185 0 ## NH 0 20 ## NJ 0 42 ## NM 0 66 ## NV 0 33 ## NY 0 124 ## OH 0 176 ## OK 154 0 ## OR 0 72 ## RI 0 10 ## SC 92 0 ## SD 131 0 ## TN 190 0 ## TX 507 0 ## UT 58 0 ## VA 264 0 ## VT 0 28 ## WA 0 78 ## WI 144 0 ## WV 0 110 ## WY 46 0 cat(&quot;The expansion states are: \\n&quot;) ## The expansion states are: table(dat_canonical$state.abb[dat_canonical$expand == 1]) ## ## AR AZ CA CO CT DE HI IA IL KY MA MD MI MN ND NH NJ NM NV NY OH OR RI VT WA WV ## 150 30 115 126 16 6 8 198 204 240 28 48 166 174 106 20 42 66 33 124 176 72 10 28 78 110 cat(&quot;The non-expansion states are: \\n&quot;) ## The non-expansion states are: table(dat_canonical$state.abb[dat_canonical$expand == 0]) ## ## AL FL GA ID KS ME MO MS NC NE OK SC SD TN TX UT VA WI WY ## 134 134 318 88 210 32 230 163 200 185 154 92 131 190 507 58 264 144 46 length(table(dat_canonical$state.abb[dat_canonical$expand == 1])) ## [1] 26 length(table(dat_canonical$state.abb[dat_canonical$expand == 0])) ## [1] 19 The two groups are as follows: Expansion states (treated): AR, AZ, CA, CO, CT, DE, HI, IA, IL, KY, MA, MD, MI, MN, ND, NH, NJ, NM, NV, NY, OH, OR, RI, VT, WA, WV Non-expansion states (control): AL, FL, GA, ID, KS, ME, MO, MS, NC, NE, OK, SC, SD, TN, TX, UT, VA, WI, WY "],["naive-estimator.html", "4.4 Naive estimator", " 4.4 Naive estimator A naive estimate of ATT would just be the difference in means between the treated and control groups in the period following the expansion. naive &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year &gt;= 2014]) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year &gt;= 2014]) print(naive) ## [1] -13.65797 The naive estimate suggests that uninsured rate dropped by -13.66 percentage points following the Medicaid expansion in 2014. But can we trust this estimate? Not really! Here are some reasons why the naive estimate fails: One way to assess the validity of naive estimate is to compare the (natural) experiment on hand with the randomized control case. Note that we are very far away from the randomized controlled trial in this case. The treatment (decision to expand Medicaid) is not random. Note that states voluntarily decided to expand Medicaid. This means that expansion versus non-expansion states may be very different in terms of pre-treatment characteristics. For example, many of the southern states did not expand Medicaid. Also, pre-treatment uninsured rates of southern states are generally higher compared to non-southern states. The naive comparison can simply be capturing the difference in pre-treatment characteristics correlated with the treatment assignment. The baseline outcome among the treatment group may differ significantly from the control group. For example, southern states have higher population of Blacks compared to non-South. Typically, uninsured rate is higher among Blacks. Hence, it is difficult to disentagle the influence of democraphic composition versus Medicaid expansion. Let’s evaluate the difference in uninsured rate between the expansion versus the non-expansion states in the pre-treatment year (2013). naive_pre &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year &lt; 2014]) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year &lt; 2014]) print(naive_pre) ## [1] -7.844546 Note that treatment units on average have 7.68 percentage points lower uninsured rate compared to the control units even prior to the treatment. Hence, the naive estimator captures the pre-existing differerences in outcome; something that we don’t want. Since the treatment is not randomized a lot of baseline characteristics that influence the outcome across the treated and control groups may differ dramatically. If these variables are thought to influence the dynamics of the outcome variable, then we will be capturing the influence of such variables rather than the treatment itself. "],["canonical-difference-in-differences-framework.html", "4.5 Canonical Difference in Differences Framework", " 4.5 Canonical Difference in Differences Framework We would like to alleviate the aforementioned concerns. One way to address the second concern, i.e., outcomes in pre-treatment period may differ significantly between the treatment and control groups, is to take out the mean difference in outcome during the pre-treatment period from the mean difference in outcome post treatment. This approach uses two groups and two periods, which is termed as the canonical DiD case. The canonical DiD can be seen using a \\(2\\times 2\\) matrix. # Create the data for the 2x2 matrix data &lt;- data.frame( group = rep(c(&quot;Control&quot;, &quot;Treated&quot;), each = 2), time = rep(c(&quot;Pre&quot;, &quot;Post&quot;), times = 2), outcome = c(5, 5, 5, 7), # Example outcomes label = c(&quot;Y_11&quot;, &quot;Y_01&quot;, &quot;Y_10&quot;, &quot;Y_00&quot;) # Labels for matrix cells ) # Base plot ggplot(data, aes(x = time, y = group)) + geom_tile(fill = &quot;lightblue&quot;, color = &quot;black&quot;) + # Create the matrix grid geom_text(aes(label = label), size = 6, fontface = &quot;bold&quot;) + # Add cell labels annotate(&quot;text&quot;, x = 1, y = 2.55, label = &quot;Pre-Treatment&quot;, size = 5, fontface = &quot;italic&quot;) + annotate(&quot;text&quot;, x = 2, y = 2.55, label = &quot;Post-Treatment&quot;, size = 5, fontface = &quot;italic&quot;) + annotate(&quot;text&quot;, x = 0.45, y = 2, label = &quot;Control Group&quot;, angle = 90, size = 5, fontface = &quot;italic&quot;) + annotate(&quot;text&quot;, x = 0.45, y = 1, label = &quot;Treated Group&quot;, angle = 90, size = 5, fontface = &quot;italic&quot;) + labs( title = &quot;2x2 Difference-in-Differences Matrix Illustration&quot;, x = NULL, y = NULL ) + theme_minimal() + theme( axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank(), plot.title = element_text(hjust = 0.5, size = 16) ) Note that the naive estimator simply is: \\(E(Y_{11}) - E(Y_{01})\\). This can be considered as the first difference. Next, we construct the second difference across the two groups during the pre-treatment period as: \\(E(Y_{10}) - E(Y_{00})\\). The difference-in-differences estimate: $_{did} = \\(E[Y_{11} - Y_{01}] - E[Y_{10} - Y_{00}]\\). This defines the term “difference-in-differences” as it involves two differences in means across the treatment and control group; one post treatment and the other prior to the treatment. In the ACA-Medicaid expansion example that involves two groups and two time periods: cat(&quot;did estimate= \\n&quot;, naive - naive_pre) ## did estimate= ## -5.813426 This suggests that uninsured rate dropped by 5.81 percentage points following the Medicaid expansion in year 2014. Let’s formally visit the DiD approach to appreciate some necessary assumptions while connecting it with ATT. The ATT is given as: \\[\\begin{equation} \\tau = E(Y^1(1) - Y^0(1)| D = 1) \\tag{4.1} \\end{equation}\\] Here, \\(Y^1(1)\\) is the outcome following the treatment, and \\(Y^0(1)\\) is the counterfactual, i.e., the outcome without the treatment. \\(E(Y^0(1)| D = 1)\\), the conditional mean outcome of the treatment group in absence of the treatment, is not revealed as it is the counterfactual. Our job still remains to come up with a valid counterfactual. The validity of the difference-in-differences estimate rest on the parallel trend assumption. Let’s consider the canonical case of DiD with two groups (treatment &amp; control) and two periods (before and after treatment). Formally the parallel trend assumption (PTA) is given as: \\[\\begin{equation} E(Y^0(1) - Y^0(0)| D = 1) = E(Y^0(1) - Y^0(0)| D = 0) \\tag{4.2} \\end{equation}\\] Here, we have written the parallel trend assumption using the potential outcomes. \\(Y^0(1)\\) is the potential outcome in the post-treatment period in absence of the treatment, \\(Y^0(0)\\) is the potential outcome in pre-treatment period. All forms of potential outcomes are revealed (observed) except \\(E(Y^0(1) | D = 1)\\), which is average outcome for the treated group in the post-treatment period in absence of the treatment. The parallel trend assumption states that, in absence of the treatment, outcomes for the treatment and control groups after the treatment would follow similar trend to that of the pre-treatment period. How does the parallel trend assumption help in identifying the ATT \\((\\delta)\\)? To see this, lets expand equation @ref{eq:att}. \\[ \\begin{align} \\delta = E(Y^1(1)| D = 1) - E(Y^0(1)| D = 1) \\\\ = E(Y^1(1)| D = 1) - E(Y^0(1)| D = 1) + E(Y^0(0)| D = 1) - E(Y^0(0)| D = 1) \\\\ = \\{E(Y^1(1)| D = 1) - E(Y^0(0)| D = 1)\\} - \\{E(Y^0(1)| D = 1) - E(Y^0(0)| D = 1)\\} \\\\ = \\{E(Y^1(1)| D = 1) - E(Y^0(0)| D = 1)\\} - \\{E(Y^0(1)| D = 0) - E(Y^0(0)| D = 0)\\} \\\\ = \\{E(Y(1)| D = 1) - E(Y(0)| D = 1)\\} - \\{E(Y(1)| D = 0) - E(Y(0)| D = 0)\\} \\end{align} \\] Moving from line 3 to 4 makes use of the parallel trend assumption as given in equation @ref{eq:ptrend1}. So it turns out that under the parallel trend assumption, DiD framework uncovers the ATT. However, the parallel trend assumption cannot be exactly tested since it is impossible to observe the potential outcome of the treated group in absence of the treatment. If we cannot provide a feasible test for the parallel trend assumption, how can we then attest for the validity of the DiD estimate? "],["did-in-multi-period-set-up.html", "4.6 DiD in multi-period set up", " 4.6 DiD in multi-period set up So far we have only considered the canonical DiD (two groups and two period DiD framework). However, data may be available for several periods before and after the treatment implementation. In this case, we will be in the setting of multi-period DiD. For example, for the Medicaid expansion case, we have data spanning from years 2010 to 2017. We’d like to utilize all of these years rather than just years 2013 and 2014 as in the canonical DiD setting. Using the multi-period set up can be helpful for the following reasons: We can trace the dynamic effects of the treatment following the implementation. This means that we can trace the ATT estimate of Medicaid expansion for periods following its implementation, i.e., in 2014, 2015, 2016, and 2017. If the effects of expansion are increasing over time, then this setting will be very helpful in uncovering such dynamics. In other words, we can trace the potential heterogeneous effects of the treatment over time. Next, the multi-period setting allows us to way to check the validity of the parallel trend assumption. As mentioned above, the exact test for the parallel trend assumption is infeasible. However, we can deduce suggestive evidence to assess validity of this assumption. See below. Let’s first plot the mean uninsured rate between the years 2010 and 2017 across the treatment versus the control groups. # estimate the mean uninsured rate by expansion status and year dat_sum &lt;- mort_allcauses %&gt;% group_by(year, expand) %&gt;% summarize(uninsured = mean(sahieunins138)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. post_cf_treat &lt;- dat_sum$uninsured[dat_sum$expand == 0] + naive_pre dat_treat_cf &lt;- data.frame(year = seq(2010, 2017, 1), expand = rep(1, 8), cf = post_cf_treat) dat_sum &lt;- dat_sum %&gt;% left_join(dat_treat_cf, by = c(&quot;year&quot;, &quot;expand&quot;)) f_uninsured &lt;- ggplot(dat_sum, aes(x = year, y = uninsured, group = as.factor(expand), color = as.factor(expand)), shape = as.factor(expand)) + geom_point() + geom_line() + theme_minimal() + geom_vline(xintercept = 2014, linetype = &quot;dashed&quot;) ggplot(dat_sum, aes(x = year, y = uninsured, group = as.factor(expand), color = as.factor(expand)), shape = as.factor(expand)) + geom_point() + geom_line() + theme_minimal() + geom_vline(xintercept = 2014, linetype = &quot;dashed&quot;) + geom_line(data = dat_sum, aes(y = cf), linetype = &quot;dashed&quot;, color = &quot;lightblue&quot;, linewidth = 1) ## Warning: Removed 8 rows containing missing values or values outside the scale range (`geom_line()`). The plot shows that uninsured rate trended parallelly between the treated and control groups prior to the expansion. The rates dropped following the expansion year (2014) across both the treated and control groups, but the magnitude of drop in uninsured rate is higher for the treated group.8 The DiD uses the control group as the counterfactual for the treated group. In other words, we are assuming that in absence of the treatment, outcome for the treated group would have evolved similarly to that of the control group. This is the key assumption of DiD – the parallel trend assumption. Note that, once we difference out the pre-treatment means between the treated and controls groups, we obtain the dashed line, which is the counterfactual for the treated group. In this case, the counterfactual line overlaps the average outcome for the treated group, suggesting that outcome was trending similarly across the two groups during the pre-treatment period. Loosely speaking, ATT is the average gap between the outcome for the treated group (solid blue line) and the counterfactual outcome (dashed line) during the post-treatment period. As previously mentioned, we are unable to provide an actual test for parallel trend assumption due to the missing data on treated observations following the treatment in a state when treatment is absent. This does not mean that we are absolutely helpless. There are a list of things that can be done to provide suggestive evidence in favor of or lack of parallel trend. They are: Check pre-treatment summary statistics across treatment and control groups. Say, treatment and control groups have highly different pre-treatment characteristics. It is unlikely that parallel trend assumption will hold in this case. It is because pre-treatment variables that influence the outcome can induce different dynamics in trends. Hence, the outcome trends between the treated and control groups may vary even in absence of the treatment. Usually trends in outcomes across treated and control groups are assessed to evaluate parallel trends. If outcome is trending parallely between the two groups prior to the treatment, then it provides evidence in favor of PTA. However, concluding that PTA does not hold in a case of non-parallel trend is a narrow assessment at the best, since this approach merely depicts unconditional trends. PTA may have more leverage once (pre-treatment) covariates are accounted for. The next approach that has been used widely is the event-study. We will discuss this approach soon. In most cases, unconditional parallel trend assumption may not be very convincing. Why? The drop in uninsured rate for the control group can be explained by the implementation of other aspects of ACA such as subsidies and employment mandate.↩︎ "],["conditional-parallel-trend-assumption.html", "4.7 Conditional Parallel Trend Assumption", " 4.7 Conditional Parallel Trend Assumption We have discussed the parallel trend assumption already. This is the unconditional parallel trend assumption, which assumes that the parallel trend assumption holds without accounting for any covariates. One way to bolster the credibility of the parallel trend assumption is to claim that it only holds conditional on covariates. For example, since the decision of whether to expand Medicaid was under the states’ discretion, we may want to condition on a state’s partisan leaning, i.e. Democrat versus Republican Governor. In the canonical model, the parallel trend assumption can be modified to obtain the conditional parallel trend assumption. This means we are assuming that the parallel trend only exists once conditioned on covariates. This is given as below. \\[\\begin{equation} E(Y^0(1) - Y^0(0)| D = 1, X) = E(Y^0(1) - Y^0(0)| D = 0, X) \\tag{4.3} \\end{equation}\\] The conditional parallel trend is easy to understand when covariates required are small in number and if covariates are discrete. For example, say, in the case of ACA-Medicaid example, the parallel trend assumption holds once accounting for state’s partisan leaning in 2013 (pre-treatment). What does this mean exactly? This would be equivalent to running two different DiD estimates: one for the group with Democrat Governor and the other for the Republican governor. This gives two DiD estimates. Next, we average these two estimates to get the DiD estimate for the whole sample. GovernorisDemocrat1Yes is a variable that indicates whether a state’s governor is of the Democrat party. # we need to first get the state partisan data for 2013 and merge with the main data # note that it is recommended to conduct all data cleaning in separate files. # we are doing this in a fly, so I break that rule. dat_gov2013 &lt;- dat_canonical %&gt;% filter(year == 2013) %&gt;% dplyr::select(c(countyfips, GovernorisDemocrat1Yes)) %&gt;% rename(GovernorisDemocrat1Yes2013 = GovernorisDemocrat1Yes) dat_canonical &lt;- dat_canonical %&gt;% merge(dat_gov2013, by = c(&quot;countyfips&quot;), all.x = TRUE) # get the first and second differences for states with democrat governor in 2013 first_diff_dem &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year == 2014 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 1], na.rm = T) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year == 2014 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 1], na.rm = T) second_diff_dem &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year == 2013 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 1], na.rm = T) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year == 2013 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 1], na.rm = T) # get the first and second differences for the state with the republican state in 2013 first_diff_rep &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year == 2014 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 0], na.rm = T) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year == 2014 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 0], na.rm = T) second_diff_rep &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year == 2013 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 0], na.rm = T) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year == 2013 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 0], na.rm = T) did_dem &lt;- first_diff_dem - second_diff_dem did_rep &lt;- first_diff_rep - second_diff_rep # fraction with Democrat Governor gov_dem &lt;- mean(dat_canonical$GovernorisDemocrat1Yes2013, na.rm = T) # weighted average did_estimate &lt;- (gov_dem * did_dem) + (1 - gov_dem) * did_rep # conditional DiD estimate cat(&quot;did estimate (controling for state partisan leaning in 2013) = \\n&quot;, did_estimate) ## did estimate (controling for state partisan leaning in 2013) = ## -5.091879 # unconditional DiD estimate (note: we already have this from previous estimation) cat(&quot;did estimate (no controls) = \\n&quot;, naive - naive_pre) ## did estimate (no controls) = ## -5.813426 What we observe is that DiD estimate that accounts for a state’s political leaning in 2013 is slightly lower than the unconditional version. The difference is not too large in this case. However, I’d still be inclined to trust the conditional DiD estimate. Why so? This is because the condition forces comparison across the treatment vs. control units with the same political leaning in 2013. For example, treated units under the Democrat regime are compared to control units also under the Democrat regime. The same goes with treated and control units falling under the Republican Governor in 2013. Since, ACA was highly politicized and also since Democrat vs. Republican states are quite different in socio-economic factors, we need to make sure that we are comparing units with similar political leaning. In other words, we are conducting relatively more similar comparison. "],["some-concerns-with-controls.html", "4.8 Some concerns with controls", " 4.8 Some concerns with controls Ok, we sort of argued that conditional DiD may perform better in the real world. Like everything, this does not come easily. Here are some concerns regarding including covariates. First of all, we don’t always clearly know what to control for in the real world. I previously made the case that a state’s political leaning is an important variable and one should account for it. However, there might be other variables that I’m completely missing out. We can use economic reasoning, past studies in the literature, as well as data based methods (e.g., double lasso for variable selection) to decide on controls. There are good controls and there are bad controls. Let’s say you think its important to improve comparability between the treated and control units. To do so, you pick income in 2014 as a control. This, I’d argue, is an example of a bad control. Why? Its because income in 2014 might be affected by the ACA-Medicaid expansion, which can lead to bias in the estimate of interest. We’d want to make sure that the controls are not directly affected by the reform itself. This is why mostly researchers rely on pre-treatment variables rather than post-treatment variables as controls. Earlier, we looked at the case of a binary variable (Republican vs. Democrat Governor in 2013) as a covariate. However, in this setting, if the number of control increases, then the sample space thins out fairly quickly. Say, we add the following binary controls: \\(i)\\) urban|rural, \\(ii)\\) south|non-south, \\(iii)\\) high|low uninsured unit based on 2013 (baseline) uninsured rates. Here, we’d have \\(2^{4}\\) different splitting of the sample. If you decide to add in more controls, the number of subgroups will increase exponentially. Note that we’ve only considered binary controls so far. This issue worsens if you add in continuous variables as controls. This is known as the curse of dimensionality. There are several ways to avoid this curse. One relatively less taxing approach is to incorporate controls in the regression format. However, this leads to its own issues. Firstly, the covariates are being linearly incorporated in the regression, which leads to a linear functional form assumption. Second, if the effect of the reform varies along the covariate, then this might lead to a bias on the estimate of interest. Hence, we’d want to incorporate controls in a more flexible way using the inverse probability weighting for DiD or Doubly Robust framework tailored for DiD. But more on this later! "],["the-2-times-2-difference-in-differences-estimate.html", "4.9 The \\(2 \\times 2\\) Difference-in-Differences Estimate", " 4.9 The \\(2 \\times 2\\) Difference-in-Differences Estimate Now that we’ve disscussed the difference-in-differences framework (mostly canonical DiD), we would like to look at various ways of estimating DiD. As we’ve seen, (a canonical) DiD estimation can simply be done non-parametrically by using the appropriate differences in means. Another way to do it is by using the regression framework, which provides several advantages including but not limited to: \\(i)\\) estimation of standard errors; \\(ii)\\) ease of accounting for necessary covariates; and \\(iii)\\) feasible estimation of DiD with multiple treatment groups with staggered treatment. Let’s begin with the canonical DiD framework using the regression format. I’m going to set it up as the following: \\[\\begin{equation} \\label{eq:DiD_reg} Y_{it} = \\alpha + \\tau Post_{it} \\times D_{i} + \\sigma Post_{it} + \\eta D_{i} + \\epsilon_{it} \\end{equation}\\] Let’s rewrite the DiD estimator from before as: \\(\\tau_{did} = \\underbrace{E[Y_{11} - Y_{10} | D = 1]}_{first\\; difference} - \\underbrace{E[Y_{01} - Y_{00} | D = 0]}_{second\\; difference}\\) We’d want to see whether estimation of \\(\\tau\\) in the regression above uncovers the DiD estimate. Let’s look at the following conditional expectations. 1). expected outcome for treated group post treatment: \\(E(Y | D = 1, Post = 1) = \\alpha + \\tau + \\sigma + \\eta\\) 2). expected outcome for treated group pre treatment: \\(E(Y | D = 1, Post = 0) = \\alpha + \\eta\\) 3). expected outcome for control group post treatment: \\(E(Y | D = 0, Post = 1) = \\alpha + \\sigma\\) 4). expected outcome for control group pre treatment: \\(E(Y | D = 0, Post = 0) = \\alpha\\) If you do the math, the first difference is given as \\(\\tau + \\sigma\\) and the second difference is \\(\\sigma\\). The DiD – the difference between the first and the second difference – is \\(\\tau\\). Hence, the estimation of \\(\\tau\\), in the regression framework above, is the DiD estimate. Let’s apply this to our ACA-Medicaid example. # lets create the post, treat, and the interaction between the post and treat (labeled as did) dat_canonical &lt;- dat_canonical %&gt;% mutate(post = ifelse(year &gt;= 2014, 1, 0), treat = ifelse(expand == 1, 1, 0), did = post * treat) reg_did &lt;- lm(sahieunins138 ~ did + post + treat, data = dat_canonical) summary(reg_did) ## ## Call: ## lm(formula = sahieunins138 ~ did + post + treat, data = dat_canonical) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.2196 -5.1221 -0.7641 4.7804 26.7359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.4641 0.1890 219.41 &lt;2e-16 *** ## did -5.8134 0.4126 -14.09 &lt;2e-16 *** ## post -4.7840 0.2673 -17.89 &lt;2e-16 *** ## treat -7.8445 0.2918 -26.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.655 on 5650 degrees of freedom ## Multiple R-squared: 0.425, Adjusted R-squared: 0.4247 ## F-statistic: 1392 on 3 and 5650 DF, p-value: &lt; 2.2e-16 did &lt;- naive - naive_pre print(did) ## [1] -5.813426 Note that the difference in mean did and the regression coefficient on the interaction between \\(Post \\times treat\\) are the same. Plus, we get the standard errors as well. Of course, the standard errors are not adjusted for clustering. The errors of observations within a cluster (the ground at which the treatment is implemented, in our case state) will be correlated, so we need to account for it. The way to do it is to cluster the standard error at the state level. This can be done using the fixest library and feols command. reg_did_cluster &lt;- feols(sahieunins138 ~ did + post + treat, data = dat_canonical, cluster = ~state.abb) summary(reg_did_cluster) ## OLS estimation, Dep. Var.: sahieunins138 ## Observations: 5,654 ## Standard-errors: Clustered (state.abb) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.46411 2.041183 20.31376 &lt; 2.2e-16 *** ## did -5.81343 1.266824 -4.58898 3.6981e-05 *** ## post -4.78400 0.278769 -17.16117 &lt; 2.2e-16 *** ## treat -7.84455 2.612190 -3.00305 4.3962e-03 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 7.65276 Adj. R2: 0.424736 Note that the did coefficient remains unchanged. However, the standard error is inflated after clustering the errors at the state level. "],["event-study-model.html", "4.10 Event study model", " 4.10 Event study model As we have discussed, the validity of DiD estimate depends on the parallel trend assumption. Since the direct test for parallel trend requires the potential outcome of treated unit in absence of the treatment, it is not feasible. Although we are not going to be able to directly test the parallel trend, we can provide suggestive evidence in favor of (or lack of) parallel trend. For this, we need multi-period data, particularly for periods prior to the implementation of the treatment. A simple way to assess parallel trend is to evaluate the unconditional means across the treated and untreated units (expansion and non-expansion states in our case). This is shown in Section 4 (DiD in multi-period set up). I’ve included the figure here. f_uninsured Here, we see that the uninsured rates between the expansion and non-expansion states trended similarly (or parallely) prior to the treatment. This allows us to argue that trends in uninsured rate would’ve remained similar (or would not differ systematically) in absence of the ACA-Medicaid reform. This by far is the simplest but yet powerful way to argue parallel trend assumption in practice. However, note that units in non-expansion states on average had higher uninsured rate compared to their counterparts in the expansion states even prior to the reform. Ideally, we would want treated and control units to have similar baseline features. This is where regression comes into play. Using regression, we can evaulate a difference-in-differences model by accounting for necessary covariates.9 Moreover, it allows us to evaluate dynamic effects of the treatment, i.e., the impacts of the treatment over time (1st period, 2nd period, and so on). The event study model can be written as: \\[\\begin{equation} \\label{eq:DiD_eventstudy} Y_{it} = \\alpha + \\underbrace{\\sum_{j = -k}^{k}}_{j \\neq -1} \\tau_j \\times 1(\\underbrace{t - G}_{r} = j) \\times D_i + \\sigma_{t} + \\eta D_{i} + \\epsilon_{it} \\end{equation}\\] So what are these notations here? \\(\\alpha\\) is the intercept \\(1(\\underbrace{t - G}_{r} = j)\\): This is an indicator that turns on (takes the value 1) when the relative time \\(r\\) in the data is equal to \\(j\\), and turns off otherwise (takes the value 0). The relative time, \\(r\\), is simply the difference between the period \\(t\\) and the implementation year of the reform \\(E\\). For simplicity, I have the minimum and maximum of relative time as \\(-k\\) and \\(k\\), respectively. But of course, this can vary in practice. The omitted category is the year before the reform (i.e., when \\(r = -1\\)). We’ll discuss more about the omitted category later on. \\(\\sigma_t\\): Is the time fixed effects. It captures the changes that are common across treatment and control units over time. \\(D_i\\): Is the fixed effects for treated/control units. In practice, treatment and control units can fundamentally differ in several characteristics. Accounting for \\(D_i\\) separately captures the average difference in the outcome between treatment and control units that does not change over time (time invariant). In other words, controling for \\(D_i\\) accounts for time invariant heterogeneity across the treatment vs. control groups. For example, we saw that expansion units on average had lower uninsured rate even prior to the reform compared to the treated units. For instance, this aspect of the difference in outcomes across the two groups is accounted by \\(D_i\\). From a specification perspectice, the main difference between the canonical DiD specification and the event study specification is the incorporation of the term \\(1(\\underbrace{t - G}_{r} = j)\\), which is interacted with the treatment indicator, \\(D_i\\). This allows us to evaluate the effect of the treatment separately for a given period following (or before) the treatment implementation. Such dynamic effects are picked up by \\(\\widehat{\\tau_t}\\). Let’s try and break down whats going on in the event study specification. First, it is important to realize the role of the omitted category. In the event study specification above, I’ve dropped the period prior to the reform. Note that this is essential from a theroretical standpoint, since inclusion of all periods would result to a fully saturated model and create multicollinearity. Dropping the period prior to the treatment implementation uses this period as the relative period. From point 2, we can think of the event-study specification as estimating several DiD type models, where the second difference is fixed and pertains to the omitted period, while the period of interest varies. Let me elaborate on this. To see this, note that when \\(t - G = -1\\), the conditional expectation for the treated group is \\(E(Y | D = 1, t = G - 1) = \\alpha + \\sigma_{(G-1)} + \\eta\\) and and for the control group is: \\(E(Y | D = 0, G-1) = \\alpha + \\sigma_{(G-1)}\\). \\(E(Y | D = 1, G - 1) - E(Y | D = 0, G-1)\\) is synonymous to the second difference: i.e., the difference in conditional means between the treatment and control units in the period before the treatment implementation. For the first difference, let’s look at the relative period \\(t - G = 0\\), the period of the reform implementation. The conditional expectation for the treatment group is: \\(E(Y | D = 1, t = G) = \\alpha + \\tau_0 + \\sigma_{G} + \\eta\\) and that for the control group is: \\(E(Y | D = 0, t = G) = \\alpha + \\sigma_{G}\\). Here, the first difference is: \\(E(Y | D = 1, t = G) - E(Y | D = 0, t = G)\\). The DiD estimand during the period of the reform, \\(r=0\\), is given as: \\[\\begin{equation} \\underbrace{E(Y | D = 1, t = G) - E(Y | D = 0, t = G)}_{first\\; difference} - \\underbrace{E(Y | D = 1, t = G - 1) - E(Y | D = 0, t = G-1)}_{second\\; difference} = \\tau_0 \\end{equation}\\] The DiD estimand for the period following the reform \\((r= 1)\\) is given as: \\[\\begin{equation} \\underbrace{E(Y | D = 1, t = G+1) - E(Y | D = 0, t = G+1)}_{first\\; difference} - \\underbrace{E(Y | D = 1, t = G - 1) - E(Y | D = 0, t = G-1)}_{second\\; difference} = \\tau_1 \\end{equation}\\] Similarly, we can think of the event study model as nesting the DiD estimation pertaining to the relative periods, \\(r=2, \\; 3\\) and so on. This way, the event study specification allows us to estimate period specific treatment effects from relative time period \\(-k\\) to \\(k\\) in relation to the conditional mean difference between the treated and control groups a period prior to the treatment implementation. Following the estimation of the event study model, we will have two sets of estimates: i) \\(\\tau_{-k}, \\; \\tau_{-k+1}, ..., \\; \\tau_{-2}\\), and ii) \\(\\tau_{1}\\), \\(\\tau_{2}\\), …, \\(\\tau_{k}\\). The estimation of \\(\\tau\\)s prior to the treatment allows us to make inference regarding the parallel trend assumption. If \\(\\widehat{\\tau}_j\\) for \\(j&lt;-1\\) is close to zero, then it provides suggestive evidence that the outcomes between the treatment and control units are trending similarly prior to the treatment. Let’s estimate the event study model for the ACA-Medicaid data. Note that we are using the larger data set where year spans from 2010 to 2018. First create the relative time variable. mort_allcauses &lt;- mort_allcauses %&gt;% mutate(yeararound = year - yearexpand) # view table(mort_allcauses$yeararound) ## ## -4 -3 -2 -1 0 1 2 3 ## 2828 2824 2825 2827 2827 2827 2821 2826 Note that the relative time spans from -4 to 3. Since, we are only considering the states that implemented ACA-Medicaid expansion in year 2014, -4 pertains to year 2010, -3 to 2011, and so on. Next, let’s create relative time indicators and interact them with the expansion status. In the model, this pertains to \\(1(\\underbrace{t - G}_{r} = j) \\times D_i\\) part. mort_allcauses &lt;- mort_allcauses %&gt;% mutate(rel_pre4 = ifelse(yeararound == -4, 1, 0), # indicator for r = -4 rel_pre4 = rel_pre4 * expand, # interact the indicator for r = -4 with expansion status rel_pre3 = ifelse(yeararound == -3, 1, 0), rel_pre3 = rel_pre3 * expand, rel_pre2 = ifelse(yeararound == -2, 1, 0), rel_pre2 = rel_pre2 * expand, rel_pre1 = ifelse(yeararound == -1, 1, 0), rel_pre1 = rel_pre1 * expand, rel_post0 = ifelse(yeararound == 0, 1, 0), rel_post0 = rel_post0 * expand, rel_post1 = ifelse(yeararound == 1, 1, 0), rel_post1 = rel_post1 * expand, rel_post2 = ifelse(yeararound == 2, 1, 0), rel_post2 = rel_post2 * expand, rel_post3 = ifelse(yeararound == 3, 1, 0), rel_post3 = rel_post3 * expand) Now thats done, let’s specify the model and estimate it using OLS. # quick look at the data head(mort_allcauses) ## # A tibble: 6 × 16 ## countyfips year state.abb expand yearexpand sahieunins138 GovernorisDemocrat1Yes yeararound rel_pre4 rel_pre3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 2010 AL 0 2014 40.6 0 -4 0 0 ## 2 1001 2011 AL 0 2014 41.6 0 -3 0 0 ## 3 1001 2012 AL 0 2014 39.6 0 -2 0 0 ## 4 1001 2013 AL 0 2014 39.6 0 -1 0 0 ## 5 1001 2014 AL 0 2014 31.9 0 0 0 0 ## 6 1001 2015 AL 0 2014 27.5 0 1 0 0 ## # ℹ 6 more variables: rel_pre2 &lt;dbl&gt;, rel_pre1 &lt;dbl&gt;, rel_post0 &lt;dbl&gt;, rel_post1 &lt;dbl&gt;, rel_post2 &lt;dbl&gt;, ## # rel_post3 &lt;dbl&gt; # specify the event study model reg_es &lt;- lm(sahieunins138 ~ rel_pre4 + rel_pre3 + rel_pre2 + rel_post0 + rel_post1 + rel_post2 + rel_post3 + factor(state.abb) + factor(year), data = mort_allcauses ) # print summary of the results summary(reg_es) ## ## Call: ## lm(formula = sahieunins138 ~ rel_pre4 + rel_pre3 + rel_pre2 + ## rel_post0 + rel_post1 + rel_post2 + rel_post3 + factor(state.abb) + ## factor(year), data = mort_allcauses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.6227 -3.3080 -0.3909 2.8382 22.5153 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.12501 0.24654 166.806 &lt; 2e-16 *** ## rel_pre4 0.43065 0.27124 1.588 0.11236 ## rel_pre3 -0.09744 0.27131 -0.359 0.71949 ## rel_pre2 0.26399 0.27129 0.973 0.33052 ## rel_post0 -5.81879 0.27126 -21.451 &lt; 2e-16 *** ## rel_post1 -7.87920 0.27124 -29.049 &lt; 2e-16 *** ## rel_post2 -7.98776 0.27138 -29.434 &lt; 2e-16 *** ## rel_post3 -8.28500 0.27126 -30.543 &lt; 2e-16 *** ## factor(state.abb)AR 2.56819 0.34884 7.362 1.87e-13 *** ## factor(state.abb)AZ -0.36864 0.53905 -0.684 0.49406 ## factor(state.abb)CA -2.08319 0.36686 -5.678 1.38e-08 *** ## factor(state.abb)CO -0.21062 0.35968 -0.586 0.55817 ## factor(state.abb)CT -10.20489 0.68943 -14.802 &lt; 2e-16 *** ## factor(state.abb)DE -8.94031 1.06539 -8.392 &lt; 2e-16 *** ## factor(state.abb)FL 4.55616 0.30746 14.819 &lt; 2e-16 *** ## factor(state.abb)GA 6.79004 0.25928 26.188 &lt; 2e-16 *** ## factor(state.abb)HI -13.89239 0.93335 -14.884 &lt; 2e-16 *** ## factor(state.abb)IA -8.67819 0.33384 -25.995 &lt; 2e-16 *** ## factor(state.abb)ID 5.41830 0.34531 15.691 &lt; 2e-16 *** ## factor(state.abb)IL -7.21457 0.33243 -21.702 &lt; 2e-16 *** ## factor(state.abb)KS -0.24081 0.27825 -0.865 0.38680 ## factor(state.abb)KY -3.25760 0.32535 -10.013 &lt; 2e-16 *** ## factor(state.abb)MA -21.15043 0.55287 -38.256 &lt; 2e-16 *** ## factor(state.abb)MD -7.38614 0.45980 -16.064 &lt; 2e-16 *** ## factor(state.abb)ME -9.17346 0.49516 -18.526 &lt; 2e-16 *** ## factor(state.abb)MI -5.82545 0.34295 -16.986 &lt; 2e-16 *** ## factor(state.abb)MN -12.83729 0.34039 -37.714 &lt; 2e-16 *** ## factor(state.abb)MO -0.34838 0.27350 -1.274 0.20275 ## factor(state.abb)MS 4.11382 0.29357 14.013 &lt; 2e-16 *** ## factor(state.abb)NC 3.46979 0.28095 12.350 &lt; 2e-16 *** ## factor(state.abb)ND -2.67388 0.37311 -7.166 7.94e-13 *** ## factor(state.abb)NE -3.09839 0.28556 -10.850 &lt; 2e-16 *** ## factor(state.abb)NH -3.10364 0.62940 -4.931 8.23e-07 *** ## factor(state.abb)NJ 1.53171 0.47986 3.192 0.00141 ** ## factor(state.abb)NM 4.53961 0.41884 10.839 &lt; 2e-16 *** ## factor(state.abb)NV 7.92988 0.51812 15.305 &lt; 2e-16 *** ## factor(state.abb)NY -10.64965 0.36130 -29.476 &lt; 2e-16 *** ## factor(state.abb)OH -5.42861 0.33978 -15.977 &lt; 2e-16 *** ## factor(state.abb)OK 7.94963 0.29731 26.739 &lt; 2e-16 *** ## factor(state.abb)OR -1.75628 0.40918 -4.292 1.78e-05 *** ## factor(state.abb)RI -7.25614 0.84428 -8.594 &lt; 2e-16 *** ## factor(state.abb)SC 1.69597 0.34075 4.977 6.50e-07 *** ## factor(state.abb)SD -0.54719 0.30966 -1.767 0.07724 . ## factor(state.abb)TN -2.13244 0.28390 -7.511 6.07e-14 *** ## factor(state.abb)TX 14.46773 0.24455 59.160 &lt; 2e-16 *** ## factor(state.abb)UT -0.19189 0.39555 -0.485 0.62760 ## factor(state.abb)VA -1.06957 0.26702 -4.006 6.21e-05 *** ## factor(state.abb)VT -16.08079 0.55287 -29.086 &lt; 2e-16 *** ## factor(state.abb)WA -1.59608 0.40083 -3.982 6.86e-05 *** ## factor(state.abb)WI -11.97572 0.30207 -39.645 &lt; 2e-16 *** ## factor(state.abb)WV -2.60614 0.37019 -7.040 1.98e-12 *** ## factor(state.abb)WY 3.09814 0.43006 7.204 6.03e-13 *** ## factor(year)2011 -0.83040 0.17588 -4.721 2.36e-06 *** ## factor(year)2012 -2.02063 0.17585 -11.491 &lt; 2e-16 *** ## factor(year)2013 -2.70561 0.17574 -15.395 &lt; 2e-16 *** ## factor(year)2014 -7.48354 0.17580 -42.569 &lt; 2e-16 *** ## factor(year)2015 -11.52848 0.17582 -65.568 &lt; 2e-16 *** ## factor(year)2016 -13.47716 0.17593 -76.605 &lt; 2e-16 *** ## factor(year)2017 -13.09156 0.17585 -74.447 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.033 on 22546 degrees of freedom ## Multiple R-squared: 0.825, Adjusted R-squared: 0.8246 ## F-statistic: 1833 on 58 and 22546 DF, p-value: &lt; 2.2e-16 Let’s cluster the standard error at the state level. I’m going to do this using feols command from fixest package. Using feols you can automatically create the relative time indicators and interact them with the treatment status as in the coding below. reg_es_cluster &lt;- feols(sahieunins138 ~ i(yeararound, expand, ref = -1) | year + state.abb, data = mort_allcauses, cluster = ~state.abb ) summary(reg_es_cluster) ## OLS estimation, Dep. Var.: sahieunins138 ## Observations: 22,605 ## Fixed-effects: year: 8, state.abb: 45 ## Standard-errors: Clustered (state.abb) ## Estimate Std. Error t value Pr(&gt;|t|) ## yeararound::-4:expand 0.430654 0.500730 0.860051 3.9442e-01 ## yeararound::-3:expand -0.097442 0.381948 -0.255117 7.9982e-01 ## yeararound::-2:expand 0.263994 0.326565 0.808399 4.2321e-01 ## yeararound::0:expand -5.818789 1.266136 -4.595706 3.6185e-05 *** ## yeararound::1:expand -7.879198 1.417949 -5.556757 1.5057e-06 *** ## yeararound::2:expand -7.987764 1.421451 -5.619445 1.2195e-06 *** ## yeararound::3:expand -8.284998 1.466376 -5.649981 1.1003e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 5.02671 Adj. R2: 0.824559 ## Within R2: 0.127101 We see that the relative time estimates from 3 and 2 are equal to one another. However, the clustered standard errors are inflated. We can then plot the event study estimates. # Extract coefficients and confidence intervals event_study_results &lt;- fixest::coefplot(reg_es_cluster, main = &quot;Event Study Estimates&quot;) event_study_results[[1]] ## estimate ci_low ci_high estimate_names estimate_names_raw id x ## yeararound::-4:expand 0.43065369 -0.5785019 1.4398093 yeararound::-4:expand yeararound::-4:expand 1 1 ## yeararound::-3:expand -0.09744166 -0.8672076 0.6723243 yeararound::-3:expand yeararound::-3:expand 1 2 ## yeararound::-2:expand 0.26399433 -0.3941533 0.9221419 yeararound::-2:expand yeararound::-2:expand 1 3 ## yeararound::0:expand -5.81878875 -8.3705184 -3.2670591 yeararound::0:expand yeararound::0:expand 1 4 ## yeararound::1:expand -7.87919826 -10.7368868 -5.0215097 yeararound::1:expand yeararound::1:expand 1 5 ## yeararound::2:expand -7.98776439 -10.8525099 -5.1230189 yeararound::2:expand yeararound::2:expand 1 6 ## yeararound::3:expand -8.28499809 -11.2402855 -5.3297107 yeararound::3:expand yeararound::3:expand 1 7 ## y ## yeararound::-4:expand 0.43065369 ## yeararound::-3:expand -0.09744166 ## yeararound::-2:expand 0.26399433 ## yeararound::0:expand -5.81878875 ## yeararound::1:expand -7.87919826 ## yeararound::2:expand -7.98776439 ## yeararound::3:expand -8.28499809 We see that the estimates on \\(\\tau_j\\) for \\(j&lt;-1\\) is close to zero and statistically insignificant at the conventional levels. This provides a suggestive evidence in favor of the parallel trend assumption. However, the estimates following the reform implementation drops drastically, demonstrating the reduction in uninsured rate due to the ACA-Medicaid expansion. Again what is necessary is a subject to debate, which we will stay away from for now.↩︎ "],["two-way-fixed-effect-twfe-revisited.html", "4.11 Two way fixed effect (TWFE) Revisited", " 4.11 Two way fixed effect (TWFE) Revisited We have already seen the TWFE and its importance in accounting for unobserved heterogeneity. The TWFE is heavily linked to the difference-in-differences setting (perhaps mistakenly). However, note that the TWFE estimator is not equal to the DiD estimator unless the treatment effects are homogeneous across both units and time. \\[\\begin{equation} \\label{eq:TWFE} Y_{it} = \\theta_{t} + \\eta_{i} + \\alpha D_{it} + v_{it} \\;.....TWFE \\end{equation}\\] Here, \\(Y_{it}\\) is the outcome of individual \\(i\\) in period \\(t\\) (\\(t \\in \\{1,\\;2,\\;...,\\;T\\}\\)) \\(\\theta_{t}\\) is the time fixed effects; \\(\\eta_{i}\\) is the unit fixed effect \\(D_{it}\\) captures whether individual \\(i\\) is treated in time \\(t\\) Equation above is the TWFE. In two groups and two-period setting, the above equation can be estimated in a number of different ways. Let’s simulate data to look. Assign treatment effect = 20 Data Arrange 1: Demeaning to get rid of \\(\\eta_i\\) from TWFE equation (Within Estimator) Let’s look at the concept behind the within estimator. In the two-period two-group case, TWFE can be written as: \\[\\begin{equation} Y_{i1} = \\theta_{1} + \\eta_{i} + \\alpha D_{i1} + v_{i1} \\nonumber \\\\ Y_{i2} = \\theta_{2} + \\eta_{i} + \\alpha D_{i2} + v_{i2} \\end{equation}\\] where, \\(i\\) is represented by 1 (treatment group) and 0 (untreated group). Adding the sub-equations and dividing by the number of time period \\((T=2)\\) yields: \\[\\begin{equation} \\frac{Y_{i1}+Y_{i2}}{2} = \\frac{\\theta_{1}+\\theta_{2}}{2} + \\frac{2\\eta_{i}}{2} + \\frac{\\alpha (D_{i1}+D_{i2})}{2} + \\frac{v_{i1}+v_{i2}}{2} \\\\ Y_{i} = \\frac{\\theta_{1}+\\theta_{2}}{2} + \\eta_{i} + \\alpha D_{i} + v_{i} \\nonumber \\end{equation}\\] Substracting the above equation from the TWFE yields the following: \\[\\begin{equation} Y_{it}-Y_{i} = \\theta_{t} - \\frac{\\theta_{1}+\\theta_{2}}{2} + \\alpha (D_{it}-D_i) + (v_{it}-v_i) \\end{equation}\\] The code shows data arranging for the within estimator. ########################### # Treatment group ########################### treat_t &lt;- rep(1, 1000) period_t &lt;- rep(c(0, 1), each = 500) id &lt;- rep(seq(1, 500, 1), 2) #for the panel nature of data y_treat &lt;- 20 * period_t + 7 + rnorm(1000, 0, 5) treatdata &lt;- data.frame(treat = treat_t, period = period_t, Y = y_treat, id = id) treatdata &lt;- treatdata %&gt;% mutate(Ytrans = Y - mean(Y), D = treat * period - mean(treat * period)) ########################## # control group ########################## control_t &lt;- rep(0, 1000) period_c &lt;- rep(c(0, 1), each = 500) id &lt;- rep(seq(501, 1000, 1), 2) y_control &lt;- 3 + rnorm(1000, 0, 5) controldata = data.frame(treat = control_t, period = period_c, Y = y_control, id = id) controldata &lt;- controldata %&gt;% mutate(Ytrans = Y - mean(Y), D = treat * period - mean(treat * period)) data = rbind(treatdata, controldata) Data Arrange 2: First differencing Let’s briefly look at the concept behind first differencing. Write TWFE as: \\[\\begin{equation} Y_{i1} = \\theta_{1} + \\eta_{i} + \\alpha D_{i1} + v_{i1} \\nonumber \\\\ Y_{i2} = \\theta_{2} + \\eta_{i} + \\alpha D_{i2} + v_{i2} \\end{equation}\\] for \\(i \\in \\{0,\\;1\\}\\). Then, \\[\\begin{equation} Y_{i2} - Y_{i1} = \\theta_{2} - \\theta_{1} + \\alpha (D_{i2}-D_{i1}) + (v_{i2} - v_{i1}) \\end{equation}\\] The code shows data arranging for the first difference estimator. # First the treated group fd_treat1 &lt;- treatdata %&gt;% filter(period == 0) %&gt;% dplyr::select(-c(&quot;Ytrans&quot;)) colnames(fd_treat1) &lt;- c(&quot;treat1&quot;, &quot;period1&quot;, &quot;Y1&quot;, &quot;id&quot;) fd_treat2 &lt;- treatdata %&gt;% filter(period == 1)%&gt;% dplyr::select(-c(&quot;Ytrans&quot;)) colnames(fd_treat2) &lt;- c(&quot;treat2&quot;, &quot;period2&quot;, &quot;Y2&quot;, &quot;id&quot;) fd_treat &lt;- merge(fd_treat1, fd_treat2, by = &quot;id&quot;, all.x = T) fd_treat &lt;- fd_treat %&gt;% mutate(Y_FD = Y2 - Y1, D = (period2 * treat2) - (period1 * treat1)) # Then the control group fd_control1 &lt;- controldata %&gt;% filter(period == 0) %&gt;% dplyr::select(-c(&quot;Ytrans&quot;)) colnames(fd_control1) &lt;- c(&quot;treat1&quot;, &quot;period1&quot;, &quot;Y1&quot;, &quot;id&quot;) fd_control2 &lt;- controldata %&gt;% filter(period == 1)%&gt;% dplyr::select(-c(&quot;Ytrans&quot;)) colnames(fd_control2) &lt;- c(&quot;treat2&quot;, &quot;period2&quot;, &quot;Y2&quot;, &quot;id&quot;) fd_control &lt;- merge(fd_control1, fd_control2, by = &quot;id&quot;, all.x = T) fd_control &lt;- fd_control %&gt;% mutate(Y_FD = Y2 - Y1, D = (period2 * treat2) - (period1 * treat1)) FDdata = rbind(fd_treat, fd_control) "],["various-ways-of-estimation.html", "4.12 Various ways of estimation", " 4.12 Various ways of estimation Typical Estimation reg1 &lt;- lm(Y ~ treat:period + treat + period, data) summary(reg1) ## ## Call: ## lm(formula = Y ~ treat:period + treat + period, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.830 -3.455 -0.093 3.467 16.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.95751 0.22379 13.216 &lt;2e-16 *** ## treat 4.26159 0.31648 13.466 &lt;2e-16 *** ## period -0.07377 0.31648 -0.233 0.816 ## treat:period 19.95384 0.44757 44.583 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.004 on 1996 degrees of freedom ## Multiple R-squared: 0.8002, Adjusted R-squared: 0.7999 ## F-statistic: 2665 on 3 and 1996 DF, p-value: &lt; 2.2e-16 Within Estimator reg2 &lt;- lm(Ytrans ~ D + period, data) summary(reg2) ## ## Call: ## lm(formula = Ytrans ~ D + period, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.830 -3.455 -0.093 3.467 16.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03689 0.19376 0.190 0.849 ## D 19.95384 0.44746 44.594 &lt;2e-16 *** ## period -0.07377 0.31640 -0.233 0.816 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.003 on 1997 degrees of freedom ## Multiple R-squared: 0.6641, Adjusted R-squared: 0.6637 ## F-statistic: 1974 on 2 and 1997 DF, p-value: &lt; 2.2e-16 First Difference reg3 &lt;- lm(Y_FD ~ D, FDdata) summary(reg3) ## ## Call: ## lm(formula = Y_FD ~ D, data = FDdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.074 -4.372 0.223 4.794 21.751 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.07377 0.31216 -0.236 0.813 ## D 19.95384 0.44146 45.200 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.98 on 998 degrees of freedom ## Multiple R-squared: 0.6718, Adjusted R-squared: 0.6715 ## F-statistic: 2043 on 1 and 998 DF, p-value: &lt; 2.2e-16 Imputation method This traces the counterfactual (untreated potential outcome) of the treated group using paths of the untreated group. Using the first difference (or within transformation): \\[\\begin{equation} Y_{i2}(0) - Y_{i1}(0) = \\theta_{2} - \\theta_{1} + v_{i2}-v_{i1} \\nonumber \\\\ \\Delta Y_{it}(0) = \\theta_t + \\Delta v_{it} \\end{equation}\\] where, \\(\\theta_{t-1}\\) is normalized to 0. Estimate the above equation using only the untreated group and estimate \\(\\hat{\\theta_t}\\). This is the time trend in the untreated group. The parallel trend assumption states that the outcome in treated group would have moved in a similar way to the untreated group in absence of the treatment. So, let’s use \\(\\hat{\\theta_t}\\) to adjust for the pathway in the treated group and find the potential outcome in the treated group in absence of the treatment. Note that \\(\\hat{\\theta_t} = \\frac{1}{n_0}\\sum_1^n (1-D_i) \\Delta Y_{it}\\). \\[\\begin{equation} \\hat{Y_{it}(0)} = Y_{it-1} + \\hat{\\theta_t} \\end{equation}\\] Then write \\(ATT_{imp}\\) as: \\[\\begin{equation} ATT_{imp} = \\frac{1}{n_1}\\sum_{1}^{n}D_{i}(Y_{it}-\\hat{Y_{it}(0)}) \\\\ = \\frac{1}{n_1}\\sum_{1}^{n}D_{i}(Y_{it}-({Y_{it-1}+\\hat{\\theta_{t}}}) ) \\\\ = \\frac{1}{n_1}\\sum_{1}^{n}D_{i}(Y_{it}-({Y_{it-1}}) - \\frac{1}{n_0}\\sum_{1}^{n}(1-D_{i})(Y_{it}-({Y_{it-1}}) \\\\ = \\frac{1}{n_1}\\sum_{1}^{n}D_{i}\\Delta Y_{it} - \\frac{1}{n_0}\\sum_{1}^{n}(1-D_{i}) \\Delta Y_{it} \\end{equation}\\] reg &lt;- lm(Y_FD ~ period2, subset(FDdata, treat1 == 0)) FDdata$yhattreat = FDdata$Y1 + reg[[1]][[1]] FDdata$imp = FDdata$Y2 - FDdata$yhattreat mean(FDdata$imp[FDdata$treat1 == 1]) ## [1] 19.95384 Frisch-Waugh Theorem "],["multi-period-multi-group-and-variation-in-treatment-timing.html", "4.13 Multi Period, Multi Group and Variation in Treatment Timing", " 4.13 Multi Period, Multi Group and Variation in Treatment Timing trueeffect &lt;- 10 intercep &lt;- 10 N &lt;- 5000 T &lt;- 20 early &lt;- 5 late &lt;- 15 datagen &lt;- function(T, N, group){ timeT &lt;- rep(1:T, each = N) treatT &lt;- rep(1, length(timeT)) groupT &lt;- rep(group, length(timeT)) df &lt;- data.frame(time = timeT, treat = treatT, group = groupT) return(df) } dftreat &lt;- datagen(T, N, 1) # early treatment dftreat2 &lt;- datagen(T, N, 2) # late treatment dfuntreat &lt;- datagen(T, N, 3) # untreated data &lt;- rbind(dftreat, dftreat2, dfuntreat) # generating policy variables data &lt;- data %&gt;% mutate(policy = 0, policy = ifelse(group == 1 &amp; time &gt; early, 1, policy), policy = ifelse(group == 2 &amp; time &gt; late, 1, policy), dumtreat1 = ifelse(group == 1, 1, 0), dumtreat2 = ifelse(group == 2, 1, 0), dumtreat3 = ifelse(group == 3, 1, 0)) e &lt;- rnorm(nrow(data), 0, 5) data &lt;- data %&gt;% mutate(Y = 1 + trueeffect*dumtreat1*policy + trueeffect*dumtreat2*policy + time + dumtreat1*2 + dumtreat2*4 + e, Ypot = 1 + 1*dumtreat1*policy + 1*dumtreat2*policy + time + dumtreat1*2 + dumtreat2*4 + e) datasum &lt;- data.frame(data %&gt;% group_by(group, time) %&gt;% summarise(meanY = mean(Y), meanYpot = mean(Ypot)) ) ## `summarise()` has grouped output by &#39;group&#39;. You can override using the `.groups` argument. # data for potential outcome datasumpot &lt;- datasum %&gt;% dplyr::select(c(group, time, meanYpot)) %&gt;% mutate(group = 10*group) datasum &lt;- datasum %&gt;% dplyr::select(-c(meanYpot)) colnames(datasumpot) &lt;- c(&quot;group&quot;, &quot;time&quot;, &quot;meanY&quot;) datasum &lt;- rbind(datasum, datasumpot) vlines &lt;- data.frame(xint = c(6, 16)) datasum$group = factor(datasum$group) ggplot(datasum, aes(x = time, y = meanY, group = group)) + geom_line(aes(linetype = group, color = group),size = 2) + #geom_point(aes(shape = group, size = 3)) + scale_linetype_manual(name = &quot;Linetype&quot;,values = c(&quot;1&quot; = 1, &quot;2&quot; = 1, &quot;3&quot; = 1, &quot;10&quot; = 2, &quot;20&quot; = 2, &quot;30&quot; = 2), guide = &quot;none&quot;) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(color = &quot;black&quot;), legend.position = &quot;none&quot;) + xlab(&quot;period&quot;) + ylab(&quot;Outcome&quot;) + geom_vline(data = vlines, aes(xintercept = xint), linetype = &quot;dashed&quot;) + annotate(x = c(5.2, 15.2, 2, 9, 17), y = c(0, 0, 35, 35, 35), label = c(bquote(t[k]^&quot;*&quot;), bquote(t[l]^&quot;*&quot;), &quot;PRE(k)&quot;, &quot;MID(k,l)&quot;, &quot;POST(l)&quot;), geom = &quot;text&quot;, parse = TRUE) The figure above depicts the case of three groups: \\(i)\\) early treated (treatment starting from the \\(6^{th}\\) period, \\(t_{k}^{*}\\)); \\(ii)\\) late treated (\\(16^{th}\\) period, \\(t_{k}^{*}\\)); and untreated group. The figure provides an example of staggered treatment adoption. This means that units are treated at different points in time and once treated they are always treated. The solid lines represent observed outcomes, whereas the dotted lines are the potential outcomes. For the untreated group and not yet treated periods the observed outcomes are also the potential outcomes. To go further, let’s introduce some notations. \\(T\\) is defined as the number of periods. Groups are defined based on the timing of the treatment of the unit. \\(G_{i}\\) indicates the group of the unit and all set of groups include \\(\\zeta \\in \\{2,\\; ...., T,\\;T+1\\}\\). Units that are treated in period 1 are dropped in this setup. One reason to do so is that no pretreatment outcomes are observed for this group, which means that it is not possible to use the parallel trend assumption. T+1 group is used to denote units that remain untreated throughout the period (never treated group). It is possible that eventually all units are treated. In this case, one can limit the data to time period with not yet treated group. \\(Y_{it}(g)\\) is denoted as the outcome of unit \\(i\\) observed at time \\(t\\) when the unit was treated in period \\(g\\). \\(Y_{it}(0)\\) is the potential outcome of unit \\(i\\) at time \\(t\\) had the unit not been treated. There are a few assumptions that we need to consider. There are more assumputions highlighted in (callaway2022?) paper. But I will focus on two main ones. Staggered Treatment Assignment. For all units and for all \\(t = 2,\\;3,...,\\;T\\), \\(D_{it-1}=1\\) implies \\(D_{it}=1\\). This means that once a unit is treated, it remains treated. Parallel Trend Assumption for Multi Period and Variation in Treatment Timing. For all \\(t = 2,\\;3,...,\\;T\\) \\[\\begin{equation} E[\\Delta Y_{it}(0)|G=g]=E[\\Delta Y_{it}(0)] \\end{equation}\\] This basically says that the average pathway of group g if it was untreated (potential outcome) would be same as the average pathway of other group (untreated) for each time period. This holds for each group \\(g \\in \\zeta\\). In other words, this can be thought as an extension of the parallel trend assumption for \\(2 \\times 2\\), but in this case it should hold for each \\(g \\in \\zeta\\) and for each time period \\(t\\). The parallel trend described in the assumption above is highly general, meaning that it assumes parallel trends for any groups. However, this may not be the case as groups can be very different in observed characteristics and can have different pathways in absence of the treatment. Hence, there are other versions of parallel trend assumption to consider: a. parallel trend holds only for groups with similar observed characteristics b. parallel trend holds only for certain time periods c. parallel trend holds only for those group who ever participate in the treatment "],["problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html", "4.14 Problem with TWFE in Multiple Group with Treatment Timing Variation", " 4.14 Problem with TWFE in Multiple Group with Treatment Timing Variation To understand what TWFE is estimating, write the potential outcome as: \\[\\begin{equation} Y_{it}(0) = \\theta_{t} + \\eta_{i} + v_{it}..... (A) \\end{equation}\\] Then write the TWFE using the potential outcome As \\[\\begin{equation} Y_{it} = Y_{it}(0) + 1\\{t \\geq G_{i}\\}(Y_{it}(G_i)-Y_{it}(0))..... (B) \\end{equation}\\] Note that substituting \\((A)\\) into \\((B)\\) gives the TWFE. Let’s look at the terms in the above equation closely. $ 1{t G_{i}}$ represents \\(D_{it}\\) and \\((Y_{it}(G_i)-Y_{it}(0))\\) is the \\(\\alpha\\) parameter in equation TWFE. The equality \\((Y_{it}(G_i)-Y_{it}(0))=\\alpha\\) indicates that the effects of being treated is the same for each group in \\(\\zeta\\) and the effects do not vary over time. In other words, the effects of treatment are homogeneous across units and over time. "],["what-is-twfe-estimating-when-there-is-treatment-timing-variation.html", "4.15 What is TWFE Estimating when there is Treatment Timing Variation?", " 4.15 What is TWFE Estimating when there is Treatment Timing Variation? Using (goodman2021?)’s decomposition the TWFEDD estimate \\(\\hat{\\alpha}^{DD}\\) can be written as: \\[\\begin{equation} \\hat{\\alpha}^{DD} = \\sum_{k \\neq U} s_{ku} \\hat{\\alpha}^{2 \\times 2}_{k} + \\underbrace{\\sum_{k \\neq U} \\sum_{l&gt;k}[s^k_{kl}\\hat{\\alpha}_{kl}^{2 \\times 2,k}+ s^l_{kl}\\hat{\\alpha}_{kl}^{2 \\times 2,l}]}_\\text{timing only estimator}....TWFE(decomposition) \\end{equation}\\] Here, \\(\\hat{\\alpha}^{2 \\times 2}_{kU} = [\\bar{y}_k^{Post(k)}-\\bar{y}_k^{Pre(k)}] - [\\bar{y}_U^{Post(k)}-\\bar{y}_U^{Pre(k)}]\\); this is when group \\(k\\) is compared to untreated group \\(U\\). \\(\\hat{\\alpha}^{2 \\times 2, k}_{kl} = [\\bar{y}_k^{MID(k,l)}-\\bar{y}_k^{Pre(k)}] - [\\bar{y}_l^{MID(k,l)}-\\bar{y}_l^{Pre(k)}]\\); this is when early group \\((k)\\) is compared to late treated group \\((l)\\) during the period when group \\(l\\) is not yet treated. \\(\\hat{\\alpha}^{2 \\times 2, l}_{kl} = [\\bar{y}_l^{POST(l)}-\\bar{y}_k^{MID(k,l)}] - [\\bar{y}_k^{POST(l)}-\\bar{y}_k^{MID(k,l)}]\\); this is when late group \\((l)\\) is compared to early treated group \\((k)\\) using the window between \\(MID(k,l)\\) and \\(POST(l)\\) when the treatment status of group \\((k)\\) does not change. Here, early treated group is being used as the control group. Note that the second block in TWFE(decomposition) uses variation in timing of treatment to identify the effects. Each group serves as the control to the other during the window when the treatment status do not change. \\(s_{ku}\\), \\(s_{kl}^{k}\\), and $s_{kl}^{l} are the weights placed on the estimates that compares: \\(i)\\) treated to untreated units (giving rise to 2 \\(2 \\times 2\\) DD estimates); \\(ii)\\) early treated to late treated in between \\(PRE(k)\\) and \\(MID(k,l)\\) window; and \\(iii)\\) late treated to early treated between \\(MID(k,l)\\) to \\(POST(l)\\) window, respectively. (goodman2021?) presents the following interpretation for the weights: \\[\\begin{equation} s_{kU} = \\frac{(n_k + n_U)^2 \\overbrace{n_{kU}(1-n_{kU})\\bar{D}_k(1-\\bar{D}_k)}^{\\hat{V}^{D}_{kU}} }{\\hat{V}^{D}} \\end{equation}\\] \\[\\begin{equation} s_{kl}^{k} = \\frac{((n_k + n_l)(1-\\bar{D}_l))^2 \\overbrace{n_{kl}(1-n_{kl})\\frac{\\bar{D}_k-\\bar{D}_l}{1-\\bar{D}_l}\\frac{1-\\bar{D}_k}{1-\\bar{D}_l}}^{\\hat{V}^{D,k}_{kl}} }{\\hat{V}^{D}} \\end{equation}\\] and \\[\\begin{equation} s_{kl}^{l} = \\frac{((n_k + n_l)\\bar{D}_k)^2 \\overbrace{n_{kl}(1-n_{kl})\\frac{\\bar{D}_l}{\\bar{D}_k}\\frac{\\bar{D}_k-\\bar{D}_l}{\\bar{D}_k}}^{\\hat{V}^{D,l}_{kl}} }{\\hat{V}^{D}} \\end{equation}\\] where, \\[\\begin{equation} \\sum_{k \\neq U} s_{ku} + \\sum_{k \\neq l} \\sum_{l&gt;k}(s_{kl}^{k} + s_{kl}^{l}) = 1 \\end{equation}\\] The TWFE estimate depends on weight implied to each of the \\(2\\times 2\\) DD estimate. The weights depend on the sample size of the group that is treated as well as the untreated group. Note that the weight also depends on the variance of the subsample based on the treated vs untreated groups. For instance, \\({\\hat{V}^{D,k}_{kl}}\\) denotes the variance in \\(D_i\\) for the subsample defined by groups \\(k\\) and \\(l\\), for the period \\(Pre\\) and \\(Mid(k,\\;l)\\). Let us take a look at \\(\\bar{D}_k(1-\\bar{D}_k)\\), \\(\\frac{\\bar{D}_k-\\bar{D}_l}{1-\\bar{D}_l}\\frac{1-\\bar{D}_k}{1-\\bar{D}_l}\\), and \\(\\frac{\\bar{D}_l}{\\bar{D}_k}\\frac{\\bar{D}_k-\\bar{D}_l}{\\bar{D}_k}\\) more closely. It is seen that these values are maximized when treatment occurs at the middle of the time window the researcher uses. In other words, the TWFE estimate depends on when the treatment occurs in the panel; if there is heterogeneous effects between groups, these effects are going to be emphasized (or de-emphasized) depending on wherein the given time window the treatment falls. This can be explained using a simulation that uses treatment effects of 10 and 15 for the early and late treated groups, respectively. The timing window comprise of 20 periods; the treatment timing of the early treated group is fixed at period 9, whereas the treatment timing for the late treated group is allowed to vary backwards from period 16 to 10. The figure shows that the higher treatment effect for the late treated group is supressed when the treatment timing is towards the end of the panel; the treatment effect increases as the treatment period for the late treated group approaches to the middle of the panel. When the treatment for the late period occurs at the middle of the panel (period 10), the estimate is very close to 12.5 – the average effect of early and late treated groups. Note that \\(K\\) timing group yields \\(K^2 - K\\) \\(2\\times 2\\) “timing-only” DD estimates (\\(\\hat{\\beta}^{2\\times 2 k}_{kl}\\) or \\(\\hat{\\beta}^{2\\times 2 l}_{kl}\\)); one untreated unit (throught out the time window) yields \\(K^2\\) DD estimates. "],["assumptions-governing-twfedd-estimate.html", "4.16 Assumptions governing TWFEDD estimate", " 4.16 Assumptions governing TWFEDD estimate The TWFEDD estimate measures weighted average of all possible \\(2 \\times 2\\) DD average treatment effects on treated. In the case of groups being defined as treatment timing \\(k,\\;g,\\;U\\), the \\(2\\times 2\\) DD estimates can be written as: \\[\\begin{equation} \\hat{\\alpha}^{2\\times 2}_{kU} = [\\bar{y}_k^{POST(k)} - \\bar{y}_k^{PRE(k)}] - [\\bar{y}_U^{POST(k)} - \\bar{y}_U^{PRE(k)}] \\end{equation}\\] \\[\\begin{equation} \\hat{\\alpha}^{2\\times 2 k}_{kl} = [\\bar{y}_k^{MID(k,l)} - \\bar{y}_k^{PRE(k)}] - [\\bar{y}_l^{MID(k,l)} - \\bar{y}_l^{PRE(k)}] \\end{equation}\\] \\[\\begin{equation} \\hat{\\alpha}^{2\\times 2 l}_{kl} = [\\bar{y}_l^{POST(l)} - \\bar{y}_l^{MID(k,l)}] - [\\bar{y}_k^{POST(l)} - \\bar{y}_k^{MID(k,l)}] \\end{equation}\\] Now, let us express the estimates based on the counterfactuals. First, write \\[\\begin{equation} y_{it} = D_{it}Y_{it}(t_i) + (1-D_{it})Y_{0} \\end{equation}\\] where, \\(Y_{it}\\) is the outcome of unit \\(i\\) in time \\(t\\) and \\(Y_{0}\\) is the counterfactual outcome. Following (callaway2022?) define ATT for group \\(k\\) at time period \\(\\tau \\geq k\\) as \\(ATT_{k}(\\tau) = E[Y_{i\\tau}(t^{*}_{k}) - Y_{i\\tau}(0)|t_i = k]\\) Now, let us define \\(W\\) as the date range or windows with \\(T_W\\) periods. In practice, \\(W\\) represents the post treatment window in \\(2 \\times 2\\) DD. But note that there are \\(T_{W}\\) periods. In our case above, \\(W\\) for group \\(k\\) represents the \\(MID(k,l)\\) plus the \\(POST(l)\\) windows and \\(T_{W} = 2\\). Group \\(k\\) is treated in two windows – \\(MID(k,l)\\) and \\(POST(l)\\); hence, the \\(ATT_{k}(W)\\) is just the average of ATTs across the windows. \\[\\begin{equation} ATT_{k}(W) = \\frac{1}{T_{W}} \\sum_{t \\in W} E[Y_{it}(k)-Y_{it}(0)|t_{i}=k] \\end{equation}\\] Now, define the change in average untreated potential outcome between pre and the post period as: \\[\\begin{equation} \\Delta Y_{k}^{0}(W_1, W_0) = \\frac{1}{T_{W_1}} \\sum_{t \\in W_1} E[Y_{it}(0)|t_{i}=k] - \\frac{1}{T_{W_0}} \\sum_{t \\in W_1} E[Y_{it}(0)|t_{i}=k] \\end{equation}\\] Using this notation, the \\(2 \\times 2\\) \\(\\hat{\\beta}\\)s can be written as: \\[\\begin{equation} \\hat{\\alpha}_k^{2\\times 2} = ATT_{k}^{(POST(k))} + \\overbrace{[ \\Delta Y_{k}^{0}(POST(k),PRE(k)) - \\Delta Y_{U}^{0}(POST(k),PRE(k))}^{parallel\\;trend}] \\end{equation}\\] \\[\\begin{equation} \\hat{\\alpha}_{kl}^{2\\times 2 k} = ATT_{k}^{(MID(k,l))} + \\overbrace{[ \\Delta Y_{k}^{0}(MID(k,l),PRE(k)) - \\Delta Y_{U}^{0}(MID(k,l),PRE(k))}^{parallel\\;trend}] \\end{equation}\\] \\[\\begin{equation} \\hat{\\alpha}_{kl}^{2\\times 2 l} = ATT_{l}^{(MID(k,l))} + \\overbrace{[ \\Delta Y_{l}^{0}(POST(l),MID(k,l)) - \\Delta Y_{k}^{0}(POST(l),MID(k,l))}^{parallel\\;trend}] + [ATT_k(MID(k,l))-ATT_k(POST(l))] \\end{equation}\\] While \\(\\hat{\\alpha}_{k}^{2\\times 2}\\) and \\(\\hat{\\alpha}_{k}^{2 \\times 2}\\) depends on the parallel trend assumption, \\(\\hat{\\alpha}_{k}^{2 \\times 2}\\) also depends on the difference between group \\(k&#39;s\\) \\(ATT\\) in \\(MID(k,l)\\) and \\(POST(l)\\). This is because the late treatment group is compared also with the early treatment group, and if there is presence of treatment dynamic in early treated group, this will show up in \\(\\hat{\\alpha}_{kl}^{2\\times 2 l}\\). Substituting the expressions of \\(\\hat{\\alpha}_{k}^{2\\times 2}\\), \\(\\hat{\\alpha}_{k}^{2 \\times 2}\\), and \\(\\hat{\\alpha}_{kl}^{2\\times 2 l}\\) into the TWFE decomposition yields the following: \\[\\begin{equation} \\plim_{N \\to \\infty} \\hat{\\alpha} = \\alpha = VWATT + VWCT + \\Delta ATT \\end{equation}\\] where, VWATT is the variance weighted average treatment effect on the treated; VWCT is the variance weighted common trends; and \\(\\Delta ATT\\) is the change in average treatment effect on treated of group \\(k\\) between the \\(Mid(k,l)\\) and \\(Posk(l)\\) period (treatment effect dynamics or heterogeneity over time). An intuition is that parallel trend assumption justifies comparing treated vs. untreated (or not yet treated groups), and deviation in pathways of outcome can be attributed to treatement. As such, (callaway2022?) refers to these groups (untreated and not yet treated) as “good comparison” groups. Now, early treatment group, that serves as the comparison group for the late treated group, can be a “bad comparison” if the treatment effect (of early treated group) varies with time. "],["how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html", "4.17 How Does Treatment Effect Heterogeneity in Time Affect TWFE?", " 4.17 How Does Treatment Effect Heterogeneity in Time Affect TWFE? As we have seen earlier, the TWFE estimator is baised even in cases where the parallel trend assumption holds given that the treatment effect varies over time. As of VWATT, this is a weighted version of each \\(2 \\times 2\\) DD estimate and the weights are dependent on treatment timing as well as treatment heterogeneity across groups. For example, states with high anti-smoking sentiments have increases cigarette taxes earlier; increases in cigarette taxes can affect populace living in state with higher anti-smoking sentiments more compared to states with low anti smoking sentiments. Now given that states with relatively lower of anti-smoking sentiments increased cigarette taxes later (and if this falls in the middle of the panel), the VWATT will provide higher weights to the states with lower anti-smoking sentiments. In this case, the estimate of cigarette taxes will be underestimated. "],["causal-forest.html", "5 Causal Forest ", " 5 Causal Forest "],["introduction.html", "5.1 Introduction", " 5.1 Introduction The generalized random forest is a method that is quite flexible in estimating the quantity of interest. The theory of it is built using the moment criterion: \\(E[\\psi_{\\theta_i, \\; \\upsilon_i} (O_i) | X_i] = 0, \\; for \\; all \\; x \\; in \\; \\chi\\) Getting down to the nuts and bolts of the theory is beyond the scope of this write-up. Rather, we would want to take a closer look at causal forests – a component of GRF framework. "],["summary-of-grf.html", "5.2 Summary of GRF", " 5.2 Summary of GRF It seeks a generalized way to conduct causal inference under a non-parametric framework. GRF relies on random forest. Methods developed to aid causal inference such as: \\(i)\\) randomized controlled trial, \\(ii)\\) comparison between treatment and control units under unconfoundedness assumption, \\(iii)\\) difference-in-differences, and \\(iv)\\) panel data methods; can fit into GRF framework. To do so, one needs to feed in the method-specific encoding into the GRF framework (to guide the splitting process). "],["motivation-for-causal-forests.html", "5.3 Motivation for Causal Forests", " 5.3 Motivation for Causal Forests Let’s expand on estimating the average treatment effect of a treatment intervention \\(W\\). The specifics are listed as: \\(W_i \\in \\{0, \\; 1\\}\\): treatment intervention \\(X_i\\): covariates \\(Y_i\\): response/outcome In the parametric framework \\(\\tau\\), the treatment effect, is estimated using the following specification: \\(Y_i = \\tau W_i + \\beta_1 X_i + \\epsilon_i\\) The validity of \\(\\hat{\\tau}\\) as a causal estimand is justified under the following three assumptions. Unconfoundedness: \\(Y^{(0)}_i, \\; Y^{(1)}_i \\perp W_i | X_i\\). Treatment assignment is independent of the potential outcome once conditioned on the covariates. In other words, controling for covariates makes the treatment assignment as good as random. \\(X_i\\)s influence \\(Y_i\\)s in a linear way. The treatment effect is homogeneous. Assumption 1 is the identification assumption. In the traditional sense, one can control for \\(X_i\\)s in the regression framework and argue that this assumption is met. Even if all \\(X\\)s that influence the treatment assignment are observed (this is the assumption that we make throughout), we are unsure how \\(X\\)s affect the treatment. Often \\(X\\)s can affect treatment in a non-linear way. Assumptions 2 and 3 can be questioned and relaxed. One can let data determine the way \\(X\\) needs to be incorporated in the model specification (relaxing assumption 2). Moreover, treatment effects can vary across some covariates (relaxing assumption 3). First, lets relax assumption 2. This leads to the following partially linear model: \\(Y_i = \\tau W_i + f(X_i) + \\epsilon_i \\;............. equation 1\\) where, \\(f\\) is a function that maps out how \\(X\\) affects \\(Y\\). However, we don’t know \\(f\\) in practice. So, how do we go about estimating \\(\\tau\\)? The causal forest framework under GRF connects the old-school literature of causal inference with ML methods. Robinson (1988) shows that if two intermediate (nuiscance) objects, \\(e(X_i)\\) and \\(m(X_i)\\) are known, one can estimate \\(\\tau\\). The causal forest framework under GRF utilizes this result. Here: \\(e(X_i)\\) is the propensity score; the probability of being treated. \\(E[W_i| X_i = x]\\) \\(m(X_i)\\) is the conditional mean of \\(Y\\). \\(E[Y_i | X_i = x] = f(x) + \\tau e(x)\\) Demeaning equation 1 (substracting \\(m(x)\\)) gives the following residual-on-residual regression: \\(Y_i - m(x) = \\tau (W_i - e(x)) + \\epsilon \\; .............. equation 2\\) Intuition for equation (2) proceeds as follow. Note that \\(m(x)\\) is the conditional mean of Y given \\(X_i = x\\).10 This means that units with similar \\(X\\)s will have similar estimates for \\(m(x)\\) in \\(W=\\{0, \\; 1\\}\\), which would mean that estimates on \\(e(x)\\) would also be similar for these units across both treatment and control group. Now, consider that the treatment is positive; this will show up in \\(Y_i\\). \\(Y_i - m(x)\\) will be higher for \\(W=1\\) compared to \\(W=0\\) for similar estimates of \\(m(x)\\). On the other side, \\(W_i - e(x)\\) is positive for \\(W=1\\) and negative for \\(W=0\\) for similar estimates of \\(e(x)\\). Such variations in the left and right hand side quantities will allow to capture postive estimates on \\(\\tau\\). To gain ML methods are used to estimate \\(m(x)\\) and \\(e(x)\\) and residual-on-residual regression is used estimate \\(\\tau\\). It turns out that even noisy estimates of \\(e(x)\\) and \\(m(x)\\) can give ``ok” estimate of \\(\\tau\\). How to estimate \\(m(x)\\) and \\(e(x)\\)? Use ML methods (boosting; random forest) Use cross-fitting for prediction. prediction of observation \\(i&#39;s\\) outcome &amp; treatment assignment is obtained without using the observation ``\\(i\\)“. Lets take a look at residual-on-residual in the case of homogeneous treatment effect. # generate data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) # Generate W and Y W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) prob &lt;- 0.4 + 0.2 * (X[, 1] &gt; 0) Y &lt;- 2.5 * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n) # Train regression forests mx &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) ex &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) Wcen &lt;- W - ex$predictions Ycen &lt;- Y - mx$predictions reg &lt;- summary(lm(Ycen ~ Wcen)) reg ## ## Call: ## lm(formula = Ycen ~ Wcen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6687 -0.7147 -0.0139 0.7059 3.9119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.00539 0.02359 -0.228 0.819 ## Wcen 2.45817 0.04791 51.305 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.055 on 1998 degrees of freedom ## Multiple R-squared: 0.5685, Adjusted R-squared: 0.5683 ## F-statistic: 2632 on 1 and 1998 DF, p-value: &lt; 2.2e-16 print(paste0(&quot;The treatment effect estimate based on residual-on-residual regression is: &quot;, coefficients(reg)[2])) ## [1] &quot;The treatment effect estimate based on residual-on-residual regression is: 2.45817277748273&quot; print(paste0(&quot;The true treatment effect is: &quot;, 2.5)) ## [1] &quot;The true treatment effect is: 2.5&quot; We can also think of \\(m(x)\\) as the case when we ignore \\(W\\), although we know that treatment took place. This way, \\(m(x) = \\mu_{0}(x) + e(x)\\tau\\), where \\(\\mu_{0}(x)\\) is the baseline conditional expectation without the treatment. This makes it easy to see that units with similar features will have similar estimates of \\(m(x)\\).↩︎ "],["causal-forest-1.html", "5.4 Causal Forest", " 5.4 Causal Forest Both regression and causal forests consist of: 1) Building phase; and 2) estimation phase. The intuition regarding the regression/causal forest can be gleaned using the following figure. Figure 5.1: Figure 1. Adaptive weights In this simple case, the sample is partitioned into \\(N_1\\) and \\(N_2\\) neighborhoods accorinng to the splitting rule that the squared difference in sub-sample specific treatment effect is the maximum, i.e., \\(n_{N_1}n_{N_2}(\\tau_{N_1} - \\tau_{N_2})^2\\) is the maximum. This by construction leads to constant treatment effect in the neighborhood, while the effects may vary across the neighborhoods. This intuition allows us to relax assumption 3, and re-write the partially linear estimation framework as: \\(Y_i = \\tau(x) W_i + f(X_i) + \\epsilon_i\\). Here the estimate of the treatment effect \\(\\tau\\) is allowed to vary with the test point \\(x\\). In reference to Figure 1 above, \\(N_1\\) and \\(N_2\\) are neighborhoods where treatment effects are constant. To estimate the treatment effect of the test point \\(x\\), \\(\\tau(x)\\), we would run a weighted residual-on-residual regression of the form. \\(\\tau(x) := lm(Y_i - m(X_i)^{-i} \\sim \\tau(W_i - e(X_i)^{-i}), \\; weights = 1\\{X_i \\in N(x)\\}\\) where \\(m(X_i)^{-i}\\) and \\(e(X_i)^{-i}\\) are obtained from cross-fitting. The weights play a pivotal role here and takes a value 1 if \\(X_i\\) belongs to the same neighborhoods as \\(x\\). In the above figure, examples in \\(N_2\\) receive non-zero weight while those in \\(N_1\\) receive zero weight. However, this example only pertains to a tree. But we’d want to build a forest and apply the same analogy. Adaptive weights. The forest consists of \\(B\\) trees, so the weights for each \\(X_i\\) pertaining to the test point \\(x\\) is based off of all \\(B\\) trees. The causal forest utilizes adaptive weights using random forests. The tree specific weight for an example \\(i\\) at the \\(b^{th}\\) tree is given as: \\(\\alpha_{ib}(x) = \\frac{1(X_i \\in L_{b}(x))}{|L_{b}(x)|}\\), where \\(L(x)\\) is the leaf (neighborhood) that consist of the test sample \\(x\\). The forest specific weight for an example \\(i\\) is given as: \\(\\alpha_{i}(x) = \\frac{1}{B} \\sum_{b = 1}^{B} \\frac{1(X_i \\in L(x))}{|L(x)|}\\) It tracks the fraction of times an obsevation \\(i\\) falls on the same leaf as \\(x\\) in the course of the forest. Simply, it shows how similar \\(i\\) is to \\(x\\). Regression Forest. It utilizes the adaptive weights given to an example \\(i\\) (\\(i = \\{1, \\; 2, \\; ..., N\\}\\)) and constructs a weighted average to form the prediction of \\(x\\). The prediction for \\(x\\) based on the regression forest is: \\(\\hat{\\mu}(x) = \\frac{1}{B}\\sum_{i = 1}^{N} \\sum_{b=1}^{B} Y_{i} \\frac{1(X_i \\in L_{b}(x)}{|L_b(x)|}\\) \\(= \\sum_{i = 1}^{N} Y_{i} \\alpha_{i}\\) Note that this is different from the traditional prediction from the random forest that averages predictions from each tree. \\(\\hat{\\mu}(x.trad) = \\sum_{b = 1}^{B} \\frac{\\hat{Y}_b}{B}\\) Causal Forest. Causal forest is analogous to the regression forest in a sense that the target is \\(\\tau(x)\\) rather than \\(\\mu(x)\\). Conceptually the difference is encoded in the splitting criteria. While splitting, regression forest is based on the criterion: \\(\\max n_{N_1} n_{N_2}(\\mu_{N_1} - \\mu_{N_2})^2\\), whereas the causal forest is based on \\(\\max n_{N_1} n_{N_2}(\\tau_{N_1} - \\tau_{N_2})^2\\). In a world with infinite computing power, for each potential axis aligned split that extends from the parent node, one would estimate treatment effects at two of the child nodes (\\(\\tau_{L}\\) and \\(\\tau_{R}\\)) and go for the split that maximizes the squared difference between child specific treatment effects. However, in practice this is highly computationally demanding and infeasible. The application of causal forest estimates \\(\\tau_{P}\\) at the parent node and uses the gradient based function to guide the split. At each (parent) node the treatment effect is estimated only once. Once the vector of weights are determined for \\(i\\)s, the following residual-on-residual is ran: \\(\\tau(x) := lm(Y_i - m(X_i^{-i}) \\sim \\tau(x)(W_i - e(X_i)^{-i}), \\; weights = \\alpha_i(x)\\) This can be broken down as: Estimate \\(m^{-i}(X_i)\\) and \\(e^{-i}(X_i)\\) using random forest. Then estimate \\(\\alpha_i(x)\\). For each new sample point \\(x\\), a vector of weight will be determined based on adaptive weighting scheme of the random forest. Note that the weights will change for each new test point. Run a weighted residual-on-residual regression given by the equation above. "],["an-example-of-causal-forest.html", "5.5 An example of causal forest", " 5.5 An example of causal forest rm(list = ls()) library(devtools) #devtools::install_github(&quot;grf-labs/grf&quot;, subdir = &quot;r-package/grf&quot;) library(grf) library(ggplot2) # generate data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) # Train a causal forest. W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n) # Train a causal forest c.forest &lt;- causal_forest(X, Y, W) # predict using the training data using out-of-bag prediction tau.hat.oob &lt;- predict(c.forest) hist(tau.hat.oob$predictions) # Estimate treatment effects for the test sample tau.hat &lt;- predict(c.forest, X.test) plot(X.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2), xlab = &quot;x&quot;, ylab = &quot;tau&quot;, type = &quot;l&quot;) lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 2) # estimate conditional average treatment effect (CATE) on the full sample cate &lt;- average_treatment_effect(c.forest, target.sample = &quot;all&quot;) print(paste(&quot;Conditinal Average Treatment Effect (CATE) is: &quot;, cate[[1]])) ## [1] &quot;Conditinal Average Treatment Effect (CATE) is: 0.405843073161648&quot; # estimate conditional average treatment effect on treated catt &lt;- average_treatment_effect(c.forest, target.sample = &quot;treated&quot;) paste(&quot;Conditional Average Treatment Effect on the Treated (CATT)&quot;, catt[[1]]) ## [1] &quot;Conditional Average Treatment Effect on the Treated (CATT) 0.492339168239352&quot; # Add confidence intervals for heterogeneous treatment effects; growing more trees recommended tau.forest &lt;- causal_forest(X, Y, W, num.trees = 4000) tau.hat &lt;- predict(tau.forest, X.test, estimate.variance = TRUE) # for the test sample ul &lt;- tau.hat$predictions + 1.96 * sqrt(tau.hat$variance.estimates) ll &lt;- tau.hat$predictions - 1.96 * sqrt(tau.hat$variance.estimates) tau.hat$ul &lt;- ul tau.hat$ll &lt;- ll tau.hat$X.test &lt;- X.test[,1] ggplot(data = tau.hat, aes(x = X.test, y = predictions)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;grey70&quot;) + geom_line(aes(y = predictions)) + theme_bw() ###################################################### # # # In some cases prefitting Y and W separately may # be helpful. Say they use different covariates. # ###################################################### # Generate a new data n &lt;- 4000 p &lt;- 20 X &lt;- matrix(rnorm(n * p), n, p) TAU &lt;- 1 / (1 + exp(-X[, 3])) W &lt;- rbinom(n, 1, 1 / (1 + exp(-X[, 1] - X[, 2]))) # X[, 1] and X[, 2] influence W Y &lt;- pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n) # X[, 2], X[, 3], X[, 4:6] influence Y. So different set of Xs influence Y # Build a separate forest for Y and W forest.W &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) W.hat &lt;- predict(forest.W)$predictions # this gives us the estimated propensity score (probability of treated) #plot(W.hat, X[, 1], col = as.factor(W)) #plot(W.hat, X[, 2], col = as.factor(W)) forest.Y &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) # note that W is not used here Y.hat &lt;- predict(forest.Y)$predictions # this gives the conditional mean of Y or m(x) #plot(Y, Y.hat) forest.Y.varimp &lt;- variable_importance(forest.Y) forest.Y.varimp ## [,1] ## [1,] 0.002939109 ## [2,] 0.464324588 ## [3,] 0.386426876 ## [4,] 0.040967280 ## [5,] 0.025417033 ## [6,] 0.053186112 ## [7,] 0.002563865 ## [8,] 0.001711964 ## [9,] 0.001464095 ## [10,] 0.001727238 ## [11,] 0.002546653 ## [12,] 0.001280970 ## [13,] 0.002841205 ## [14,] 0.002328143 ## [15,] 0.001422961 ## [16,] 0.001131292 ## [17,] 0.001542447 ## [18,] 0.002144746 ## [19,] 0.002829431 ## [20,] 0.001203992 # selects the important variables selected.vars &lt;- which(forest.Y.varimp / mean(forest.Y.varimp) &gt; 0.2) selected.vars ## [1] 2 3 4 5 6 # Trains a causal forest tau.forest &lt;- causal_forest(X[, selected.vars], Y, W, W.hat = W.hat, Y.hat = Y.hat, # specify e(x) and m(x) tune.parameters = &quot;all&quot;) # See if a causal forest succeeded in capturing heterogeneity by plotting # the TOC and calculating a 95% CI for the AUTOC. train &lt;- sample(1:n, n / 2) train.forest &lt;- causal_forest(X[train, ], Y[train], W[train]) eval.forest &lt;- causal_forest(X[-train, ], Y[-train], W[-train]) rate &lt;- rank_average_treatment_effect(eval.forest, predict(train.forest, X[-train, ])$predictions) rate ## estimate std.err target ## -0.002787873 0.04889062 priorities | AUTOC plot(rate) paste(&quot;AUTOC:&quot;, round(rate$estimate, 2), &quot;+/&quot;, round(1.96 * rate$std.err, 2)) ## [1] &quot;AUTOC: 0 +/ 0.1&quot; "],["heterogeneous-treatment-effects.html", "6 Heterogeneous Treatment Effects", " 6 Heterogeneous Treatment Effects This article summarizes heterogeneous treatment effects using ML. Simply put, its defined as the variation in response to treatment across several subgroups. For example, the impacts of Medicaid expansion on labor market outcomes can vary depending on uninsured rate prior to the expansion; the effects of discussion intervention program aimed to normalize disscussion regarding menstruation can increase demand for menstrual health products at a higher rate among those with high psychological cost in the baseline; in personalized medical treatment, we would want to identify the sub-group with higher response to a particular type of treatment. It is different from average treatment effect (ATE) such that the ATE focuses on the whole group, while heterogeneous treatment effect pertains to the specific sub-group characterized by features (\\(X\\)s). In this sense, one can think of ATE as the weighted average of subgroup specific ATEs. Using the potential outcome framework, ATE is given by: \\(E[Y_i^{1} - Y_i^{0}]\\). The heterogeneous treatment is: \\(E[Y_i^{1} - Y_i^{0} | X_i = x ]\\). Its the treatment conditional on \\(X_i\\), which is determined prior to observing the data. Hence, its also termed as the conditional average treatment effect (CATE). One simple example borrowed from Wager’s lecture notes to illustrate the concept is that of smoking in Geneva and Palo Alto. Say, two RCTs are conducted in Palo Alto and Geneva to evaluate whether cash incentives among teenagers can reduce the prevalence of smoking. # Palo-Alto smoke_mat &lt;- function(smoke_vec){ smoke &lt;- matrix(0, nrow =2, ncol = 3) smoke[, 1] &lt;- c(&quot;Treat&quot;, &quot;Control&quot;) smoke[ ,2] &lt;- c(smoke_vec[1], smoke_vec[2]) smoke[ ,3] &lt;- c(smoke_vec[3], smoke_vec[4]) return(smoke) } smoke &lt;- smoke_mat(c(152, 2362, 5, 122)) colnames(smoke) &lt;- c(&quot;Palo Alto&quot;, &quot;Non-S.&quot;, &quot;Smoker&quot;) data.frame(smoke) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) Palo.Alto Non.S. Smoker Treat 152 5 Control 2362 122 smoke &lt;- smoke_mat(c(581, 2278, 350, 1979)) colnames(smoke) &lt;- c(&quot;Geneva&quot;, &quot;Non-S.&quot;, &quot;Smoker&quot;) data.frame(smoke) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) Geneva Non.S. Smoker Treat 581 350 Control 2278 1979 \\(\\hat{\\tau}_{PA} = \\frac{5}{152+5} - \\frac{122}{2362 + 122} \\approx -1.7 pp\\) \\(\\hat{\\tau}_{GVA} = \\frac{350}{581+350} - \\frac{1979}{2278 + 1979} \\approx -8.9 pp\\) \\(\\hat{\\tau} = \\frac{2641}{2641 + 5188}\\tau_{PA} + \\frac{5188}{2641 + 5188}\\tau_{GVA}\\). Here, \\(\\hat{\\tau}_{PA}\\) is an estimate of \\(E[smoke \\;prevalence | \\; W = 1, \\; X = PA] \\; - \\; E[smoke \\;prevalence | \\; W = 0, \\; X = PA]\\), and its the treatment effect particular to Palo Alto. The average treatment effect \\(\\hat{\\tau}\\) is the weighted average of the two treatment effects. "],["some-ways-to-estimate-cate.html", "6.1 Some ways to estimate CATE", " 6.1 Some ways to estimate CATE Robinson’s partially linear model for homogeneous treatment effect is written as: \\(Y_i = \\tau W_i + f(X_i) + \\epsilon_i \\; ........(equation \\; 1)\\) Here, \\(\\tau\\) is assumed constant across sub-spaces of \\(X\\). We can expand to write Robinson’s partially linear model as: \\(Y_i = \\tau(X_i) W_i + f(X_i) + \\epsilon_i \\; ........(equation \\; 2)\\) where, \\(\\tau(.)\\) varies with \\(x\\). Equation 2 can be expressed as residual-on-residual regression format of: \\(Y_i - m(X_i) = \\tau(X_i) (W_i - e(X_i)) + \\epsilon_i \\; ........(equation \\; 3)\\) where, \\(m(x)\\) is the conditional expectation of \\(Y\\) given \\(X\\). \\(m(x) = E[Y_i | \\; X_i = x] = \\mu_{W = 0}(X_i) + \\tau(X_i) e(X_i)\\), where \\(\\mu_{0}(X_i)\\) is the baseline conditional response (in absense of treatment) and \\(e(x) = P(W_i = 1 | \\; X_i = x)\\).11 \\(\\tau(X)\\) is parameterized as \\(\\tau(x) = \\psi(x).\\beta\\), where \\(\\psi\\) is some pre-determined set of basis functions: \\(\\chi \\rightarrow R^k\\). A feasible loss function can be devised using equation 3 and using estimates of \\(m(x)\\) and \\(e(x)\\) from cross-fitting. \\(L = \\frac{1}{n} \\sum_{i = 1}^n((Y_i - \\hat{m}(X_i)^{-k(i)}) - (W_i - \\hat{e}(X_i)^{-k(i)}) \\; \\psi(X_i).\\beta)^2\\). Note that the parameter of interest is \\(\\beta\\). LASSO can be used to estimate \\(\\hat{\\beta}\\), where: \\(\\hat{\\beta} = argmin_{\\beta}\\{L + \\lambda \\; ||\\beta||_{1}\\}\\), where \\(\\lambda\\) is the regularizer on the complexity of \\(\\tau(.)\\).12 Note: The other approach is to use random forest to measure out weight of an observation \\(i\\) in relation to the test point \\(x\\). This approach is done using causal forest in the Generalized Random Forest framework. The distinction between \\(m(x)\\) and \\(m(X_i)\\) is such that the former is estimation performed at the new data point \\(x\\).↩︎ One can build a highly complex model and improve the in-sample fit. However, this model may perform badly while predicting out-of-sample cases. As such, the complexity of the model should be penalized while training the model.↩︎ "],["estimation.html", "6.2 Estimation", " 6.2 Estimation set.seed(194) # Generate Data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) prob &lt;- 0.4 + 0.2 * (X[, 1] &gt; 0) Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmax(X[, 3], 0) + rnorm(n) ################################### ################################### # # # 1. estimate m(X) and e(X) # using cross-fitting # ################################### ################################### # cross-fitting index K &lt;- 10 # total folds ind &lt;- sample(1:length(W), replace = FALSE, size = length(W)) folds &lt;- cut(1:length(W), breaks = K, labels = FALSE) index &lt;- matrix(0, nrow = length(ind) / K, ncol = K) for(f in 1:K){ index[, f] &lt;- ind[which(folds == f)] } # Build a function to estimate conditional means (m(x) and e(x)) using random forest fun.rf.grf &lt;- function(X, Y, predictkfold){ rf_grf &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) p.grf &lt;- predict(rf_grf, predictkfold)$predictions return(p.grf) } # storing predict.mat &lt;- matrix(0, nrow = nrow(index), ncol = K) # to store e(x) predict.mat2 &lt;- predict.mat # to store m(x) # for each fold k use other folds for estimation for(k in seq(1:K)){ predict.mat[, k] &lt;- fun.rf.grf(X = X[c(index[, -k]), ], Y = W[index[, -k]], predictkfold = X[c(index[, k]), ]) predict.mat2[, k] &lt;- fun.rf.grf(X = X[c(index[, -k]), ], Y = Y[c(index[, -k])], predictkfold = X[c(index[, k]), ]) } W.hat &lt;- c(predict.mat) Y.hat &lt;- c(predict.mat2) ################################ ################################ # # 2. Use LASSO to minimize # the loss function ################################ ################################ # rearrange features and response according to index XX &lt;- X[c(index), ] YY &lt;- Y[c(index)] WW &lt;- W[c(index)] resid.Y &lt;- YY - Y.hat resid.W &lt;- WW - W.hat # Create basis expansion of features for(i in seq(1, ncol(XX))) { if(i == 1){ XX.basis &lt;- bs(XX[, i], knots = c(0.25, 0.5, 0.75), degree = 2) }else{ XX.basisnew &lt;- bs(XX[, i], knots = c(0.25, 0.5, 0.75), degree = 2) XX.basis &lt;- cbind(XX.basis, XX.basisnew) } } resid.W.X &lt;- resid.W * XX.basis resid.W.X &lt;- model.matrix(formula( ~ 0 + resid.W.X)) #plot(XX[ ,1], pmax(XX[ , 1], 0)) # cross validation for lasso to tune lambda lasso &lt;- cv.glmnet( x = resid.W.X, y = resid.Y, alpha = 1, intercept = FALSE ) #plot(lasso, main = &quot;Lasso penalty \\n \\n&quot;) # lambda with minimum MSE best.lambda &lt;- lasso$lambda.min lasso_tuned &lt;- glmnet( x = resid.W.X, y = resid.Y, lambda = best.lambda, intercept = FALSE ) #print(paste(&quot;The coefficients of lasso tuned are:&quot;, coef(lasso_tuned), sep = &quot; &quot;)) pred.lasso &lt;- predict(lasso, newx = XX.basis) ######################### # # Causal Forest # ######################### X.test &lt;- matrix(0, nrow = nrow(X), ncol = ncol(X)) X.test[, 1] &lt;- seq(-3, 3, length.out = nrow(X)) tau.forest &lt;- causal_forest(X, Y, W) tau.forest ## GRF forest object of type causal_forest ## Number of trees: 2000 ## Number of training samples: 2000 ## Variable importance: ## 1 2 3 4 5 6 7 8 9 10 ## 0.707 0.045 0.030 0.037 0.034 0.029 0.026 0.035 0.024 0.033 tau.hat &lt;- predict(tau.forest, X.test)$predictions par(oma=c(0,4,0,0)) plot(XX[order(XX[ , 1]), 1], pred.lasso[order(XX[, 1])], ylim = c(0, 3), t = &quot;l&quot;, xlab = &quot; &quot;, ylab = &quot; &quot;, xlim = c(-3, 3), lwd = 1.5) par(new = TRUE) plot(XX[order(XX[, 1]), 1], pmax(XX[order(XX[, 1]), 1], 0), col =&quot;red&quot;, ylim = c(0, 3), t = &quot;l&quot;, xlab = &quot;X1&quot;, ylab = &quot;tao(x)&quot;, xlim = c(-3, 3), lwd = 1.5) par(new = TRUE) plot(X.test[order(X.test[, 1]), 1], tau.hat[order(X.test[, 1])], t = &quot;l&quot;, col = &quot;blue&quot;, ylim = c(0, 3), xlab = &quot;X1&quot;, ylab = &quot;&quot;, xlim = c(-3, 3), lwd = 1.5) legend(&quot;topleft&quot;, c(&quot;Loss min Lasso&quot;, &quot;True Effect&quot;, &quot;Causal Forest&quot;), col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty = rep(1, 3)) "],["some-remarks-and-questions.html", "6.3 Some Remarks and Questions", " 6.3 Some Remarks and Questions For LASSO, we are using the basis of polynomial splines of degree 2 with interior knots at 25th, 75th, and 50th percentiles of each feature. We can see that although the effects are picked up, its slightly late and are lower compared to the true effect. A basis for linear splines performs well in this case. The causal forest framework on the other hand performs better. "]]
