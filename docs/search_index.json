[["index.html", "IPW and AIPW 1 Work in Progress", " IPW and AIPW Vinish Shrestha 2024-07-05 1 Work in Progress This is a work in progress. "],["causal-inference-an-introduction.html", "2 Causal Inference: An Introduction", " 2 Causal Inference: An Introduction “Correlation is not causality” is one of the most frequently used lines in social science. In a lab experiment, a researcher can perform controlled experiments to determine whether A causes B by controlling for confounders. However, the complexities and interrelations of human behavior create a setting starkly different from the controlled environment of a lab, making things much more convoluted. Causal inference, therefore, can be seen as a process to determine whether A causes B in both lab settings and out-of-lab scenarios. A simple example. Say, we are interested in evaluating the effects of a tutoring program on exam scores for an introductory course. To begin, in this simple example, we assume that the treatment is (completely) randomly assigned. The class is randomly divided into two groups: one group receives the treatment (treatment group) and the other group does not receive the treatment (control group). Proper randomization means that each individual has an equal probability of receiving the treatment or not receiving it. This approach with an arbitrarily high probability ensures balance in both observed and unobserved factors as the sample size grows such that any differences in outcomes between the treatment and control groups can be attributed to the treatment itself, rather than to pre-existing differences between the groups.1 Balance here is defined as an instance when all pre-treatment covariates between the treatment and control groups are similar. If this is attained then it increases confidence that the treatment and the control units are comparable. Set up. We use \\(W\\) to denote the treatment status such that \\(W_i \\in \\{0, \\; 1\\}\\), \\(Y_i\\) is the exam score following the treatment assignment, and \\(X_i\\) are the covariates (e.g., gender, race). The subscript \\(i\\) indicates an individual or unit of observation. Of course, balance is not guranteed and in such cases one should think hard whether differences in covariates matter, and if they do, adjustment should be applied.↩︎ "],["potential-outcome-framework-neyman-rubin-causal-model.html", "2.1 Potential Outcome Framework: Neyman-Rubin Causal Model", " 2.1 Potential Outcome Framework: Neyman-Rubin Causal Model We are going to use the potential outcome framework to describe the impacts of the treatment following the Neyman-Rubin causal model (Splawa-Neyman, Dabrowska, and Speed 1923 [1990]; Rubin 1974). Define \\(Y_i(0)\\) and \\(Y_i(1)\\) as the potential outcomes for an individual \\(i\\) in the case of treatment and without treatment, respectively. The potential outcomes are not realized yet. As such it is wrong to say that \\(Y_{i}(0) = Y_i\\). Let’s spend some time discussing various formats of the potential outcome in relation to what is observed versus what is not. \\([Y_{i}(0)|W_i = 0].\\) Here, the expression in the bracket is read as the outcome of an unit \\(i\\) in the no-treatment state conditional upon \\(i\\) actually not receiving the treatment. This is an observed outcome. \\([Y_{i}(0)|W_i = 1].\\) Here, the expression is asking for what the outcome of an unit \\(i\\) who received the treatment \\((W_i = 1)\\) would be in absence of the treatment. This is not observed and is termed as the counterfactual. The same goes with the potential outcome \\(Y_{i}(1)\\) – the outcome if \\(i\\) were to be treated. The observed variable, \\(Y_i\\), can be written as a function of the potential outcomes as follows: \\[\\begin{equation} Y_i = W \\times Y_i(1) + (1-W)\\times Y_i(0) \\end{equation}\\] The fundamental problem is that one cannot observe both \\(Y_i(0)\\) and \\(Y_i(1)\\) at the same time. As such, the causal inference through the lens of Neyman-Rubin causal framework can be seen as the missing data problem. If one has the data for \\(Y_i(1)\\) then the \\(Y_i(0)\\) counterpart is missing and vice-versa. Much of causal inference is finding ways to deal with the missing-data problem. The independence assumption allows us to proceed further with causality. Formally, a complete random assignment of treatment means: \\(W_i \\perp Y_i(0), Y_i(1)\\). This states that the treatment assignment is independent of potential outcomes. Quite literally, this means that the treatment assignment is not related to the potential outcome. In other words, the treatment assignment is completely random (probability of being treated is 0.5 in the case of binary treatment). The independence assumption also states that the treatment assignment is independent of any covariates \\(X_i\\). In our particular example, this means that the probability of receiving the treatment is the same for different groups defined by these covariates, such as gender and race. Specifically, females are equally likely to get treated compared to males, and Blacks are equally likely to be treated compared to Whites. Within both the treatment and control groups, it is highly likely that the proportion of Blacks and Whites, as well as males and females, will be similar – an attribute known as balance. The independence assumption is one of the necessary assumptions to proceed further but it is not sufficient. Additional two assumptions are needed to proceed ahead: overlap and Stable Unit Treatment Value Assumption (SUTVA). The overlap assumption states that observations in both the treatment and control groups fall within the common support. For instance, this assumption is violated if the treatment group consist of all females and the control group consist of all males as one would not be able to attain balance in covariates. The independence and overlap assumption together constitute a property known as stong ignorability of assignment, which is necessary for the identification of the treatment effect. The SUTVA assumption is the no interference assumption defining that the treatment status of one unit should not affect the potential outcome for other units. In our example, tutoring treatment for a unit in the treatment group should not change the potential outcome for other units. This assumption breaks down if there is a spillover effect, for example, if the a student in the treatment group helps her friend in the control group. References "],["average-treatment-effect-ate.html", "2.2 Average treatment effect (ATE)", " 2.2 Average treatment effect (ATE) Our target is to estimate the effects of the treatment. For a brief moment, let’s assume the presence of a parallel universe that includes Alia, Ryan, Shrey, Samaira, and Rakshya in the course. In one universe (actual universe) the treatment for these individuals are randomly allocated: \\(W_{Alia} = 1\\), \\(W_{Ryan} = 0\\), \\(W_{Shrey} = 0\\), \\(W_{Samaira} = 1\\), \\(W_{Rakshya} = 0\\). In the other (parallel) universe, everything is similar to the actual universe except that the treatment status is exactly opposite. In this case, individual specific treatment effect can be estimated by taking the difference in individual specific outcomes across two universes. For example, the treatment effect for Alia is \\(Y_{Alia}(1) - Y_{Alia}(0).\\) This is feasible since a perfect counterfactual is available for all the units given the parallel universe. The average of such individual treatment effect gives the average treatment effect, ATE. The target is to estimate average treatment effect (ATE), which is defined as: \\[\\begin{equation} ATE = E(Y_i(1)) - E(Y_i(0)) \\end{equation}\\] \\(Y_i(1)\\) denotes the outcome for an unit \\(i\\) in presence of treatment, whereas \\(Y_i(0)\\) is the realization for the same unit \\(i\\) in absence of the treatment. As we know, it is impossible to measure the unit \\(i\\) in two different states (with and without treatment). A major difficulty is that one cannot observe units simultaneously with and without treatment in reality. This means that the perfect counterfactual does not really exist. This again emphasizes causal inference as a missing data problem – when estimating the treatment effect of an unit \\(i\\), \\(Y_i(0)\\) is not observed if \\(Y_i(1)\\) is and vice-versa. This unfortunately does not allow us to estimate individualized treatment effect. The best we can do (as of yet) is use the independence assumption as well as overlap assumption together and evalute ATE. Note that the independence condition, $ W_i Y_i(0), ; Y_i(1)$, gives: \\(E(Y_i(0)|W_i = 1) = E(Y_i(0)|W_i = 0)\\). The term, \\(E(Y_i(0))\\), in ATE equation is replaced by \\(E(Y_i|W_i = 0)\\). In the case of a pure randomized experiment, the ATE is given as: \\[\\begin{equation} ATE = E(Y_i | W_i = 1) - E(Y_i | W_i = 0) \\end{equation}\\] ATE evaluates treatment effect for the whole population by comparing the treated units to the control units. "],["rct.html", "2.3 RCT", " 2.3 RCT Randomized Controlled Trials (RCTs) are the cornerstone of causal inference and are often referred to as the gold standard. The quality of non-experimental studies is frequently assessed by comparing how closely the observational setting approximates an RCT. In an RCT, the Average Treatment Effect (ATE) is identified through the randomization of treatment assignment. This process ensures that the treatment and control groups are comparable, making RCTs a straightforward yet immensely powerful tool for establishing causal relationships. In a simple RCT setting the treatment is binary – the units are either assigned to the treatment group \\((W_i = 0)\\) or the control group \\((W_i = 1)\\). The implicit assumption in this design is that each unit has an equal probability of being treated. The treatment assignment for the RCT setting can be attained using a Bernoulli process, where each unit has an independent probability \\(\\pi\\) of receiving treatment. Specifically, each unit is assigned to the treatment group with probability \\(\\pi\\) and to the control group with probability \\(1 - \\pi\\). # a bernoulli process of treatment assignment library(ggplot2) fun_treat_assign &lt;- function(N, prob, treat.type){ treatment &lt;- rbinom(N, size = 1, p = prob) dat &lt;- data.frame(treatment = treatment, type = treat.type) return(dat) } # p = 0.5 for each unit dat1 &lt;- fun_treat_assign(N = 10000, prob = 0.5, treat.type = &quot;p = 0.5&quot;) # p = 0.3 for each unit dat2 &lt;- fun_treat_assign(N = 10000, prob = 0.2, treat.type = &quot;p = 0.2&quot;) dat.assign &lt;- rbind(dat1, dat2) # plot ggplot(dat.assign, aes(x = treatment)) + geom_histogram(fill= &quot;skyblue&quot;, color = &quot;black&quot;) + facet_wrap(~ type) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. A practical example of this is an unbiased coin toss used to determine treatment assignment. In this case, a head could correspond to the treatment group (e.g., \\(W_i = 1\\)), and a tail to the control group (e.g., \\(W_i = 0\\)). This method exemplifies Bernoulli-randomization, where the assignment is determined by a random process, ensuring that each unit has an equal probability of being assigned to either group. In an RCT setting, the difference-in-means estimator is given as: \\[\\begin{equation} \\hat{\\tau} = \\frac{1}{N_t}\\sum_{W_i =1} Y_i - \\frac{1}{N_c}\\sum_{W_i =0} Y_i \\end{equation}\\] The difference-in-mean estimator is unbiased and consistent for the average treatment effect. "],["average-treatment-effect-on-the-treated-att.html", "2.4 Average treatment effect on the treated (ATT)", " 2.4 Average treatment effect on the treated (ATT) The average treatment effect on the treated is concerned with the evaluation of treatment effects for only those units that are treated. Formally, it is defined as: \\[\\begin{equation} ATT = E(Y_i(1) - Y_i(0) | W_i = 1) \\end{equation}\\] ATE is only concerned with a subset of the population who received the treatment, \\(E[.|W_i = 1]\\). Here, ATT is comparing outcomes among the treated units in presence of the treatment versus what the outcomes would have been in absence of the treatment only for units receiving the treatment. Hence, only one segment of the counterfactual (potential outcome) is required. In our example, the counterfactual for Alia and Samaira would allow estimation of ATT, whereas ATE requires counterfactual for everyone. The independence condition states that on average the potential outcomes for the treated group in absence of the treatment would be similar to the average outcome for the control group, i.e. \\(E(Y_i(0)|W_i = 1) = E(Y_i(0)|W_i = 0)\\). This allows re-writing ATT as the following: \\[\\begin{align} ATT = E\\{E(Y_i | W_i = 1) - E(Y_i | W_i = 0) | W_i = 1\\} \\\\ ATT = E(Y_i | W_i = 1) - E(Y_i | W_i = 0) \\end{align}\\] The second line follows from the independence assumption which allows this: \\(E\\{E(Y_i | W_i = 0) | W_i = 1\\} = E(Y_i | W_i = 0).\\) Under the independence assumption this means that we can estimate ATT by substrating the averages of exam score across the treated and control units. In purely randomized experiments, if there is a perfect case of compliance, then the ATT will be similar to ATE. "],["an-estimation-example.html", "2.5 An estimation example", " 2.5 An estimation example # create a function to estimate the treatment effects fun_ATE &lt;- function(tau, N){ # @arg tau: true treatment effect # @arg N: sample size # Return tau_hat: estimate of the treatment effect gender &lt;- rbinom(N, size = 1, p = 0.5) race &lt;- rbinom(N, size = 1, p = 0.5) W &lt;- rbinom(N, size = 1, p = 0.5) Y &lt;- 50 + tau * W + gender * 5 + race * 10 + rnorm(n = N, mean = 5, sd = 5) tau_hat &lt;- mean(Y[which(W == 1)]) - mean(Y[which(W == 0)]) return(tau_hat) } # print treatment effect print(paste(&quot;the ATE estimate is: &quot;, fun_ATE(tau = 10, N = 2000))) ## [1] &quot;the ATE estimate is: 9.58556087529999&quot; # run 2000 replications to get a distribution of tau_hats reps &lt;- 2000 tau.hats &lt;- rep(0, reps) for(i in 1:reps){ tau.hats[i] &lt;- fun_ATE(tau = 10, N = 2000) } # histogram of tau hats hist(tau.hats, breaks = 30) # obtaining the standard error print(paste(&quot;the mean of tau hats : &quot;, mean(tau.hats))) ## [1] &quot;the mean of tau hats : 9.98863255855275&quot; print(paste(&quot;the standard error of tau hats : &quot;, sd(tau.hats))) ## [1] &quot;the standard error of tau hats : 0.335121030202925&quot; "],["unconfoundedness-assumption.html", "2.6 Unconfoundedness assumption", " 2.6 Unconfoundedness assumption Most of the time the treatment assignment may not be fully random but can be driven by some selective covariates. Referring to the tutoring example, it may be unethical to disallow someone in the control group who wants to attend the tutoring sessions. As such, tutoring sessions may be voluntarily held, where students can select whether to attend the session. Say, females and Blacks are more likely to attend the tutoring session and both of these variables are also likely to yield higher potential outcome. This means that females and Blacks are more likely to have higher exam score in absence of the treatment compared to males and Whites. It is easy to see that the treatment assignment is correlated with the potential outcomes and the independence assumption is violated. This is true in many cases of observational settings and even in randomized experiments. We require adjustments before being able to estimate treatment effects in such cases. If we understand the treatment mechanism fairly well then we can still proceed further to estimate the treatment effects. For example, suppose the treatment assignment is (voluntarily) more tilted towards females than males and Blacks than Whites. In this case, we would want to invoke unconfoundedness (conditional independence) assumption.2 Formally, this states that \\(Y_i(0), \\; Y_i(1) \\perp W_i | X_i\\). This means that conditional upon the covariates the treatment assignment is random. To estimate ATT one would want to first estimate ATT within each strata: \\(i)\\) female-Black, \\(ii)\\) female-White, \\(iii)\\) male-Black, and \\(iv)\\) male-White, and take a weighted average of the strata-specific ATEs by using the proportion of the sample in the given strata as weights. The conditional independence assumption means that within each strata treatment assignment is random. The following code first estimates the strata specific ATTs and then summarizes them using the weighted average. Note that the true treatment effect is 10. fun_ATE2 &lt;- function(N, tau){ # @arg tau: true treatment effect # @arg N: sample size # Return tau_hat: estimate of the treatment effect using conditional randomness assumption # Return tau_hat2: estimate of the treatment effect wrongly using unconditional independence assumption # Return reg_tau: estimate from conditioning using regression but from a misspecified model # create pseudo data gender &lt;- rbinom(N, size = 1, p = 0.5) race &lt;- rbinom(N, size = 1, p = 0.5) W &lt;- rbinom(N, size = 1, p = 0.2 + 0.4 * (gender &gt; 0) + 0.2 * (race &gt; 0)) Y &lt;- 40 + 10 * W + gender * 2 + race * 5 + 25 * race * gender + rnorm(n = N, mean = 5, sd = 5) # female-Blacks tau_hat1 &lt;- mean(Y[which(W == 1 &amp; gender == 1 &amp; race == 1)]) - mean(Y[which(W == 0 &amp; gender == 1 &amp; race == 1)]) w1 &lt;- sum(gender == 1 &amp; race == 1) / N # female-Whites tau_hat2 &lt;- mean(Y[which(W == 1 &amp; gender == 1 &amp; race == 0)]) - mean(Y[which(W == 0 &amp; gender == 1 &amp; race == 0)]) w2 &lt;- sum(gender == 1 &amp; race == 0) / N # male-Blacks tau_hat3 &lt;- mean(Y[which(W == 1 &amp; gender == 0 &amp; race == 1)]) - mean(Y[which(W == 0 &amp; gender == 0 &amp; race == 1)]) w3 &lt;- sum(gender == 0 &amp; race == 1) / N # male-Whites tau_hat4 &lt;- mean(Y[which(W == 1 &amp; gender == 0 &amp; race == 0)]) - mean(Y[which(W == 0 &amp; gender == 0 &amp; race == 0)]) w4 &lt;- sum(gender == 0 &amp; race == 0) / N tau_hat &lt;- tau_hat1 * w1 + tau_hat2 * w2 + tau_hat3 * w3 + tau_hat4 * w4 tau_hat2 &lt;- mean(Y[W == 1]) - mean(Y[W == 0]) # a mis-specified regression model reg &lt;- lm(Y ~ W + gender) reg_tau &lt;- coefficients(reg)[[2]] return(list(table(gender[W == 1]), table(race[W == 1]), tau_hat, tau_hat2, reg_tau)) } ATE2_results &lt;- fun_ATE2(N = 20000, tau = 10) print(paste(c(&quot;treated males: &quot;, &quot;treated females: &quot;) , ATE2_results[[1]])) ## [1] &quot;treated males: 2996&quot; &quot;treated females: 7018&quot; print(paste(c(&quot;treated Whites: &quot;, &quot;treated Blacks: &quot;) , ATE2_results[[2]])) ## [1] &quot;treated Whites: 3989&quot; &quot;treated Blacks: 6025&quot; print(paste(&quot;ATE conditioned on Xs is :&quot;, ATE2_results[[3]])) ## [1] &quot;ATE conditioned on Xs is : 9.77831783321685&quot; print(paste(&quot;ATE not conditioned on Xs is :&quot;, ATE2_results[[4]])) ## [1] &quot;ATE not conditioned on Xs is : 19.1623140630989&quot; # get tau_hats from replications store &lt;- rep(0, reps) store2 &lt;- store store.reg &lt;- store for(i in 1:reps){ ATE.results &lt;- fun_ATE2(N = 20000, tau = 10) store[i] &lt;- ATE.results[[3]] store2[i] &lt;- ATE.results[[4]] store.reg[i] &lt;- ATE.results[[5]] } # histogram of tau_hat conditioned hist(store, main = &quot;tau hats conditioned&quot;) print(paste(&quot;The standard error from the conditioned approach is:&quot;, sd(store))) ## [1] &quot;The standard error from the conditioned approach is: 0.0809935870911819&quot; hist(store2, main = &quot;tau hats not conditioned&quot;) The ATT estimate is much closer to the true parameter, 10, when conditioned upon the covariates as compared to an unconditional approach (where the distribution of ATT estimate is centered around 19.3). This example highlights the importance of conditioning on \\(X\\)s when evaluating the treatment effects if the treatment assignment is correlated with the potential outcomes. In this case, Blacks and females are more likely to have higher scores in general even without the treatment and both of these subgroups are also more likely to be treated. Treatment is not only non-random but is systematically correlated with the outcomes. Since we have the perfect information on the treatment assignment mechanism, after conditioning for the covariates the treatment assignment is essentially random. In other words, within Black vs. White race groups, for example, the treatment assignment is randomly allocated. This allows estimation of ATE for each subgroup or strata. After estimating ATE for each strata, the ATEs are averaged using the sample size of the strata as weights. One problem with the approach highlighted above is that in complex settings, with many determinants (multi-dimensionality) of treatment or in presence of continuous covariates, the sub-space required for the analyses highlighted above will be thinned out too soon. As an alternative, regression framework has been rigorously used as a tool-kit to control for covariates. While there are benefits of using a regression framework, it is by nomeans a panacea. This is especially true if the regression models are misspecified. Below we will use a misspecified version of the regression model to see if we can recover the treatment estimate close to the true value. print(paste(&quot;ATE estimated from misspecified regression model:&quot;, ATE2_results[[5]])) ## [1] &quot;ATE estimated from misspecified regression model: 13.9933545513952&quot; print(paste(&quot;The standard error is:&quot;, sd(store.reg))) ## [1] &quot;The standard error is: 0.181184274905858&quot; hist(store.reg, main = &quot;Treatment effects using regression&quot;) The terms unconfoundedness and conditional independence (\\(heart\\; disease \\perp age \\; | \\; cholestrol\\)) are used interchangebly in causal inference literature. Conditional independence is a broader term that relates to the general field of probability and statistics, whereas unconfoundedness is more specific to causal inference. Unconfoundedness implies a specific kind of conditional independence, specific to causal inference.↩︎ "],["discussion.html", "2.7 Discussion", " 2.7 Discussion In our discussion, we explored the causal effect through the lens of the potential outcome framework, emphasizing the crucial assumptions needed for its accurate identification. We particularly focused on the independence assumption and the unconfoundedness assumption, alongside the Stable Unit Treatment Value Assumption (SUTVA) and the overlap assumption. These assumptions play a pivotal role in ensuring valid causal inference, with the conditional independence assumption being highly effective in randomized controlled trials. However, in observational studies where randomization is not possible, the conditional independence assumption can be quite stringent and challenging to meet as there might be unobserved variables driving the treatment assignment. Consequently, it is essential to leverage alternative methodologies designed for observational settings. Exploring approaches such as propensity score matching, instrumental variables, regression discontinuity design, and difference-in-differences can help overcome these challenges and improve the robustness of causal effect estimation when randomization is not feasible. By employing these methods, we can better navigate the complexities of observational data and draw more reliable causal inferences. "],["reference.html", "2.8 Reference", " 2.8 Reference "],["ipw-and-aipw.html", "3 IPW and AIPW ", " 3 IPW and AIPW "],["average-treatment-effect-a-re-cap.html", "3.1 Average Treatment Effect – A Re-cap", " 3.1 Average Treatment Effect – A Re-cap In a purely randomized controlled experiment the treatment assignment is random by construction. In other words, treated individuals have equal probability of being untreated: \\(P(W_i = 1) = P(W_i = 0)\\). This leads to: \\(W_i \\perp \\{Y_i(0), \\; Y_i(1)\\}\\). Here, \\(W_i = \\{0, \\; 1\\}\\) is the treatment assignment; \\(Y_i(0)\\) and \\(Y_i(1)\\) are potential outcomes without and with treatment. ATE. The target is to estimate the average treatment effect: \\(ATE = E[Y_i(1) - Y_i(0)]...... \\; equation (1)\\) Note that using two assumptions: i) \\(W_i \\perp \\{Y_i(0), \\; Y_i(1)\\}\\) (unconfoundedness); and ii) \\(Y_i(W) = Y_i\\) (SUTVA); the ATE estimate \\(\\hat{\\tau}\\) can be written as: \\(\\hat{\\tau} = \\frac{1}{N_T} \\sum_{W_i = 1} Y_i - \\frac{1}{N_C} \\sum_{i \\in W_i = 0} Y_i ......... equation (2)\\), where \\(n_T\\) and \\(n_C\\) are the number of treated and control units, respectively. When assumption \\(i)\\) fails, we can no longer estimate ATE using equation (2). However, we can invoke conditional unconfoundedness to estimate ATE. Unconfoundedness: The treatment assignment is as good as random once we control for \\(X\\)s. ${W_i {Y_i(0), ; Y_i(1)} | X_i } ; for ; all ; x $. Mostly in observational settings the assumption that treatment assignment is independent of the potential outcomes is false. However, if all \\(X\\)s (features) that influence the treatment are observed, then we can use unconfoundedness for causal inference. "],["a-simple-example.html", "3.2 A simple example", " 3.2 A simple example Say, you are interested in evaluating the effect of tutoring program initiated following the first exam on grades at an introductory level course. For simplicity, the possible grades are A and B. However, students who received B on their first exam are more likely to attend the tutoring session. In other words, \\(P(W_i = 1 | Y_{iFE} = A) &lt; P(W_i = 1 | Y_{iFE} = B)\\). In this case, the treatment assignment is correlated with the past grade, which can predict the grade on the second exam. Hence, using equation (2) to estimate effects of the tutoring program will result in biased estimate. Since we know that the probability of treatment is influenced by the grade on the first exam, we can estimate the conditional average treatment effect (CATE) and average them using weights to form an estimate of ATE. Let’s take a look at the data. grade_mat &lt;- function(grade_vec){ grade &lt;- matrix(0, nrow =2, ncol = 3) grade[, 1] &lt;- c(&quot;Treat&quot;, &quot;Control&quot;) grade[ ,2] &lt;- c(grade_vec[1], grade_vec[2]) grade[ ,3] &lt;- c(grade_vec[3], grade_vec[4]) return(grade) } # Y_iFS == A grade &lt;- grade_mat(c(5, 9, 2, 4)) colnames(grade) &lt;- c(&quot;1st Exam = A&quot;, &quot;A (2nd Exam)&quot;, &quot;B (2nd Exam)&quot;) grade %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) 1st Exam = A A (2nd Exam) B (2nd Exam) Treat 5 2 Control 9 4 grade &lt;- grade_mat(c(15, 1, 5, 4)) colnames(grade) &lt;- c(&quot;1st Exam = B&quot;, &quot;A (2nd Exam)&quot;, &quot;B (2nd Exam)&quot;) grade %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) 1st Exam = B A (2nd Exam) B (2nd Exam) Treat 15 5 Control 1 4 \\(\\hat{\\tau}_{FE=A} = \\frac{5}{7} - {9}{13} = 2.1 \\; pp\\) \\(\\hat{\\tau}_{FE=B} = \\frac{15}{20} - {1}{5} = 55 \\; pp\\) \\(\\hat{\\tau}_{AGG} = \\frac{20}{45} \\hat{\\tau}_{FE=A} - \\frac{25}{45} \\hat{\\tau}_{FE=B} = 31.48 \\; pp\\). The first two are CATEs, which is then averaged to form ATE using appropriate weights on the third line. This simple example using the discreate feature space provides intuition that if variables influencing the treatment assignment are observed then ATE estimate can be uncovered by taking weighted average of CATE estimates (these are also group-wise ATE).3 In this case, CATEs are different across the two sub-groups. Sometimes the core interest of analysis can be uncovering the heterogeneous treatment effects, which motivates estimation and inference on CATEs across two or more sub-groups.↩︎ "],["continuous-features-and-propensity-score.html", "3.3 Continuous features and propensity score", " 3.3 Continuous features and propensity score Previously we discussed the case of a discrete feature. In this case we estimates group-wise ATE and used the weighted average to obtain ATE. When there are many features, this approach is prone to the curse of dimensionality. Moreover, if features are continuous, we won’t be able to estimate ATE at each value of \\(x \\in \\chi\\) due to lack of enough sample size. Instead of estimating group-wise ATE and averaging them, we would want to use a more indirect approach. This is when propensity score comes in. The assumption is that we have collected enough features (discrete, continuous, interactions, higher degree polynomials) to back unconfoundedness. This again means that the treatment assignment is as good as random after controlling for \\(X_i\\). More formally, \\(W_i \\perp \\{Y_i(0), \\; Y_i(1)\\} | \\; X_i \\; ............. equation(3)\\) Intuitively, after accounting for the features that influence who receives treatment, the treatment assignment is as good as random among the sub-group with similar features. But in actuality we are not interested in splitting groups to estimate group-wise treatment effects. Propensity score \\(e(x)\\). The probability of being treated conditional on features \\(X\\)s. \\(e(x) = P(W_i = 1 | X_i = x) \\; ............. equation(4)\\). The key property of the propensity score is that it balances units in the treatment and control groups. If equation (3) holds, we can write the following: \\(W_i \\perp \\{Y_i(0), \\; Y_i(1)\\} | \\; e(X_i) \\; ............. equation(5)\\) What equation (5) says is that instead of controlling for \\(X\\) one can control for the probability of treatment \\((e(X))\\) to establish the desired property that the treatment is as good as random. The implication of equation (5) is that if we partition observations in groups with similar propensity score then we can estimate group-wise treatment effects and aggregate to form an estimate for ATE. This can be done using the propensity score stratification. "],["propensity-score-stratification.html", "3.4 Propensity score stratification", " 3.4 Propensity score stratification Order observations according to their estimated propensity score. \\(\\hat{e}(X)_{i1}, \\; \\hat{e}(X)_{i2}, ... \\; \\hat{e}(X)_{iN}\\) Form \\(J\\) strata of equal size and take the simple difference in mean between the treated and control units within each strata. These are \\(\\hat{\\tau}_j\\) for \\(j = \\{1, \\; 2, \\; ..., \\; N\\}\\). Form the ATE, \\(\\hat{\\tau}_{Strat} = \\frac{1}{J} \\sum_{j = 1}^{J} \\hat{\\tau}_j\\) Here, \\(\\hat{\\tau}_{Strat}\\) is consistent for \\(\\tau\\), meaning that \\(\\hat{\\tau}_{Strat} \\rightarrow_p \\tau\\) given that \\(\\hat{e}(x)\\) is consistent for \\(e(x)\\) and the number of strata grows appropriately with \\(N\\). However, one needs to set the number of strata, which can be a bit ad-hoc. "],["ipw-and-estimation.html", "3.5 IPW and Estimation", " 3.5 IPW and Estimation A more natural way to exploit the condition of unconfoundedness is to weight observations by their propensity score, which is known as the inverse probability weighting. As before \\(\\hat{e}(x)\\) is defined as an estimated propensity score. \\(\\hat{\\tau}_{IPW} = \\frac{1}{N}\\sum_{i = 1}^{N} (\\frac{Y_i . W_i}{\\hat{e}(X_i)} - \\frac{Y_i . (1-W_i)}{1 - \\hat{e}(X_i)}) \\; ....equation(6)\\) Intuitively, observations with high propensity score within the treated group are weighted down, while observations with higher propensity score in the control group are weighted more. In this way, propensity score is used to balance the differences in features across the treatment and control groups. Note that the validity of \\(\\hat{\\tau}\\) still hinges on the unconfoundedness assumption. Limitation of IPW Estimate. One way to analyze the accuracy of \\(\\hat{\\tau}_{IPW}\\) is to compare it with the oracle IPW estimate, \\(\\hat{\\tau}_{IPW}^{*}\\). The oracle estimate is obtained from the known propensity score. Readers are referred to Wager (2018) lecture notes regarding the details of the comparison. Briefly, comparison between \\(\\hat{\\tau}_{IPW}^{*}\\) and \\(\\hat{\\tau}_{AGG}\\) suggests that the oracle IPW under-performs \\(\\hat{\\tau}_{AGG}\\). In other words, the variance of the oracle estimate is larger than that of \\(\\hat{\\tau}_{AGG}\\). Algorithmically, we can form score as: \\((\\frac{Y_i \\times W_i}{\\hat{e}(X_i)} - \\frac{Y_i \\times (1-W_i)}{1 - \\hat{e}(X_i)})\\) The mean of it results to \\(\\hat{\\tau}\\) and the standard error of the estimate is simply \\(\\frac{\\hat{\\sigma}_{score}}{\\sqrt{N}}\\). Estimation ################################# # Author: VS # Last Revised: Jan 16, 2024 # Keywords: IPW, AIPW, GRF # # # ################################# set.seed(194) # Generate Data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) prob &lt;- 0.4 + 0.2 * (X[, 1] &gt; 0) Y &lt;- 2.56 * W + X[, 2] + pmax(X[, 1], 0) + rnorm(n) plot(X[, 1], X[, 2], col = as.factor(W)) #paste0(&quot;average treatment effect is: &quot;, round(mean(pmax(X[, 1], 0)), 3)) ################################# ################################# # # Inverse Probability Weighting # ################################# ################################# # use the random forest to get the propensity score dat &lt;- data.frame(W, X) n_features &lt;- length(setdiff(names(dat), &quot;W&quot;)) # A. ranger (probability tree) rf1_ranger &lt;- ranger( W ~ ., data = dat, mtry = min(ceiling(sqrt(n_features) + 20), n_features), num.trees = 2000, probability = TRUE ) # OOB predictions from ranger p.ranger &lt;- rf1_ranger$predictions[, 1] # B. probability tree using GRF # cross-fitting index K &lt;- 10 ind &lt;- sample(1:length(W), replace = FALSE, size = length(W)) folds &lt;- cut(1:length(W), breaks = K, labels = FALSE) index &lt;- matrix(0, nrow = length(ind) / K, ncol = K) for(f in 1:K){ index[, f] &lt;- ind[which(folds == f)] } # Build RF using GRF P(W = 1 | X) fun.rf.grf &lt;- function(X, W, predictkfold){ rf_grf &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) p.grf &lt;- predict(rf_grf, predictkfold)$predictions return(p.grf) } # storing predict.mat &lt;- matrix(0, nrow = nrow(index), ncol = K) tauk &lt;- rep(0, K) tauk_oracle &lt;- rep(0, K) weighttau &lt;- rep(0, K) score &lt;- list() score_oracle &lt;- list() # for each fold i use other folds for estimation for(i in seq(1:K)){ predict.mat[, i] &lt;- fun.rf.grf(X = X[c(index[, -i]), ], W = W[index[, -i]], predictkfold = X[c(index[, i]), ]) # fold-specific treatment effect score[[i]] &lt;- ((W[index[, i]] * Y[index[, i]]) / (predict.mat[, i])) - (((1 - W[index[, i]]) * Y[index[, i]]) / (1 - predict.mat[, i])) score_oracle[[i]] &lt;- ((W[index[, i]] * Y[index[, i]]) / (prob[i])) - (((1 - W[index[, i]]) * Y[index[, i]]) / (1 - prob[i])) tauk[i] &lt;- mean(score[[i]]) tauk_oracle[i] &lt;- mean(score_oracle[[i]]) } # ipw using oracle propensity score and propensity score estimated from grf alpha &lt;- 0.05 # 5 percent level of significance #ipw.ranger &lt;- mean(((W * Y) / (p.ranger)) - (((1 - W) * Y) / (1 - p.ranger))) ipw.grf &lt;- weighted.mean(tauk, weights = weighttau) ipw.oracle &lt;- weighted.mean(tauk_oracle, weights = weighttau) sd.ipw &lt;- sd(unlist(score)) sd.oracle &lt;- sd(unlist(score_oracle)) ll &lt;- ipw.grf - (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ul &lt;- ipw.grf + (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ll_oracle &lt;- ipw.oracle - (sd.oracle / sqrt(length(unlist(score_oracle)))) * qnorm(1 - alpha/2) ul_oracle &lt;- ipw.oracle + (sd.oracle / sqrt(length(unlist(score_oracle)))) * qnorm(1 - alpha/2) result.ipw &lt;- c(&quot;IPW estimate&quot; = round(ipw.grf, 3), &quot;se&quot; = round(sd.ipw / (sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll, 3), &quot;upper bound&quot; = round(ul, 3)) result.oracle.ipw &lt;- c(&quot;IPW Oracle estimate&quot; = round(ipw.oracle, 3), &quot;se&quot; = round(sd.oracle / (sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll_oracle, 3), &quot;upper bound&quot; = round(ul_oracle, 3)) print(result.ipw) ## IPW estimate se lower bound upper bound ## 2.551 0.100 2.354 2.748 print(result.oracle.ipw) ## IPW Oracle estimate se lower bound upper bound ## 2.705 0.106 2.497 2.913 Note that \\(e(x)\\) is estimated via cross-fitting. 1. The data is divided into \\(K\\)-folds. 2. For each fold \\(k\\), model building is administered using \\(-k\\) folds. 3. Using Step 2, predictions are generated for units in the \\(k^{th}\\) fold. "],["aipw-and-estimation.html", "3.6 AIPW and Estimation", " 3.6 AIPW and Estimation Augmented Inverse Probability Weighting (AIPW) provides a robust way to estimate ATE by alleviating the limitation of IPW estimate. Following the IPW approach, estimation of ATE is given in equation (6). The other approach to estimate \\(\\tau\\) is to think of it from the conditional response approach. Write \\(\\mu_{w}(x) = E[Y_i| \\; X_i = x, W_i = w]\\). Then: \\(\\tau(x) = E[Y_i| \\; X_i = x, W_i = 1] - E[Y_i| \\; X_i = x, W_i = 0]\\) This is the regression outcome approach, where \\(\\tau = E[\\mu_{1}(x) - \\mu_{0}(x)]\\). The consistent estimator can be formed by using: \\(\\hat{\\tau}(x) = N^{-1} \\sum_{i = 1}^{N} \\mu_{1}(X_i) - \\mu_{0}(X_i)\\). AIPW approach combines both IPW approach as well as regression outcome approach to estimate \\(\\tau\\). \\(\\hat{\\tau}_{AIPW} = \\frac{1}{N} \\sum_{i = 1}^{N} (\\mu_{1}(X_i) - \\mu_{0}(X_i) + \\frac{(Y_i - \\hat{\\mu}_1(X_i)). W_i}{\\hat{e}(X_i)} - \\frac{(Y_i - \\hat{\\mu}_0(X_i)). (1-W_i)}{1 - \\hat{e}(X_i)})\\) ML approach using cross-fitting is used to estimate both \\(\\hat{e}(x)\\) and \\(\\hat{\\mu}_{w}(x)\\). Following the cross-fitting structure, we can formally write the estimate for \\(\\tau\\) as: \\(\\hat{\\tau}_{AIPW} = \\lowerbracket{\\frac{1}{N} \\sum_{i = 1}^{N} (\\mu_{1}^{-k(i)}(X_i) - \\mu_{0}^{-k(i)}(X_i)}_{consistent \\; estimate \\; of \\; \\tau} + \\frac{(Y_i - \\hat{\\mu}_1^{-k(i)}(X_i)). W_i}{\\hat{e}^{-k(i)}(X_i)} - \\frac{(Y_i - \\hat{\\mu}_0^{-k(i)}(X_i)). (1-W_i)}{1 - \\hat{e}^{-k(i)}(X_i)})\\) The AIPW approach can be thought of estimating ATE taking the difference across conditional responses. Next, the residuals are adjusted using weights given by the propensity score. There are two attractive features of AIPW estimate. First, $_{AIPW} is consistent as long as \\(\\hat{e}(x)\\) or \\(\\hat{\\mu}_{w}(x)\\) is consistent. This is because \\(E[(Y_i - \\hat{\\mu}_{W_i}(X_i)) \\approx 0\\). Second, \\(\\hat{\\tau}_{AIPW}\\) is a good approximation to oracle \\(\\hat{\\tau}_{AIPW}^{*}\\) as long as \\(\\hat{\\mu}(.)\\) and \\(\\hat{e}(.)\\) are reasonably accurate. If one estimate is highly accurate, then it can compensate lack of accuracy on the other estimate. If both \\(\\hat{\\mu}(.)\\) and \\(\\hat{e}(.)\\) are \\(\\sqrt{n}\\)-consistent4, then the following holds. \\(\\sqrt{n}(\\hat{\\tau}_{AIPW} - \\hat{\\tau}_{AIPW}^{*}) \\rightarrow_p 0\\). ####################### # # Augmented IPW (aipw) # ####################### #n_features2 &lt;- length(setdiff(names(dat2), &quot;Y&quot;)) # ranger #funrf_ranger &lt;- function(dat){ # rf2 &lt;- ranger( # Y ~ ., # data = dat, # mtry = min(ceiling(sqrt(n_features) + 20), n_features), # respect.unordered.factors = &quot;order&quot;, # seed = 123, # num.trees = 2000 # ) # return(rf2) #} # storing predict.mat2a &lt;- matrix(0, nrow = nrow(index), ncol = K) predict.mat2b &lt;- predict.mat2a aipwK &lt;- rep(0, K) weightaipK &lt;- rep(nrow(index) / length(index), K) for(i in seq(1:K)){ # E(Y | X, W = 1) using cross-fitting predict.mat2a[, i] &lt;- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], predictkfold = cbind(X[c(index[, i]), ], 1)) # E(Y | X, W = 0) using cross-fitting predict.mat2b[, i] &lt;- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], predictkfold = cbind(X[c(index[, i]), ], 0)) noise &lt;- ((W[index[, i]] * (Y[index[, i]] - predict.mat2a[, i])) / (predict.mat[, i])) - (((1 - W[index[, i]]) * (Y[index[, i]] - predict.mat2b)) / (1 - predict.mat[, i])) score[[i]] &lt;- predict.mat2a[, i] - predict.mat2b[, i] + noise aipwK[i] &lt;- mean(score[[i]]) } aipw.grf &lt;- weighted.mean(aipwK, weights = weightaipK) sd.aipw &lt;- sd(unlist(score)) ll &lt;- aipw.grf - (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ul &lt;- aipw.grf + (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) result.aipw &lt;- c(&quot;AIPW Est.&quot; = round(aipw.grf, 3), &quot;se&quot; = round(sd.aipw/(sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll, 3), &quot;upper bound&quot; = round(ul, 3)) ###################### # grf ###################### # Train a causal forest tau.forest &lt;- causal_forest(X, Y, W) # Estimate the conditional average treatment effect on the full sample (CATE). grf_ate &lt;- average_treatment_effect(tau.forest, target.sample = &quot;all&quot;) grf_att &lt;- average_treatment_effect(tau.forest, target.sample = &quot;treated&quot;) ## PRINT ALL #print(paste0(&quot;average treatment effect is: &quot;, round(mean(pmax(X[, 1], 0)), 3))) print(paste(&quot;treatment effects according to naive estimator:&quot;, round(mean(Y[which(W == 1)]) - mean(Y[which( W == 0)]), 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to naive estimator: 2.674&quot; print(paste(&quot;treatment effects according to IPW using&quot;, K, &quot;fold cross-fittin:&quot;, round(ipw.grf, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to IPW using 10 fold cross-fittin: 2.551&quot; print(paste(&quot;treatment effects according to IPW oracle:&quot;, round(ipw.oracle, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to IPW oracle: 2.705&quot; print(paste(&quot;treatment effects according to AIPW using&quot;, K, &quot;fold cross-fitting:&quot;, round(aipw.grf, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to AIPW using 10 fold cross-fitting: 2.563&quot; print(paste(&quot;treatment effects according to GRF:&quot;, round(grf_ate[[1]], 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to GRF: 2.545&quot; print(result.ipw) ## IPW estimate se lower bound upper bound ## 2.551 0.100 2.354 2.748 print(result.aipw) ## AIPW Est. se lower bound upper bound ## 2.563 0.033 2.498 2.628 print(grf_ate) ## estimate std.err ## 2.54516745 0.04705196 This means that \\(\\hat{\\mu}(.)\\) converges to \\(\\hat{\\mu}\\) at the ↩︎ "],["assessing-balance.html", "3.7 Assessing Balance", " 3.7 Assessing Balance ########################################## # # # Assessing Balance # # ########################################## XX &lt;- X[c(index), ] YY &lt;- Y[c(index)] WW &lt;- W[c(index)] e.hat &lt;- c(predict.mat) # unadjusted means.treat &lt;- apply(XX[WW == 1, ], 2, mean) means.control &lt;- apply(XX[WW == 0, ], 2, mean) abs.mean.diff &lt;- abs(means.treat - means.control) var.treat &lt;- apply(XX[WW == 1, ], 2, var) var.control &lt;- apply(XX[WW == 0, ], 2, var) std &lt;- sqrt(var.treat + var.control) # adjusted means.treat.adj &lt;- apply(XX*WW / e.hat, 2, mean) means.control.adj &lt;- apply(XX*(1 - WW) / (1 - e.hat), 2, mean) abs.mean.diff.adj &lt;- abs(means.treat.adj - means.control.adj) var.treat.adj &lt;- apply(XX * WW / e.hat, 2, var) var.control.adj &lt;- apply(XX * (1 - WW) / (1 - e.hat), 2, var) std.adj &lt;- sqrt(var.treat.adj + var.control.adj) # plot unadjusted and adjusted differences par(oma=c(0,4,0,0)) plot(-2, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, xlim=c(-.01, 1.01), ylim=c(0, ncol(XX)+1), main=&quot;&quot;) axis(side=1, at=c(-1, 0, 1), las=1) lines(abs.mean.diff / std, seq(1, ncol(XX)), type=&quot;p&quot;, col=&quot;blue&quot;, pch=19) lines(abs.mean.diff.adj / std.adj, seq(1, ncol(XX)), type=&quot;p&quot;, col=&quot;orange&quot;, pch=19) legend(&quot;topright&quot;, c(&quot;Unadjusted&quot;, &quot;Adjusted&quot;), col=c(&quot;blue&quot;, &quot;orange&quot;), pch=19) abline(v = seq(0, 1, by=.25), lty = 2, col = &quot;grey&quot;, lwd=.5) abline(h = 1:ncol(XX), lty = 2, col = &quot;grey&quot;, lwd=.5) mtext(paste0(&quot;X&quot;, seq(1, ncol(XX))), side=2, cex=0.7, at=1:ncol(XX), padj=.4, adj=1, col=&quot;black&quot;, las=1, line=.3) abline(v = 0) hist(e.hat, breaks = 100, freq = FALSE) "],["causal-forest.html", "4 Causal Forest ", " 4 Causal Forest "],["introduction.html", "4.1 Introduction", " 4.1 Introduction The generalized random forest is a method that is quite flexible in estimating the quantity of interest. The theory of it is built using the moment criterion: \\(E[\\psi_{\\theta_i, \\; \\upsilon_i} (O_i) | X_i] = 0, \\; for \\; all \\; x \\; in \\; \\chi\\) Getting down to the nuts and bolts of the theory is beyond the scope of this write-up. Rather, we would want to take a closer look at causal forests – a component of GRF framework. "],["summary-of-grf.html", "4.2 Summary of GRF", " 4.2 Summary of GRF It seeks a generalized way to conduct causal inference under a non-parametric framework. GRF relies on random forest. Methods developed to aid causal inference such as: \\(i)\\) randomized controlled trial, \\(ii)\\) comparison between treatment and control units under unconfoundedness assumption, \\(iii)\\) difference-in-differences, and \\(iv)\\) panel data methods; can fit into GRF framework. To do so, one needs to feed in the method-specific encoding into the GRF framework (to guide the splitting process). "],["motivation-for-causal-forests.html", "4.3 Motivation for Causal Forests", " 4.3 Motivation for Causal Forests Let’s expand on estimating the average treatment effect of a treatment intervention \\(W\\). The specifics are listed as: \\(W_i \\in \\{0, \\; 1\\}\\): treatment intervention \\(X_i\\): covariates \\(Y_i\\): response/outcome In the parametric framework \\(\\tau\\), the treatment effect, is estimated using the following specification: \\(Y_i = \\tau W_i + \\beta_1 X_i + \\epsilon_i\\) The validity of \\(\\hat{\\tau}\\) as a causal estimand is justified under the following three assumptions. Unconfoundedness: \\(Y^{(0)}_i, \\; Y^{(1)}_i \\perp W_i | X_i\\). Treatment assignment is independent of the potential outcome once conditioned on the covariates. In other words, controling for covariates makes the treatment assignment as good as random. \\(X_i\\)s influence \\(Y_i\\)s in a linear way. The treatment effect is homogeneous. Assumption 1 is the identification assumption. In the traditional sense, one can control for \\(X_i\\)s in the regression framework and argue that this assumption is met. Even if all \\(X\\)s that influence the treatment assignment are observed (this is the assumption that we make throughout), we are unsure how \\(X\\)s affect the treatment. Often \\(X\\)s can affect treatment in a non-linear way. Assumptions 2 and 3 can be questioned and relaxed. One can let data determine the way \\(X\\) needs to be incorporated in the model specification (relaxing assumption 2). Moreover, treatment effects can vary across some covariates (relaxing assumption 3). First, lets relax assumption 2. This leads to the following partially linear model: \\(Y_i = \\tau W_i + f(X_i) + \\epsilon_i \\;............. equation 1\\) where, \\(f\\) is a function that maps out how \\(X\\) affects \\(Y\\). However, we don’t know \\(f\\) in practice. So, how do we go about estimating \\(\\tau\\)? The causal forest framework under GRF connects the old-school literature of causal inference with ML methods. Robinson (1988) shows that if two intermediate (nuiscance) objects, \\(e(X_i)\\) and \\(m(X_i)\\) are known, one can estimate \\(\\tau\\). The causal forest framework under GRF utilizes this result. Here: \\(e(X_i)\\) is the propensity score; the probability of being treated. \\(E[W_i| X_i = x]\\) \\(m(X_i)\\) is the conditional mean of \\(Y\\). \\(E[Y_i | X_i = x] = f(x) + \\tau e(x)\\) Demeaning equation 1 (substracting \\(m(x)\\)) gives the following residual-on-residual regression: \\(Y_i - m(x) = \\tau (W_i - e(x)) + \\epsilon \\; .............. equation 2\\) Intuition for equation (2) proceeds as follow. Note that \\(m(x)\\) is the conditional mean of Y given \\(X_i = x\\).5 This means that units with similar \\(X\\)s will have similar estimates for \\(m(x)\\) in \\(W=\\{0, \\; 1\\}\\), which would mean that estimates on \\(e(x)\\) would also be similar for these units across both treatment and control group. Now, consider that the treatment is positive; this will show up in \\(Y_i\\). \\(Y_i - m(x)\\) will be higher for \\(W=1\\) compared to \\(W=0\\) for similar estimates of \\(m(x)\\). On the other side, \\(W_i - e(x)\\) is positive for \\(W=1\\) and negative for \\(W=0\\) for similar estimates of \\(e(x)\\). Such variations in the left and right hand side quantities will allow to capture postive estimates on \\(\\tau\\). To gain ML methods are used to estimate \\(m(x)\\) and \\(e(x)\\) and residual-on-residual regression is used estimate \\(\\tau\\). It turns out that even noisy estimates of \\(e(x)\\) and \\(m(x)\\) can give ``ok” estimate of \\(\\tau\\). How to estimate \\(m(x)\\) and \\(e(x)\\)? Use ML methods (boosting; random forest) Use cross-fitting for prediction. prediction of observation \\(i&#39;s\\) outcome &amp; treatment assignment is obtained without using the observation ``\\(i\\)“. Lets take a look at residual-on-residual in the case of homogeneous treatment effect. # generate data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) # Generate W and Y W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) prob &lt;- 0.4 + 0.2 * (X[, 1] &gt; 0) Y &lt;- 2.5 * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n) # Train regression forests mx &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) ex &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) Wcen &lt;- W - ex$predictions Ycen &lt;- Y - mx$predictions reg &lt;- summary(lm(Ycen ~ Wcen)) reg ## ## Call: ## lm(formula = Ycen ~ Wcen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7014 -0.7096 -0.0075 0.6923 4.0184 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.003044 0.023651 -0.129 0.898 ## Wcen 2.457639 0.048010 51.190 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.058 on 1998 degrees of freedom ## Multiple R-squared: 0.5674, Adjusted R-squared: 0.5672 ## F-statistic: 2620 on 1 and 1998 DF, p-value: &lt; 2.2e-16 print(paste0(&quot;The treatment effect estimate based on residual-on-residual regression is: &quot;, coefficients(reg)[2])) ## [1] &quot;The treatment effect estimate based on residual-on-residual regression is: 2.45763901261754&quot; print(paste0(&quot;The true treatment effect is: &quot;, 2.5)) ## [1] &quot;The true treatment effect is: 2.5&quot; We can also think of \\(m(x)\\) as the case when we ignore \\(W\\), although we know that treatment took place. This way, \\(m(x) = \\mu_{0}(x) + e(x)\\tau\\), where \\(\\mu_{0}(x)\\) is the baseline conditional expectation without the treatment. This makes it easy to see that units with similar features will have similar estimates of \\(m(x)\\).↩︎ "],["causal-forest-1.html", "4.4 Causal Forest", " 4.4 Causal Forest Both regression and causal forests consist of: 1) Building phase; and 2) estimation phase. The intuition regarding the regression/causal forest can be gleaned using the following figure. Figure 4.1: Figure 1. Adaptive weights In this simple case, the sample is partitioned into \\(N_1\\) and \\(N_2\\) neighborhoods accorinng to the splitting rule that the squared difference in sub-sample specific treatment effect is the maximum, i.e., \\(n_{N_1}n_{N_2}(\\tau_{N_1} - \\tau_{N_2})^2\\) is the maximum. This by construction leads to constant treatment effect in the neighborhood, while the effects may vary across the neighborhoods. This intuition allows us to relax assumption 3, and re-write the partially linear estimation framework as: \\(Y_i = \\tau(x) W_i + f(X_i) + \\epsilon_i\\). Here the estimate of the treatment effect \\(\\tau\\) is allowed to vary with the test point \\(x\\). In reference to Figure 1 above, \\(N_1\\) and \\(N_2\\) are neighborhoods where treatment effects are constant. To estimate the treatment effect of the test point \\(x\\), \\(\\tau(x)\\), we would run a weighted residual-on-residual regression of the form. \\(\\tau(x) := lm(Y_i - m(X_i)^{-i} \\sim \\tau(W_i - e(X_i)^{-i}), \\; weights = 1\\{X_i \\in N(x)\\}\\) where \\(m(X_i)^{-i}\\) and \\(e(X_i)^{-i}\\) are obtained from cross-fitting. The weights play a pivotal role here and takes a value 1 if \\(X_i\\) belongs to the same neighborhoods as \\(x\\). In the above figure, examples in \\(N_2\\) receive non-zero weight while those in \\(N_1\\) receive zero weight. However, this example only pertains to a tree. But we’d want to build a forest and apply the same analogy. Adaptive weights. The forest consists of \\(B\\) trees, so the weights for each \\(X_i\\) pertaining to the test point \\(x\\) is based off of all \\(B\\) trees. The causal forest utilizes adaptive weights using random forests. The tree specific weight for an example \\(i\\) at the \\(b^{th}\\) tree is given as: \\(\\alpha_{ib}(x) = \\frac{1(X_i \\in L_{b}(x))}{|L_{b}(x)|}\\), where \\(L(x)\\) is the leaf (neighborhood) that consist of the test sample \\(x\\). The forest specific weight for an example \\(i\\) is given as: \\(\\alpha_{i}(x) = \\frac{1}{B} \\sum_{b = 1}^{B} \\frac{1(X_i \\in L(x))}{|L(x)|}\\) It tracks the fraction of times an obsevation \\(i\\) falls on the same leaf as \\(x\\) in the course of the forest. Simply, it shows how similar \\(i\\) is to \\(x\\). Regression Forest. It utilizes the adaptive weights given to an example \\(i\\) (\\(i = \\{1, \\; 2, \\; ..., N\\}\\)) and constructs a weighted average to form the prediction of \\(x\\). The prediction for \\(x\\) based on the regression forest is: \\(\\hat{\\mu}(x) = \\frac{1}{B}\\sum_{i = 1}^{N} \\sum_{b=1}^{B} Y_{i} \\frac{1(X_i \\in L_{b}(x)}{|L_b(x)|}\\) \\(= \\sum_{i = 1}^{N} Y_{i} \\alpha_{i}\\) Note that this is different from the traditional prediction from the random forest that averages predictions from each tree. \\(\\hat{\\mu}(x.trad) = \\sum_{b = 1}^{B} \\frac{\\hat{Y}_b}{B}\\) Causal Forest. Causal forest is analogous to the regression forest in a sense that the target is \\(\\tau(x)\\) rather than \\(\\mu(x)\\). Conceptually the difference is encoded in the splitting criteria. While splitting, regression forest is based on the criterion: \\(\\max n_{N_1} n_{N_2}(\\mu_{N_1} - \\mu_{N_2})^2\\), whereas the causal forest is based on \\(\\max n_{N_1} n_{N_2}(\\tau_{N_1} - \\tau_{N_2})^2\\). In a world with infinite computing power, for each potential axis aligned split that extends from the parent node, one would estimate treatment effects at two of the child nodes (\\(\\tau_{L}\\) and \\(\\tau_{R}\\)) and go for the split that maximizes the squared difference between child specific treatment effects. However, in practice this is highly computationally demanding and infeasible. The application of causal forest estimates \\(\\tau_{P}\\) at the parent node and uses the gradient based function to guide the split. At each (parent) node the treatment effect is estimated only once. Once the vector of weights are determined for \\(i\\)s, the following residual-on-residual is ran: \\(\\tau(x) := lm(Y_i - m(X_i^{-i}) \\sim \\tau(x)(W_i - e(X_i)^{-i}), \\; weights = \\alpha_i(x)\\) This can be broken down as: Estimate \\(m^{-i}(X_i)\\) and \\(e^{-i}(X_i)\\) using random forest. Then estimate \\(\\alpha_i(x)\\). For each new sample point \\(x\\), a vector of weight will be determined based on adaptive weighting scheme of the random forest. Note that the weights will change for each new test point. Run a weighted residual-on-residual regression given by the equation above. "],["an-example-of-causal-forest.html", "4.5 An example of causal forest", " 4.5 An example of causal forest rm(list = ls()) library(devtools) #devtools::install_github(&quot;grf-labs/grf&quot;, subdir = &quot;r-package/grf&quot;) library(grf) library(ggplot2) # generate data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) # Train a causal forest. W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n) # Train a causal forest c.forest &lt;- causal_forest(X, Y, W) # predict using the training data using out-of-bag prediction tau.hat.oob &lt;- predict(c.forest) hist(tau.hat.oob$predictions) # Estimate treatment effects for the test sample tau.hat &lt;- predict(c.forest, X.test) plot(X.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2), xlab = &quot;x&quot;, ylab = &quot;tau&quot;, type = &quot;l&quot;) lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 2) # estimate conditional average treatment effect (CATE) on the full sample cate &lt;- average_treatment_effect(c.forest, target.sample = &quot;all&quot;) print(paste(&quot;Conditinal Average Treatment Effect (CATE) is: &quot;, cate[[1]])) ## [1] &quot;Conditinal Average Treatment Effect (CATE) is: 0.401862640556794&quot; # estimate conditional average treatment effect on treated catt &lt;- average_treatment_effect(c.forest, target.sample = &quot;treated&quot;) paste(&quot;Conditional Average Treatment Effect on the Treated (CATT)&quot;, catt[[1]]) ## [1] &quot;Conditional Average Treatment Effect on the Treated (CATT) 0.48798303690414&quot; # Add confidence intervals for heterogeneous treatment effects; growing more trees recommended tau.forest &lt;- causal_forest(X, Y, W, num.trees = 4000) tau.hat &lt;- predict(tau.forest, X.test, estimate.variance = TRUE) # for the test sample ul &lt;- tau.hat$predictions + 1.96 * sqrt(tau.hat$variance.estimates) ll &lt;- tau.hat$predictions - 1.96 * sqrt(tau.hat$variance.estimates) tau.hat$ul &lt;- ul tau.hat$ll &lt;- ll tau.hat$X.test &lt;- X.test[,1] ggplot(data = tau.hat, aes(x = X.test, y = predictions)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;grey70&quot;) + geom_line(aes(y = predictions)) + theme_bw() ###################################################### # # # In some cases prefitting Y and W separately may # be helpful. Say they use different covariates. # ###################################################### # Generate a new data n &lt;- 4000 p &lt;- 20 X &lt;- matrix(rnorm(n * p), n, p) TAU &lt;- 1 / (1 + exp(-X[, 3])) W &lt;- rbinom(n, 1, 1 / (1 + exp(-X[, 1] - X[, 2]))) # X[, 1] and X[, 2] influence W Y &lt;- pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n) # X[, 2], X[, 3], X[, 4:6] influence Y. So different set of Xs influence Y # Build a separate forest for Y and W forest.W &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) W.hat &lt;- predict(forest.W)$predictions # this gives us the estimated propensity score (probability of treated) #plot(W.hat, X[, 1], col = as.factor(W)) #plot(W.hat, X[, 2], col = as.factor(W)) forest.Y &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) # note that W is not used here Y.hat &lt;- predict(forest.Y)$predictions # this gives the conditional mean of Y or m(x) #plot(Y, Y.hat) forest.Y.varimp &lt;- variable_importance(forest.Y) forest.Y.varimp ## [,1] ## [1,] 0.004936160 ## [2,] 0.465978865 ## [3,] 0.389054083 ## [4,] 0.044737356 ## [5,] 0.022684898 ## [6,] 0.044626517 ## [7,] 0.001665912 ## [8,] 0.002073618 ## [9,] 0.002138081 ## [10,] 0.001165914 ## [11,] 0.002234663 ## [12,] 0.001985212 ## [13,] 0.002856831 ## [14,] 0.002835794 ## [15,] 0.003005470 ## [16,] 0.001200623 ## [17,] 0.002353086 ## [18,] 0.001385852 ## [19,] 0.001805625 ## [20,] 0.001275441 # selects the important variables selected.vars &lt;- which(forest.Y.varimp / mean(forest.Y.varimp) &gt; 0.2) selected.vars ## [1] 2 3 4 5 6 # Trains a causal forest tau.forest &lt;- causal_forest(X[, selected.vars], Y, W, W.hat = W.hat, Y.hat = Y.hat, # specify e(x) and m(x) tune.parameters = &quot;all&quot;) # See if a causal forest succeeded in capturing heterogeneity by plotting # the TOC and calculating a 95% CI for the AUTOC. train &lt;- sample(1:n, n / 2) train.forest &lt;- causal_forest(X[train, ], Y[train], W[train]) eval.forest &lt;- causal_forest(X[-train, ], Y[-train], W[-train]) rate &lt;- rank_average_treatment_effect(eval.forest, predict(train.forest, X[-train, ])$predictions) rate ## estimate std.err target ## 0.04929502 0.04709752 priorities | AUTOC plot(rate) paste(&quot;AUTOC:&quot;, round(rate$estimate, 2), &quot;+/&quot;, round(1.96 * rate$std.err, 2)) ## [1] &quot;AUTOC: 0.05 +/ 0.09&quot; "],["heterogeneous-treatment-effects.html", "5 Heterogeneous Treatment Effects", " 5 Heterogeneous Treatment Effects This article summarizes heterogeneous treatment effects using ML. Simply put, its defined as the variation in response to treatment across several subgroups. For example, the impacts of Medicaid expansion on labor market outcomes can vary depending on uninsured rate prior to the expansion; the effects of discussion intervention program aimed to normalize disscussion regarding menstruation can increase demand for menstrual health products at a higher rate among those with high psychological cost in the baseline; in personalized medical treatment, we would want to identify the sub-group with higher response to a particular type of treatment. It is different from average treatment effect (ATE) such that the ATE focuses on the whole group, while heterogeneous treatment effect pertains to the specific sub-group characterized by features (\\(X\\)s). In this sense, one can think of ATE as the weighted average of subgroup specific ATEs. Using the potential outcome framework, ATE is given by: \\(E[Y_i^{1} - Y_i^{0}]\\). The heterogeneous treatment is: \\(E[Y_i^{1} - Y_i^{0} | X_i = x ]\\). Its the treatment conditional on \\(X_i\\), which is determined prior to observing the data. Hence, its also termed as the conditional average treatment effect (CATE). One simple example borrowed from Wager’s lecture notes to illustrate the concept is that of smoking in Geneva and Palo Alto. Say, two RCTs are conducted in Palo Alto and Geneva to evaluate whether cash incentives among teenagers can reduce the prevalence of smoking. # Palo-Alto smoke_mat &lt;- function(smoke_vec){ smoke &lt;- matrix(0, nrow =2, ncol = 3) smoke[, 1] &lt;- c(&quot;Treat&quot;, &quot;Control&quot;) smoke[ ,2] &lt;- c(smoke_vec[1], smoke_vec[2]) smoke[ ,3] &lt;- c(smoke_vec[3], smoke_vec[4]) return(smoke) } smoke &lt;- smoke_mat(c(152, 2362, 5, 122)) colnames(smoke) &lt;- c(&quot;Palo Alto&quot;, &quot;Non-S.&quot;, &quot;Smoker&quot;) data.frame(smoke) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) Palo.Alto Non.S. Smoker Treat 152 5 Control 2362 122 smoke &lt;- smoke_mat(c(581, 2278, 350, 1979)) colnames(smoke) &lt;- c(&quot;Geneva&quot;, &quot;Non-S.&quot;, &quot;Smoker&quot;) data.frame(smoke) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) Geneva Non.S. Smoker Treat 581 350 Control 2278 1979 \\(\\hat{\\tau}_{PA} = \\frac{5}{152+5} - \\frac{122}{2362 + 122} \\approx -1.7 pp\\) \\(\\hat{\\tau}_{GVA} = \\frac{350}{581+350} - \\frac{1979}{2278 + 1979} \\approx -8.9 pp\\) \\(\\hat{\\tau} = \\frac{2641}{2641 + 5188}\\tau_{PA} + \\frac{5188}{2641 + 5188}\\tau_{GVA}\\). Here, \\(\\hat{\\tau}_{PA}\\) is an estimate of \\(E[smoke \\;prevalence | \\; W = 1, \\; X = PA] \\; - \\; E[smoke \\;prevalence | \\; W = 0, \\; X = PA]\\), and its the treatment effect particular to Palo Alto. The average treatment effect \\(\\hat{\\tau}\\) is the weighted average of the two treatment effects. "],["some-ways-to-estimate-cate.html", "5.1 Some ways to estimate CATE", " 5.1 Some ways to estimate CATE Robinson’s partially linear model for homogeneous treatment effect is written as: \\(Y_i = \\tau W_i + f(X_i) + \\epsilon_i \\; ........(equation \\; 1)\\) Here, \\(\\tau\\) is assumed constant across sub-spaces of \\(X\\). We can expand to write Robinson’s partially linear model as: \\(Y_i = \\tau(X_i) W_i + f(X_i) + \\epsilon_i \\; ........(equation \\; 2)\\) where, \\(\\tau(.)\\) varies with \\(x\\). Equation 2 can be expressed as residual-on-residual regression format of: \\(Y_i - m(X_i) = \\tau(X_i) (W_i - e(X_i)) + \\epsilon_i \\; ........(equation \\; 3)\\) where, \\(m(x)\\) is the conditional expectation of \\(Y\\) given \\(X\\). \\(m(x) = E[Y_i | \\; X_i = x] = \\mu_{W = 0}(X_i) + \\tau(X_i) e(X_i)\\), where \\(\\mu_{0}(X_i)\\) is the baseline conditional response (in absense of treatment) and \\(e(x) = P(W_i = 1 | \\; X_i = x)\\).6 \\(\\tau(X)\\) is parameterized as \\(\\tau(x) = \\psi(x).\\beta\\), where \\(\\psi\\) is some pre-determined set of basis functions: \\(\\chi \\rightarrow R^k\\). A feasible loss function can be devised using equation 3 and using estimates of \\(m(x)\\) and \\(e(x)\\) from cross-fitting. \\(L = \\frac{1}{n} \\sum_{i = 1}^n((Y_i - \\hat{m}(X_i)^{-k(i)}) - (W_i - \\hat{e}(X_i)^{-k(i)}) \\; \\psi(X_i).\\beta)^2\\). Note that the parameter of interest is \\(\\beta\\). LASSO can be used to estimate \\(\\hat{\\beta}\\), where: \\(\\hat{\\beta} = argmin_{\\beta}\\{L + \\lambda \\; ||\\beta||_{1}\\}\\), where \\(\\lambda\\) is the regularizer on the complexity of \\(\\tau(.)\\).7 Note: The other approach is to use random forest to measure out weight of an observation \\(i\\) in relation to the test point \\(x\\). This approach is done using causal forest in the Generalized Random Forest framework. The distinction between \\(m(x)\\) and \\(m(X_i)\\) is such that the former is estimation performed at the new data point \\(x\\).↩︎ One can build a highly complex model and improve the in-sample fit. However, this model may perform badly while predicting out-of-sample cases. As such, the complexity of the model should be penalized while training the model.↩︎ "],["estimation.html", "5.2 Estimation", " 5.2 Estimation set.seed(194) # Generate Data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) prob &lt;- 0.4 + 0.2 * (X[, 1] &gt; 0) Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmax(X[, 3], 0) + rnorm(n) ################################### ################################### # # # 1. estimate m(X) and e(X) # using cross-fitting # ################################### ################################### # cross-fitting index K &lt;- 10 # total folds ind &lt;- sample(1:length(W), replace = FALSE, size = length(W)) folds &lt;- cut(1:length(W), breaks = K, labels = FALSE) index &lt;- matrix(0, nrow = length(ind) / K, ncol = K) for(f in 1:K){ index[, f] &lt;- ind[which(folds == f)] } # Build a function to estimate conditional means (m(x) and e(x)) using random forest fun.rf.grf &lt;- function(X, Y, predictkfold){ rf_grf &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) p.grf &lt;- predict(rf_grf, predictkfold)$predictions return(p.grf) } # storing predict.mat &lt;- matrix(0, nrow = nrow(index), ncol = K) # to store e(x) predict.mat2 &lt;- predict.mat # to store m(x) # for each fold k use other folds for estimation for(k in seq(1:K)){ predict.mat[, k] &lt;- fun.rf.grf(X = X[c(index[, -k]), ], Y = W[index[, -k]], predictkfold = X[c(index[, k]), ]) predict.mat2[, k] &lt;- fun.rf.grf(X = X[c(index[, -k]), ], Y = Y[c(index[, -k])], predictkfold = X[c(index[, k]), ]) } W.hat &lt;- c(predict.mat) Y.hat &lt;- c(predict.mat2) ################################ ################################ # # 2. Use LASSO to minimize # the loss function ################################ ################################ # rearrange features and response according to index XX &lt;- X[c(index), ] YY &lt;- Y[c(index)] WW &lt;- W[c(index)] resid.Y &lt;- YY - Y.hat resid.W &lt;- WW - W.hat # Create basis expansion of features for(i in seq(1, ncol(XX))) { if(i == 1){ XX.basis &lt;- bs(XX[, i], knots = c(0.25, 0.5, 0.75), degree = 2) }else{ XX.basisnew &lt;- bs(XX[, i], knots = c(0.25, 0.5, 0.75), degree = 2) XX.basis &lt;- cbind(XX.basis, XX.basisnew) } } resid.W.X &lt;- resid.W * XX.basis resid.W.X &lt;- model.matrix(formula( ~ 0 + resid.W.X)) #plot(XX[ ,1], pmax(XX[ , 1], 0)) # cross validation for lasso to tune lambda lasso &lt;- cv.glmnet( x = resid.W.X, y = resid.Y, alpha = 1, intercept = FALSE ) #plot(lasso, main = &quot;Lasso penalty \\n \\n&quot;) # lambda with minimum MSE best.lambda &lt;- lasso$lambda.min lasso_tuned &lt;- glmnet( x = resid.W.X, y = resid.Y, lambda = best.lambda, intercept = FALSE ) #print(paste(&quot;The coefficients of lasso tuned are:&quot;, coef(lasso_tuned), sep = &quot; &quot;)) pred.lasso &lt;- predict(lasso, newx = XX.basis) ######################### # # Causal Forest # ######################### X.test &lt;- matrix(0, nrow = nrow(X), ncol = ncol(X)) X.test[, 1] &lt;- seq(-3, 3, length.out = nrow(X)) tau.forest &lt;- causal_forest(X, Y, W) tau.forest ## GRF forest object of type causal_forest ## Number of trees: 2000 ## Number of training samples: 2000 ## Variable importance: ## 1 2 3 4 5 6 7 8 9 10 ## 0.706 0.037 0.031 0.033 0.035 0.031 0.027 0.041 0.027 0.031 tau.hat &lt;- predict(tau.forest, X.test)$predictions par(oma=c(0,4,0,0)) plot(XX[order(XX[ , 1]), 1], pred.lasso[order(XX[, 1])], ylim = c(0, 3), t = &quot;l&quot;, xlab = &quot; &quot;, ylab = &quot; &quot;, xlim = c(-3, 3), lwd = 1.5) par(new = TRUE) plot(XX[order(XX[, 1]), 1], pmax(XX[order(XX[, 1]), 1], 0), col =&quot;red&quot;, ylim = c(0, 3), t = &quot;l&quot;, xlab = &quot;X1&quot;, ylab = &quot;tao(x)&quot;, xlim = c(-3, 3), lwd = 1.5) par(new = TRUE) plot(X.test[order(X.test[, 1]), 1], tau.hat[order(X.test[, 1])], t = &quot;l&quot;, col = &quot;blue&quot;, ylim = c(0, 3), xlab = &quot;X1&quot;, ylab = &quot;&quot;, xlim = c(-3, 3), lwd = 1.5) legend(&quot;topleft&quot;, c(&quot;Loss min Lasso&quot;, &quot;True Effect&quot;, &quot;Causal Forest&quot;), col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty = rep(1, 3)) "],["some-remarks-and-questions.html", "5.3 Some Remarks and Questions", " 5.3 Some Remarks and Questions For LASSO, we are using the basis of polynomial splines of degree 2 with interior knots at 25th, 75th, and 50th percentiles of each feature. We can see that although the effects are picked up, its slightly late and are lower compared to the true effect. A basis for linear splines performs well in this case. The causal forest framework on the other hand performs better. "]]
