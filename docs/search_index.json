[["index.html", "Causal Inference 1 Preface", " Causal Inference Vinish Shrestha 2026-02-23 1 Preface Simply put, causal inference is about answering the question: * Does X cause Y*? It’s one of the fundamental approaches in modern-day science. Like everything else, the field is ever-evolving and spans across multiple disciplines. For example, say you are interested in evaluating the efficacy of a new vaccine. Or, you are trying to analyze the long-run impacts of school shooting on mental health. Or might be intrigued by how an intervention by U.S. Immigration and Customs Enforcement in Minnesotta affects mental health of Minnesotians. In all these instances, causal inference has a role to play. The gold standard of causal inference is an experimental setting where treatment assignment is randomly allocated. However, experimental designs are not always available and are restricted due to practical constraints such as cost, time, and effort. For instance, it is highly unlikely that you will be using an experimental design for your thesis. In such cases researchers need to rely on non-experimental identification techniques as an alternative. In this write-up, I’ll discuss techniques used to understand causal relationship in non-experimental settings using the potential outcome framework. The general idea is as such: if person “A” is treated, the ideal comparison to person “A” would be person “A” herself, but during the untreated state. However, it is impossible to observe person “A” in both treated and untreated states simultaneously. Hence, we need to find suitable comparisons to person “A” to be able to evaluate the effect of the treatment. In summary, the idea is to form counterfactual outcomes which is used in comparison to the treated unit to estimate the effect of a treatment. This impossibility of observing both potential outcomes for the same individual is often referred to as the fundamental problem of causal inference. Because we can never directly observe the counterfactual outcome, estimating causal effects necessarily requires assumptions. These assumptions allow us to argue that certain observed units can serve as reasonable stand-ins for the missing counterfactuals. Much of causal inference, therefore, is not about finding perfect comparisons, but about carefully justifying why particular comparisons are credible. Causal inference has also benefited massively from modern ML methods, which have opened doors for many possibilities. Where appropriate, I will attempt to combine ML techniques to aid causal inference throughout this write-up. Overall, I’ve tried emphasizing intuition rather than mathematical rigor. I hope that the write-up provides you with solid intuition regarding the topics discussed. However, note that it is by no means comprehensive. This is a work in progress and I intend to update it every semester bit by bit. So far, I’ve written most of the codes in R, while some parts use Python. In the future, I hope to incorporate code examples in both languages. "],["introduction.html", "2 Introduction", " 2 Introduction These lectures aim to explore the concept of cause and effect, a fundamental pursuit in science. At its core, we often ask whether X causes Y , and if so, by how much. The ideal way to address such questions is through a carefully designed experiment—one that is controlled and systematic. In an experimental setting, the researcher typically has control over key aspects of the setup: assigning treatments, ensuring controls are in place, and accurately measuring outcomes. However, the luxury of conducting controlled experiments is often out of reach, especially for many real-world research questions. This collection of lectures focuses on the challenge of causal inference in such situations, specifically when relying on observational data. The emphasis will be on intuitive and empirical approaches, rather than delving deeply into the theoretical foundations of causal inference. "],["a-lab-experiment.html", "2.1 A lab experiment", " 2.1 A lab experiment Let’s consider a simple example. Suppose you want to investigate the importance of sunlight for the growth of a jade plant. How might you proceed? You might start by obtaining two baby jade plants from the same seller. Place jade plant A near the window sill, where it can receive ample sunlight, and place jade plant B in a closet within the same room, where it receives no sunlight. To ensure fairness, keep the room temperature uniform and water both plants equally, once a week. After some time, you evaluate the growth of both plants. Of course, this is just a thought experiment—you wouldn’t actually do this because we already know the plant in the closet would not survive. However, this example illustrates the core idea of a controlled experiment. What’s happening in our thought experiment? We’ve essentially conducted a minimal lab experiment: Treatment Assignment: The “treatment,” or variable of interest, is the exposure to sunlight. You control which plant receives the treatment and which does not. Controlling for Other Variables: Factors like water and temperature, which could also influence plant growth, have been held constant. Measurement: You can (more or less) accurately measure the growth of the plant (e.g., height). Eventually, you can measure which one survived. "],["challenges.html", "2.2 Challenges", " 2.2 Challenges When exploring cause-and-effect relationships, a fundamental challenge arises: we often cannot conduct controlled lab experiments for most real-world questions. Let’s consider an example to illustrate this. Imagine you, as a researcher, want to study the relationship between having a college degree and health outcomes. Ideally, you’d want to observe an individual, say person A, in two states: i) with college level education; and ii) without college level education. Next, you’d want to measure her health outcomes in these two states and make comparison. However, this is not possible with the main reason being that you cannot observe two individuals in different states at a given point in time. Mostly, you’ll have to tackle your research questions with observational data. Let’s look at the observational data. Specifically, your dependent variable is the prevalence of poor health, and your independent variable is the percentage of the population with a college degree. You obtain county-level data for the year 2010, which includes the fraction of people in poor health and the percentage of the population with a college degree. # load in county level uninsured rate data merged with other variables mort_allcauses &lt;- read_feather( file.path(datapath, &quot;NVSS_data_county_2010to2017_merged_allcauses.feather&quot;)) %&gt;% filter(year == 2010 &amp; age == 0 &amp; race_name == &quot;black&quot;) %&gt;% dplyr::select(&quot;countyfips&quot;, &quot;year&quot;, &quot;state.abb&quot;, &quot;expand&quot;, &quot;yearexpand&quot;, &quot;sahieunins_allinc&quot;, &quot;GovernorisDemocrat1Yes&quot;, &quot;mortality_rate1000&quot;, &quot;percap_income_2010&quot;, &quot;rural_urban_code2013a&quot;, &quot;p_college2010&quot;, &quot;prop_black_2010&quot;, &quot;prop_white_2010&quot;, &quot;infant.mort&quot;, &quot;poor.health&quot;, &quot;low.birthweight&quot;) %&gt;% filter(duplicated(.)) %&gt;% arrange(countyfips, year) f0 &lt;- ggplot(subset(mort_allcauses), aes(y = poor.health, x = p_college2010)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + xlab(&quot;percent college 2010&quot;) + ylab(&quot;fraction poor health 2010&quot;) f1 &lt;- ggplot(subset(mort_allcauses), aes(y = percap_income_2010, x = p_college2010)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + xlab(&quot;percent college 2010&quot;) + ylab(&quot;per capita income 2010&quot;) f3 &lt;- ggplot(subset(mort_allcauses), aes(y = sahieunins_allinc, x = p_college2010)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + xlab(&quot;uninsured rate in 2010&quot;) + ylab(&quot;fraction poor health 2010&quot;) f1 ## `geom_smooth()` using formula = &#39;y ~ x&#39; At first glance, the data reveals a relationship between these variables. But how do we interpret this relationship? Does having a college degree cause better health, or is something else at play? Let’s take a look at more wholesome illustrations. f0 + f1 + f3 ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; We see that percent college is negatively correlated with poor health. However, places with higher proportion of college graduates also have higher per capita income and lower uninsured rates. Both income and insurance status may have a causal effect on one’s health. Hence, it is unwise to make a rushed claim that college education leads to better health status. It may, or it may not. We dont know yet! "],["dag-directed-acyclic-graph.html", "2.3 DAG (Directed Acyclic Graph)", " 2.3 DAG (Directed Acyclic Graph) A simple way to keep track of whats going on is to make use of causal diagrams. This is known as Directed Acyclic Graph (DAG) in various fields like statistics, computer science, and epidemiology. It’s mainly used to depict the relationship between variables. I’m going to use several set of assumptions to provide illustrations depicting the relationship between variables. This will allow us to configure some concerns that obstruct inference on causality. Here is the first causal diagram pertaining to our example. library(dagitty) # libraries for DAG library(ggdag) ## ## Attaching package: &#39;ggdag&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## filter # Define a causal diagram dag &lt;- dagitty(&quot; dag { college -&gt; health college -&gt; income college -&gt; ins income -&gt; health ins -&gt; health } &quot;) # Visualize the DAG ggdag(dag) + theme_minimal() + ggtitle(&quot;Causal Diagram Example A.&quot;) + theme_void() Consider the data-generating process (DGP) depicted above. The DGP represents the underlying set of mechanisms or laws of the universe that produce the data we observe. However, these mechanisms are not immediately apparent to us. Essentially, our goal is to uncover and understand the phenomena governing the DGP. Note that variable of interest is college education. As shown in the causal diagram above, the arrow moves away from college education to the rest of the variables. Then the arrows from other variables point to health. Literally reading this: College affects Health College affects insurance status. Insurance affects health. (mechanism through how college affects health). College affects income. Income then affects health. (another mechanism through which college affects health). Since, we are trying to trace the causal link between college education and health, mechanisms through which college education affects health are good. These are the ``good pathways” and we don’t need to be concerned about them. However, I’d argue that the DAG in example A misrepresents the DGP. Let’s consider a slightly better scenario. Here, we allow income to cause health. # Define a causal diagram dag &lt;- dagitty(&quot; dag { college -&gt; health income -&gt; college college -&gt; ins income -&gt; health ins -&gt; health } &quot;) # Visualize the DAG ggdag(dag) + theme_minimal() + ggtitle(&quot;Causal Diagram Example B.&quot;) + theme_void() Note that income causes both health and college education in the above DAG. This restricts us from identifying the cause effect of college education on health. We won’t be able to figure out whether college education leads to better health or income that is correlated with college education drives the effect. To isolate the effect of college education on health, we would want to look at individuals with the same income and utilize the variation in college education. For example, look among individuals with income of 50,000; some will have college education and some won’t. This variation in college education can be fruitful in identification. Hence, we would want to control for income. Once we have done this, we’ve blocked the bad pathway. I’d still argue the DAG presented above is based on unrealistic set of assumptions. Next, we consider the following DAG with feedback loop between college, health, income and insurance status. # Define a causal diagram dag &lt;- dagitty(&quot; dag { college -&gt; health college -&gt; income health -&gt; income income -&gt; college health -&gt; college college -&gt; ins income -&gt; health ins -&gt; health } &quot;) # Visualize the DAG ggdag(dag) + theme_minimal() + ggtitle(&quot;Causal Diagram Example C.&quot;) + theme_void() Note that now arrows are facing both ways for income, health, and college. We have a loop between income, health, and college. This means: Income can affect health; health can affect income. Income can affect college education; college can affect income. College education can affect health; health can affect college education. In other words, income, college, and health are jointly determined. The relationship between college and health is convoluted. To identify the relationship between college and health, we’d want to account for the unwanted channels. This means that we’d want to block out the following channels: i) income to college; ii) health to income; iii) health to college. The first two channels are arguably accounted for by controlling for income. How about the last channel? The DAG is saying that college causes health; and health causes college education. This is the case when causality runs both ways. We call this as reverse causality. Let’s consider another version of DAG. This is where I introduce the unobserved component. Not all of the variables governing the DGP are actually observed by the researcher. In fact, you are often limited by the data that you observe. Hence, you need to regonize the importance of variables that are in play for DAG but aren’t observed. # Define a causal diagram dag &lt;- dagitty(&quot; dag { college -&gt; health college -&gt; income health -&gt; income income -&gt; college health -&gt; college college -&gt; ins income -&gt; health ins -&gt; health unobs -&gt; college unobs -&gt; health unobs -&gt; ins unobs -&gt; income } &quot;) # Visualize the DAG ggdag(dag) + theme_minimal() + ggtitle(&quot;Causal Diagram Example D.&quot;) + theme_void() Among the all of the DAGs presented in this second, the last DAG perhaps most closely represent the DGP. However, there are two limitations here. First, is the limitation arising from data. You just don’t have data for unobserved variables. These variables actually belong to the data generation, but since you don;t have them, you cannot control for them. This leads to omitted variable bias in your inference. Second is the reverse causality problem – as discussed previously, the effect runs borth from health to college and college to health. This is like saying that better health can influence your education, and your education can also influence health. Much of causal inference is about alleviating the concerns of omitted varables and reverse causality. "],["a-simulated-dgp.html", "2.4 A simulated DGP", " 2.4 A simulated DGP Let’s consider the following DGP solely for the purpose of our understanding. College education boosts health by 10 percent. Income boosts health by 20 percent. 40 percent more people from higher income households have college education. Having insurance boosts health by 5 percent. The DAG representing the DGP is as follows: # Define a causal diagram dag &lt;- dagitty(&quot; dag { college -&gt; health income -&gt; college income -&gt; health ins -&gt; health } &quot;) # Visualize the DAG ggdag(dag) + theme_minimal() + ggtitle(&quot;Causal Diagram Representing the Made-up DGP.&quot;) + theme_void() # number of observations n &lt;- 100000 # income follows the log normal distributing income &lt;- rlnorm(n, meanlog = 1, sdlog = 0.5) # multiplying the log normal dist with 20000 income &lt;- income * 20000 # a right skewed distribution hist(income) # a log normal distribution hist(log(income)) # high income high_income &lt;- ifelse(income &gt; median(income), 1, 0) # college education college &lt;- rbinom(n, 1, 0.3 + 0.4 * high_income) # proportion of college graduates by income status print(table(college[high_income == 0])) ## ## 0 1 ## 35110 14890 print(table(college[high_income == 1])) ## ## 0 1 ## 15050 34950 # insurance status ins &lt;- rbinom(n, 1, 0.5) # health (good health 1, poor health 0) # 60 percent of people with no college, low income, and no insurance have good health # 10 percent more of people with college have good health and so on. health &lt;- rbinom(n, 1, 0.6 + 0.1 * college + 0.2 * high_income + 0.05 * ins) table(health) ## health ## 0 1 ## 22463 77537 data &lt;- data.frame(good_health = health, income = income, high_income = high_income, college = college, insurance = ins) head(data) ## good_health income high_income college insurance ## 1 0 45126.12 0 0 1 ## 2 1 55590.17 1 0 0 ## 3 1 55586.67 1 1 0 ## 4 1 55239.95 1 0 0 ## 5 1 58433.67 1 0 1 ## 6 0 36807.68 0 0 0 # building models reg1 &lt;- lm(good_health ~ college, data = data) reg2 &lt;- lm(good_health ~ college + income, data = data) reg3 &lt;- lm(good_health ~ college + high_income, data = data) reg4 &lt;- lm(good_health ~ college + income + high_income, data = data) reg5 &lt;- lm(good_health ~ college + high_income + ins, data = data) summary(reg1) ## ## Call: ## lm(formula = good_health ~ college, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8677 0.1323 0.1323 0.3164 0.3164 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.683612 0.001818 376.12 &lt;2e-16 *** ## college 0.184104 0.002575 71.51 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4071 on 99998 degrees of freedom ## Multiple R-squared: 0.04865, Adjusted R-squared: 0.04864 ## F-statistic: 5114 on 1 and 99998 DF, p-value: &lt; 2.2e-16 summary(reg2) ## ## Call: ## lm(formula = good_health ~ college + income, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.37420 0.01228 0.15276 0.29394 0.40536 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.824e-01 2.774e-03 209.94 &lt;2e-16 *** ## college 1.472e-01 2.660e-03 55.34 &lt;2e-16 *** ## income 1.940e-06 4.051e-08 47.88 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4025 on 99997 degrees of freedom ## Multiple R-squared: 0.06997, Adjusted R-squared: 0.06995 ## F-statistic: 3762 on 2 and 99997 DF, p-value: &lt; 2.2e-16 summary(reg3) ## ## Call: ## lm(formula = good_health ~ college + high_income, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.92627 0.07373 0.07373 0.26972 0.37519 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.624810 0.001954 319.70 &lt;2e-16 *** ## college 0.105475 0.002741 38.47 &lt;2e-16 *** ## high_income 0.195983 0.002741 71.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.397 on 99997 degrees of freedom ## Multiple R-squared: 0.09491, Adjusted R-squared: 0.09489 ## F-statistic: 5243 on 2 and 99997 DF, p-value: &lt; 2.2e-16 summary(reg4) ## ## Call: ## lm(formula = good_health ~ college + income + high_income, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.92767 0.07245 0.07495 0.27004 0.37594 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.265e-01 2.863e-03 218.843 &lt;2e-16 *** ## college 1.055e-01 2.741e-03 38.476 &lt;2e-16 *** ## income -4.541e-08 5.502e-08 -0.825 0.409 ## high_income 1.981e-01 3.774e-03 52.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.397 on 99996 degrees of freedom ## Multiple R-squared: 0.09491, Adjusted R-squared: 0.09489 ## F-statistic: 3495 on 3 and 99996 DF, p-value: &lt; 2.2e-16 summary(reg5) ## ## Call: ## lm(formula = good_health ~ college + high_income + ins, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.95025 0.04975 0.09803 0.29369 0.39905 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.600954 0.002311 260.08 &lt;2e-16 *** ## college 0.105358 0.002736 38.50 &lt;2e-16 *** ## high_income 0.195654 0.002736 71.50 &lt;2e-16 *** ## ins 0.048281 0.002507 19.26 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3963 on 99996 degrees of freedom ## Multiple R-squared: 0.09825, Adjusted R-squared: 0.09823 ## F-statistic: 3632 on 3 and 99996 DF, p-value: &lt; 2.2e-16 We know that the treatment effect of interest is 10, i.e., college increase the chances of being in good health by 10 percentage points. We’ve ran 5 different models (estimated using OLS). reg1: Misses out on other variables, particularly income. This falsely says that that college education increases the chances of being in better health by 18 pp. We’ve got an omitted variable bias problem here. reg2: Adds income in a lineary way. This reduced the coefficient on college education; but still its off from the actual effect. Perhaps, its because we linearly control for income? reg3: Adds in the status of high income (whether income is higher than the median). This is the variable that matters in the DGP. Once we account for income in this way, the coefficient on college education moves to 0.1055 – very close to true effect. One realization is that controling for the variable is not just enough; it is essential to get the correct functional form down as well. reg4: Uses the specification for reg3, but adds in control for income linearly. Not much changes. reg5: Adds in control for insurance. Since, insurance acts alone in the DGP, it does not affect the estimate on college education. "],["discussion.html", "2.5 Discussion", " 2.5 Discussion In this lecture, we’ve discussed some fundamental blocks you need to consider before starting up with research. Data plays a critical component of research and it is necessary to put in deep thought as to how the DGP might be constructed. This will allow us to recognize “good” vs. “bad” pathways and also it’ll inform you about the variables that you’d want to have in consideration. Not all of the variables that you want will be available to you. But you need to make the best out of what you have. This could mean various things including but not limited to: i) figuring out what variables you’d want but are not available, ii) giving thought to the potential problem of omitted variable bias (and reverse causality), and iii) trying out several different ways to account for controls. Ultimately, we are trying to understand something (new) about the DGP. In reality, the laws governing the DGP may not be that simple. That means we should try harder with persistence and creativity. In the next lecture we’ll talk about regression. "],["why-regression.html", "3 Why Regression?", " 3 Why Regression? We’ve discussed the “good” vs. “bad” pathways in the previous lecture. What we’d want to do is close off the “bad” pathways as much as possible. One way to do this is by using regression. In fact, we ran several regression models towards the end of the last lecture. And we were happy that by controlling for income, we were able to trace the effect of college education on health as defined by the simulated DGP. By accounting for the necessary variables, regression may allow us to estimate causal effects. However, there are strong limitations to this approach, something which will discuss soon. First, let’s discuss some fundamentals of regression. # declare paths and libraries user = Sys.info()[&quot;nodename&quot;] if(user == &quot;vinish-Legion-Pro-7-16IRX9H&quot;){ source(&quot;/home/vinish/Dropbox/Medicaid_South/code/filepath.r&quot;) root_dir &lt;- &quot;/home/vinish/Dropbox/Machine Learning/book&quot; }else{ source(&quot;/Users/user1/Dropbox/Medicaid_South/code/filepath.r&quot;) } library(pacman) p_load(fixest, dplyr, ggplot2, tidyverse, patchwork, arrow) theme_set(theme_minimal()) "],["the-best-fit-line.html", "3.1 The best-fit line", " 3.1 The best-fit line Let me first start with an illustration. Say, we want to understand the relationship between education and income. Let’s first simulate the data and plot the relationship. n &lt;- 1000 educ &lt;- sample(seq(1, 16, 1), n, replace = TRUE) income &lt;- 20000 + 2000 * educ + rnorm(n, mean = 10000, sd = 5000) dat &lt;- data.frame(educ = educ, income = income) # mean income for each value of education dat_sum &lt;- dat %&gt;% group_by(educ) %&gt;% summarize(mean_income = mean(income)) # merge dat &lt;- dat %&gt;% merge(dat_sum, by = &quot;educ&quot;, all.x = T) f0 &lt;- ggplot(dat, aes(x = educ, y = income)) + geom_point() + geom_line(aes(x = educ, y = mean_income), size = 1) + xlab(&quot;years of education&quot;) + ggtitle(&quot;Panel A&quot;) + annotate(&quot;text&quot;, x = 10, y = 35000, label = &quot;Line plotting the conditional mean&quot;, color = &quot;blue&quot;, hjust = 0) f1 &lt;- ggplot(dat, aes(x = educ, y = income)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;blue&quot;) + xlab(&quot;years of education&quot;) + annotate(&quot;text&quot;, x = 10, y = 35000, label = &quot;Best-Fit Line; E(Y|X)&quot;, color = &quot;blue&quot;, hjust = 0) + ggtitle(&quot;Panel B&quot;) f0 / f1 ## `geom_smooth()` using formula = &#39;y ~ x&#39; Each point on the figure pertains to an individual. Panel A uses the raw points, while Panel B adds in the best-fit line. This is also known as the regression line. "],["linear-regression-specification.html", "3.2 Linear Regression Specification", " 3.2 Linear Regression Specification Let’s write down the relationship between years of education and earnings using a linear simple (univariate) regression model. \\[\\begin{equation} Y_{i} = \\alpha + \\beta X_{i} + \\epsilon_{i} \\tag{3.1} \\end{equation}\\] where, \\(Y_{i}\\) is income for an individual \\(i\\), \\(X_i\\) is years of education, \\(\\alpha\\) is the y-intercept, \\(\\beta\\) is the slope, \\(\\epsilon_i\\) is the error term. We’ll observe \\(Y\\) and \\(X\\). \\(\\epsilon_{i}\\), the error term, is an unobserved random variable. The error term can be written as: \\(\\epsilon_{i} = Y_i - \\alpha - \\beta X_{i}\\). Note that the regression specification is a population concept. For example, if you could have everyone in the population in your data set, then \\(\\beta\\) is the population coefficient. The error is again a population concept. However, (almost always) you don’t observe the population; you have to work with the sample. Using a sample, you need to estimate \\(\\alpha\\) and \\(\\beta\\). For the regression to make sense, we’ve got to make some assumptions regarding the error term. Note that you don’t observe the error term. Anything that explains \\(Y\\) but is not specified in the regression is captured by the error term. For example, in the earning specification, we are missing out on experience, as earning increases with experience. Once not specified, this variable is observed by the error term. Say, if you build an eagle-eye model, by including all of the variables that should be in the model, and that you also have the functional form correctly specified. In this case, the error term just drops out and the model becomes deterministic, similar to \\(y = mx + c\\). You can move from the simple regression to multiple regression by linearly adding variabes. \\[\\begin{equation} Y_{i} = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_{i} \\tag{3.1} \\end{equation}\\] where, \\(X_2\\) can be experience. To aim for simplicity, we’ll mainly focus on simple linear regression. "],["law-of-iterated-expectation.html", "3.3 Law of iterated expectation", " 3.3 Law of iterated expectation Say you have two random variables \\(X\\) and \\(Y\\). The law of iterated expectation says that: \\(E(E(Y|X)) = E(Y)\\). The inside expectation is conditional on X values, whereas the outside expectation exhausts the \\(X\\) domain. "],["error-term.html", "3.4 Error term", " 3.4 Error term Say, that the model that you have specified is correct; i.e., \\(Y\\) is explained only by \\(X\\). It is easy to see that the error term will consist of the variation in \\(Y\\) that is unexplained by \\(X\\). In this case, the error term will just consist of noise. One widely used assumption is that the errors are: \\(i)\\) identically and independently distributed; and \\(ii)\\) comes from the normal distribution with mean 0 and variance \\(\\sigma^2\\), i.e., \\(\\epsilon \\sim (0, \\; \\sigma^2)\\). This is a strong assumption for two reasons. First, the error terms might be correlated. For example, individuals who live close each other in terms of spatial proximity might share something in common. Second, variance in the error term might vary with values on \\(Xs\\). To see this, earnings may have larger variance for individuals with higher levels of education, compared to those with lower levels of education. When the variance of the error term is not constant across all levels of the explanatory variables, this condition is known as heteroskedasticity. It violates one of the key assumptions of the Ordinary Least Squares (OLS) regression model, which assumes homoskedasticity, i.e., the error term has a constant variance. Instead of assuming that \\(E(\\epsilon) = 0\\), we’ll utilize the exogeneity assumption to make sense of regression. This assumption implicitly states that we’ve closed all of the backdoors to “bad pathways” in the DGP. Let’s say that we’ve got the following DGP. library(dagitty) # libraries for DAG library(ggdag) # Define a causal diagram dag &lt;- dagitty(&quot; dag { health -&gt; educa educa -&gt; earn health -&gt; earn race -&gt; educa race -&gt; earn height -&gt; earn } &quot;) # Visualize the DAG ggdag(dag) + theme_minimal() + ggtitle(&quot;Causal Diagram Example A.&quot;) + theme_void() In this DGP, channels from health and race are bad pathways and need to be accounted for in order to evaluate the causal effect of education on earnings. Once we have accounted for all of the bad and of course good pathways, we can see that height only affects earnings but no other variables. Since, height is not correlated to the explanatory variables \\((Xs)\\), we term height as an exogeneous variable. The exogeneity assumption states that the error term \\((\\epsilon)\\) is uncorrelated with the explanatory variables \\(Xs\\) included in the model. This means that after accounting for \\(Xs\\), the error term is independent of \\(Xs\\). For instance, in the given DGP process, once accounting for education, health, and race, the unaccounted variation in earnings is not correlated with these variables. This conditional independence of error term implies that \\(E(\\epsilon |Xs) = 0\\). Next, we can establish this following using the Law of Iterated Expectation: \\[\\begin{align} E(\\epsilon X) = E[E[\\epsilon X|X]] = E[X\\underbrace{E[\\epsilon |X]}_{=0}] \\\\ = 0 \\end{align}\\] So we’ve got two population-related assumptions: \\(E(\\epsilon) = 0\\) \\(E(\\epsilon X) = 0\\) Note that there are two unknowns (\\(\\alpha\\) and \\(\\beta\\)). Let’s first set-up the sample counterparts: \\(\\frac{1}{n} \\sum_{i}^{n}(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\) \\(\\frac{1}{n} \\sum_{i}^{n} X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0\\) Solve for \\(\\hat{\\alpha}\\) \\[\\begin{align} \\frac{1}{n} \\sum_{i}^{n}(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\\\ \\hat{\\alpha} = \\hat{Y} - \\hat{\\beta} \\hat{X} \\end{align}\\] replace, the value of \\(\\hat{\\alpha}\\) into the second equation to get: \\[\\begin{align} \\frac{1}{n} \\sum_{i}^{n} X_i(Y_i - \\hat{\\alpha} - \\hat{\\beta} X_i) = 0 \\\\ \\frac{1}{n} \\sum_{i}^{n} X_i(Y_i - \\hat{Y} + \\hat{\\beta} \\hat{X} - \\hat{\\beta} X_i) = 0 \\\\ \\frac{1}{n} \\sum_{i}^{n} X_i(Y_i - \\hat{Y}) = \\hat{\\beta} \\frac{1}{n} \\sum_{i}^{n}X_i (X_i - \\hat{X}) \\\\ \\frac{1}{n} \\sum_{i}^{n} (Y_i - \\hat{Y}) = \\hat{\\beta} \\frac{1}{n} \\sum_{i}^{n} (X_i - \\hat{X}) \\\\ \\frac{1}{n} \\sum_{i}^{n} (Y_i - \\hat{Y})(X_i - \\hat{X}) = \\hat{\\beta} \\frac{1}{n} \\sum_{i}^{n} (X_i - \\hat{X})^2 \\\\ \\hat{\\beta} = \\frac{cov(X,Y)}{var(X)} \\end{align}\\] "],["decomposition.html", "3.5 Decomposition", " 3.5 Decomposition The linear regression is estimating the conditional mean. The conditional mean function is written as: \\[\\begin{equation} E(Y_i|X_i) = \\alpha + \\beta X_i \\tag{3.2} \\end{equation}\\] The above equality directly comes from the underlying assumption \\(E(\\epsilon |X)=0\\), or the independence of error term once conditioned on \\(X\\). We can rewrite the regression specification as: \\[\\begin{equation} Y_i = \\underbrace{E(Y_i | X_i)}_{conditional \\; expectation} + \\underbrace{\\epsilon_i}_{unobserved \\; component} \\tag{3.3} \\end{equation}\\] Here, we’ve decomposed the regression specification into two parts: \\(i)\\) conditional mean; and \\(ii)\\) unobserved component. "],["estimation.html", "3.6 Estimation", " 3.6 Estimation There are different ways to estimate a regression specification. These include the method of moment estimation (as we’ve seen before), the maximum likelihood estimation (MLE), and computation estimation. Here, we take a look at the computational approach. Let’s take a look at the error term, \\(\\epsilon_i\\), a bit more carefully. We can flip the terms around and end up with. \\[\\begin{equation} \\epsilon_{i} = Y_{i} - (\\alpha + \\beta X_{i}) \\tag{3.3} \\end{equation}\\] Note that \\(Y_{i} - (\\alpha + \\beta X_{i})\\) are also termed as residuals. Intuitively, we’d want to minimize the prediction error, right? Hence, we’d want to go for the estimates for \\(\\alpha\\) and \\(\\beta\\) such that these estimates minimize the residual sum of the squares (RSS). This gives us an objective function. \\[\\begin{equation} \\underbrace{min}_{\\alpha, \\; \\beta} \\sum_{i = 1}^N \\bigg(Y_{i} - (\\alpha + \\beta X_{i})\\bigg)^2 \\tag{3.4} \\end{equation}\\] The estimates of \\(\\alpha\\) and \\(\\beta\\) are termed as \\(\\widehat{\\alpha}\\) and \\(\\widehat{\\beta}\\), respectively. Next, we’ll write an algorithm to follow the objective function. Create a grid form for \\(\\alpha\\) and \\(\\beta\\) values. alpha_vals &lt;- seq(18000, 35000, by = 10) beta_vals &lt;- seq(500, 5000, by = 10) # create a grid grid &lt;- expand.grid(alpha_vals = alpha_vals, beta_vals = beta_vals) store &lt;- rep(0, nrow(grid)) Declare the objective function. The function takes in the values of \\(X\\), \\(Y\\), \\(\\tilde{\\alpha}\\), \\(\\tilde{\\beta}\\) and gives the ssr. fun_objective &lt;- function(alpha, beta, X, Y) { # @Arg alpha: intercept # @Arg beta: slope # @Arg X: independent variable # @Arg Y: dependent variable residual_sq &lt;- (Y - alpha - beta * X)^2 ssr &lt;- sum(residual_sq) return(ssr) } Pick the estimates on \\(\\alpha\\) and \\(\\beta\\) that minimizes the ssr. for(i in seq(nrow(grid))){ store[i] &lt;- fun_objective(alpha = grid[i,1], beta = grid[i,2], X = educ, Y = income) } index &lt;- which(store == min(store)) coef &lt;- grid[index, ] Compare the estimate that minimizes the ssr with estimates produced from in-built regression library in R. print(grid[index, ]) ## alpha_vals beta_vals ## 264804 29480 2050 print(lm(income ~ educ, dat)) ## ## Call: ## lm(formula = income ~ educ, data = dat) ## ## Coefficients: ## (Intercept) educ ## 29442 2054 Let’s plot to see how ssr varies with estimates of \\(\\beta\\). Fix the value of \\(\\alpha\\) at the estimate from 4. # create a dataframe data &lt;- cbind(grid, store) # restrict the value of alpha at the one that minimizes the ssr data &lt;- data %&gt;% filter(alpha_vals == coef[[1]]) # get the estimate on beta for the set alpha so that it minimizes ssr beta_hat &lt;- beta_vals[which(data$store == min(data$store))] #plot ggplot(data, aes(x = beta_vals, y = store)) + geom_point() + geom_vline(xintercept = beta_hat) + ylab(&quot;residual sum of \\n square&quot;) + xlab(&quot;beta values&quot;) "],["running-a-regression.html", "3.7 Running a regression", " 3.7 Running a regression Regression is a flexible tool to model the relationship between the dependent and independent variables. Consider the data coming from the following simulated DGP. set.seed(1254) n &lt;- 2000 female &lt;- rbinom(n, 1, 0.5) educ &lt;- sample(seq(0, 16, 1), n, replace = TRUE) income &lt;- 20000 + (4000 * educ) -(2500 * female) + (500 * female * educ) + rnorm(n, mean = 0, sd = 2500) dat &lt;- data.frame(educ = educ, income = income, female = female) # head(dat) ## educ income female ## 1 15 80649.27 0 ## 2 13 68828.91 0 ## 3 8 56338.62 1 ## 4 16 84110.95 0 ## 5 5 38053.19 0 ## 6 0 17776.13 1 Say, you want to investigate whether the relationship between education and earnings varies by gender. More specifically, you want to evaluate whether increase in years of education has differential returns on earnings for females compared to males. How do you do this? You’d want to set up your null and alternative hypothesis and test your alternative hypothesis under the null. Null hypothesis. Returns to education on earnings does not vary by gender. Alternative hypothesis. Returns to education are different for female compared to male. Let’s first start with a simple regression specification. \\[\\begin{equation} earnings_i = \\alpha + \\beta education_i + \\epsilon_i \\end{equation}\\] This specification needs to be modified in order to account for gender. \\[\\begin{equation} earnings_i = \\alpha + \\beta_1 education_i + \\beta_2 gender + \\epsilon_i \\end{equation}\\] here, \\(gender\\) is a binary variable, taking the value \\(0\\) if male and \\(1\\) if female. The coefficient on \\(\\beta_1\\) evaluates the effect of 1 additional year of education on earnings, and the coefficient on \\(\\beta_2\\) evaluates the change in average earnings among females compared to males. However, this specification still does not model our alternative hypothesis: Returns to education are lower for female compared to male. To test this, we need to incorporate an interaction term between education and gender. \\[\\begin{equation} earnings_i = \\alpha + \\beta_1 education_i + \\beta_2 gender + \\beta_3 gender \\times education + \\epsilon_i \\end{equation}\\] Let’s break down what the coefficients are capturing: \\(\\alpha\\): captures the average earnings for males with 0 education value. \\(\\beta_1\\): captures the effect of one additional year of education on earnings for male. \\(\\beta_2\\): captures the effect of being a female on average earnings compared to male. \\(\\beta_3\\): tells us whether the impact of an additional year of education for female is different than for female. To see this: the expected returns to education for: male = \\(\\underbrace{\\alpha}_{incercept\\; male} + \\underbrace{\\beta_1}_{slope\\; male} E(education_i)\\). female: \\(\\underbrace{\\alpha + \\beta_2}_{intercept\\; female} + \\underbrace{(\\beta_1 + \\beta_3)}_{slope\\; female} \\times E(education_i)\\). The model allows for both intercept shift as well as changes in slope across gender. Let’s run the specification that tests our alternative hypothesis. reg &lt;- lm(income ~ educ + female + female*educ, dat) summary(reg) ## ## Call: ## lm(formula = income ~ educ + female + female * educ, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8806.8 -1793.6 0.5 1760.2 9360.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20066.81 158.72 126.43 &lt;2e-16 *** ## educ 3984.76 16.77 237.61 &lt;2e-16 *** ## female -2647.20 219.10 -12.08 &lt;2e-16 *** ## educ:female 512.65 23.35 21.95 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2561 on 1996 degrees of freedom ## Multiple R-squared: 0.9852, Adjusted R-squared: 0.9852 ## F-statistic: 4.436e+04 on 3 and 1996 DF, p-value: &lt; 2.2e-16 We see that a year of education leads to an increase in earnings by 3,984. On average, a female’s earning is less than that of male’s by 2,647. Also, on average, the effect of an additional year of education among females is 512 more than that of male’s. Note that these estimates are not too different from the true parameters used to generate the simulated DGP. "],["standard-errors.html", "3.8 Standard Errors", " 3.8 Standard Errors Standard Error of the Regression. Is the just the square root of the variance of the error term. Using a simple regression the standard error of the regression can be attained by the following. Write down the variance of the error term. \\(\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i\\) Note that the variance is not feasible as we don’t observe \\(\\epsilon_i\\). We’d want to get the feasible variance estimator \\(\\widehat{\\sigma}^2\\) by replacing the \\(\\epsilon_i\\) by the residual, i.e. \\(\\widehat{\\epsilon_i}\\). This gives us: \\(\\widehat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\widehat{\\epsilon_i}\\) Note that the variance estimate from 2 is biased downwards. This is because we have estimated \\(p\\) number of parameters before getting the residuals. The remaining degree of freedom is \\(n-p\\). So instead dividing the sum of the square of residuals by \\(n\\), we divide it by \\(n-p\\). \\(\\widehat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} \\widehat{\\epsilon_i}\\) Here, \\(\\widehat{\\sigma}^2\\) is the estimated variance of the error term. The square root of it is the standard error of the regression. It tells us on average how far away is the actual observation (\\(Y\\)-value) from the regression line (best-fit line or \\(E(Y|X)\\)). Lower the standard error of the regression, better the fit. Let’s estimate the standard error of the regression. set.seed(125) n &lt;- 1000 educ &lt;- sample(seq(1, 16, 1), n, replace = TRUE) income &lt;- 20000 + 2000 * educ + rnorm(n, mean = 10000, sd = 5000) dat &lt;- data.frame(educ = educ, income = income) # estimate the model reg_model &lt;- lm(income~educ, data = dat) # get the residuals resid &lt;- residuals(reg_model) # sum of the square of residuals ssr &lt;- sum(resid^2) # divide ssr by n - p var_error &lt;- ssr / (n - length((coef(reg_model)))) # get se se = sqrt(var_error) print(se) ## [1] 4992.835 # compare to the regression output summary(reg_model)$sigma ## [1] 4992.835 Standard error of the coefficient Once we’ve estimated the standard error of the regression, we can go ahead estimate the standard error of the coefficient. \\(\\widehat{var}(\\hat{\\beta}) = \\frac{\\widehat{\\sigma}^2}{var(X) \\times (n - 1)}\\) Let’s estimate the standard error of income coefficient. # estimate the variance of education coefficient var_educ &lt;- se^2 / (sd(educ)^2 * (n - 1)) # get the standard error se_educ &lt;- sqrt(var_educ) # print print(se_educ) ## [1] 34.63766 # compare the hand-estimated se with the model estimate se(reg_model)[[2]] ## [1] 34.63766 "],["an-exercise.html", "3.9 An exercise", " 3.9 An exercise setwd(file.path(root_dir, &quot;data&quot;)) # arrow to open large data sets library(arrow) # birthweight data for 2000 bw_data2000 &lt;- read_feather( &quot;NCHS_birthweight2000.feather&quot;) head(bw_data2000) ## # A tibble: 6 × 47 ## datayear pldel birattnd statenat cntynat stoccfip cntocfip stateres cntyres cityres dmage mrace dmeduc dmar mplbir nlbnd totord9 monprec nprevis dfage frace birmon dgestat gestat10 csex ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2000 1 1 33 33028 36 36059 33 33028 999 33 2 12 2 59 0 4 2 9 99 99 8 37 6 1 ## 2 2000 1 1 47 47040 51 51059 47 47103 999 21 1 12 2 47 0 3 2 11 26 1 4 43 9 1 ## 3 2000 1 1 36 36043 39 39085 36 36999 999 20 1 12 1 36 0 2 2 15 19 1 4 36 5 1 ## 4 2000 1 2 23 23033 26 26065 23 23033 999 24 1 13 1 23 0 4 2 11 26 1 11 37 6 1 ## 5 2000 1 1 05 05001 06 06001 05 05001 103 32 1 17 1 5 0 3 3 10 34 1 4 36 5 2 ## 6 2000 1 1 10 10050 12 12099 10 10050 999 30 2 14 2 10 0 2 2 6 34 2 11 36 5 2 ## # ℹ 22 more variables: dbirwt &lt;dbl&gt;, fmaps &lt;dbl&gt;, anemia &lt;dbl&gt;, cardiac &lt;dbl&gt;, lung &lt;dbl&gt;, diabetes &lt;dbl&gt;, herpes &lt;dbl&gt;, hydra &lt;dbl&gt;, hemo &lt;dbl&gt;, chyper &lt;dbl&gt;, phyper &lt;dbl&gt;, eclamp &lt;dbl&gt;, ## # renal &lt;dbl&gt;, cigar &lt;dbl&gt;, cigar6 &lt;dbl&gt;, alcohol &lt;dbl&gt;, drink &lt;dbl&gt;, dfeduc &lt;dbl&gt;, tobuse &lt;dbl&gt;, alcuse &lt;dbl&gt;, congen &lt;dbl&gt;, smsares &lt;chr&gt; Keep the following variables: a) datayear (year of birth), b) state (state of residence), c) dmage (mother’s age), d) mrace (mother’s race), e) birth weight, f) child gender, and g) marital status. You should find the corresponding variables using the natality documentation here: . Build a birthweight-education regression model in an attempt to evaluate the relationship between mother’s education and child’s birthweight. Start from a simple regression that specifies the relationship between mother’s education and child birthweight. Draw a DAG to illustrate how the DGP might look like. Add in necessary covariates following the illustration in 3. Explain the addition of the covariates, i.e., why is it important to incorporate these covariates in your model specification. Test the hypothesis that returns to education on child’s birthweight can vary by black vs. white race. Include the state of residence in your model specification. Show the results from this specification. What happens to the coefficient on mother’s education? Is it important to include the state of mother’s residence in the model specification? Why? Have you estimated the causal effect of mother’s education on children’s birthweight? Explain. "],["regression-and-gradient-descent.html", "4 Regression and Gradient Descent", " 4 Regression and Gradient Descent Course: Causal Inference Topic: Estimation of linear regression model with and without closed-form solutions .jp-Notebook, .jp-NotebookPanel-notebook { max-width: 900px; margin: auto; } .jp-Cell { padding-left: 40px; padding-right: 40px; } @media print { body { margin: 1in; } } Last time we took a look at the method of minimizing the sum of the square of residuals. Today let’s take a look at two other ways of estimating a linear regression specification: i) Normal equation method, and ii) Gradient descent. We’ll do these manually and compare our results using python libraries to see whether we’ve done it correctly. Lets first load the necessary libraries. import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.preprocessing import add_dummy_feature from sklearn import linear_model import statsmodels.api as sm root_dir = &quot;/home/vinish/Dropbox/Machine Learning&quot; sklearn is an open sourced library in python that is mainly built for predictive data analysis, which is built on top of NumPy, SciPy, and matplotlib. You can get more information about this package on sklearn. We’ll be using simulated data from sklearn’s module called “datasets” by using the make_regression() function. The true model has the following attributes: i) 2 informative features (X) ii) 1 target (Y) iii) intercept with the coefficient of 10 Let’s use make_regression() to simulate our data. X, y, coefficient = make_regression(n_samples = 1000, n_features = 2, n_informative = 2, n_targets = 1, noise = 1, bias = 10, random_state = 42, coef = True ) Note that I’ve set coef = True. This will return the model parameters and the intercept is set at 10. We take a look at the first five rows. print(f&quot;print features: {X[0:5, :]}&quot;) print(f&quot;target: {y[0:5]}&quot;) print features: [[-0.16711808 0.14671369] [-0.02090159 0.11732738] [ 0.15041891 0.364961 ] [ 0.55560447 0.08958068] [ 0.05820872 -1.1429703 ]] target: [ 3.24877735 8.66339401 19.45702327 31.55545159 3.92293402] And let’s print out the model parameters. Note these are the true coefficients that are used to generate data. print(f&quot;coefficients: {coefficient}&quot;) coefficients: [40.71064891 6.60098441] Plot the relationship between \\(X1\\) and \\(Y\\). plt.figure(figsize=(8, 5)) plt.scatter(X[:,1], y) plt.show() #plt.savefig(root_dir + &quot;/Codes/Output/make_data_scatter.pdf&quot;) png Now, we are ready to discuss two methods. Let’s start with the Normal equation method. i. Normal Equation Consider the following regression model specification: \\[ Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_i \\] The job is to estimate model parameters \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\). Note that \\(\\epsilon_i\\) is the error term and we’d want to minimize some version of this. Let’s write out the error as: \\[ \\epsilon_i = Y_i - \\alpha - \\beta_1 X_{1i} -\\beta_2 X_{2i} \\] We know that the error term \\(\\epsilon\\) is a \\(n \\times 1\\) vector. We can obtain residuals by using estimates of model parameters. Of course, we don’t want to pick any parameters – the estimates should follow some objective. One idea is to estimate the model parameters with an objective of minimizing the mean of the error. However, this is 0 by construction. So what we’d want to do instead is minimize the mean squared error. \\[ MSE(X, h_{\\theta}) = \\frac{1}{m} \\sum_{i=1}^{m} (\\theta^{T} x_i - y_i)^2 \\] From the equation above, we know that the MSE is just the mean of the sum of the squared errors. We can write the sum of the squared of errors using the matrix version as: \\[ SSE(X, \\theta) = (y - X\\theta)^T(y-X\\theta) \\] Expanding this and setting the derivatives w.r.t. \\(\\theta\\) equal to zero gives: \\[ \\begin{aligned} SSE(X, \\theta) = y^Ty - 2\\theta^{T} X^Ty + \\theta^{T}X^{T}X\\theta \\\\ \\frac{\\partial(SSE)}{\\partial{\\theta}} = -2X^{T}y + 2X^{T}X\\theta = 0 \\end{aligned} \\] Now solving for \\(\\theta\\) gives the normal equation: \\[ \\hat{\\theta} = (X^TX)^{-1}X^{T}y \\] Let’s code the normal equation and print out the estimates. Before jumping into estimating the normal equation, we’ve got to be careful and add the intercept term (all ones) on X as the simulated data from make_regression comes without it. We’ll do this using add_dummy_feature() in sklearn. # The normal equation X = add_dummy_feature(X) theta_best = np.linalg.inv((X.T @ X)) @ (X.T @ y) print(f&quot;coefficients from normal equation: {theta_best}&quot;) coefficients from normal equation: [10.00156877 40.74650082 6.62076534] ii. Gradient Descent Imagine that you are standing at the top of a mountain and want to descend the mountain as quickly as possible. One simple way is to consider a few directions – north, south, east, and west – and evaluate the steepness (gradient). Then you’d want to take a small step towards the steepest direction, pause, and re-evaluate the steepness. Doing this repeatedly gets you to the bottom of the mountain as fast as possible. The idea of gradient descent is similar in context to the aforementioned analogy. We’ve already been exposed to the idea of MSE and the objective of minimizing MSE. Instead of using the closed form normal equation to solve for the minimum of MSE, gradient descent uses gradient of MSE to adjust the estimates and move closer to the minimum. The gradient is a vector of partial derivatives of MSE that points to the direction of steepest increase increase in MSE. Hence, to minimize the loss, we’d want to move in opposite direction of the gradient. By repeatedly updating our parameter in this way, we move closer and closer to the minimum of the loss function. To simply the concept, we’ll start with the univariate case without the intercept. \\[ \\begin{aligned} Y_i = \\beta X_i + \\epsilon_i \\end{aligned} \\] The MSE and the derivative is: \\[ \\begin{aligned} MSE(\\beta) = \\frac{1}{m} (Y - \\beta X)^{T}(Y - \\beta X) \\\\ \\frac{\\partial{MSE}}{\\partial{\\beta}} = -\\frac{2}{m} X^{T}(Y - \\beta X) \\end{aligned} \\] Here, \\(\\frac{2}{m}X^{T}(Y - \\beta X)\\) is the gradient of the univariate specification, which informs the direction of the steepest increase in MSE. To reduce MSE, we therefore move in the opposite direction of the gradient. Now that we have the gradient, the gradient descent algorithm can be set up as follows: 1. Start with an initial guesses of the parameter \\((\\beta_o)\\). 2. Update estimates of parameters by moving to the opposite direction of the gradient. \\[ \\beta_{new} = \\beta_o - \\eta \\times gradient_o. \\] where, \\(\\eta\\) is the learning rate. 3. Re-evaluate the gradient using \\(\\beta_{new}\\). 4. Repeat steps 2 and 3 for a given number of times or until convergence is reached. Let’s simulate data for univariate model specification to visually see what this looks like. dat_uni = make_regression( n_samples=100, n_features=1, n_informative=1, n_targets=1, bias=0, noise=0, random_state=42, coef=True ) X_uni, y_uni, coef = dat_uni print(f&quot;The first five rows of X: {X_uni[0:5,:]} \\n \\n&quot;) print(f&quot;The first five y values: {y_uni[0:5]} \\n \\n&quot;) print(f&quot;The coefficient of univariate model is: {coef} \\n \\n&quot;) # set up gradient descent beta = -5 # initialize beta eta = 0.1 # learning rate m = X_uni.shape[0] # number of observations iter_val = 100 # number of iteration steps y_uni = y_uni.reshape((m,1)) # reshape into m*1 vector beta_store = np.ones(m) loss_store = np.ones(m) # loop for i in range(iter_val): gradient = -2/m * X_uni.T @ (y_uni - beta*X_uni) loss = 2/m * (y_uni - beta*X_uni).T @ (y_uni - beta*X_uni) beta = beta - eta*gradient beta_store[i] = beta.item() # use .item to extract scalar loss_store[i] = loss.item() # use .item to extract scalar print(f&quot;gradient descent at work: {beta_store} \\n \\n&quot;) # figure plt.figure(figsize=(8,5)) plt.scatter(beta_store, loss_store) plt.xlabel(&quot;beta value&quot;) plt.ylabel(&quot;loss&quot;) plt.title(&quot;Gradient Descent at Work&quot;) plt.show() The first five rows of X: [[ 0.19686124] [ 0.35711257] [-1.91328024] [-0.03582604] [ 0.76743473]] The first five y values: [ 8.21720459 14.90627167 -79.86242262 -1.49541829 32.03357001] The coefficient of univariate model is: 41.7411003148779 gradient descent at work: [ 2.73384129 9.18803147 14.57430322 19.06935566 22.82065107 25.95125242 28.56386053 30.74418321 32.56374696 34.0822434 35.3494875 36.4070518 37.28963019 38.02617604 38.64085208 39.15382306 39.58191721 39.93917836 40.23732664 40.48614292 40.69378975 40.86707908 41.01169573 41.13238393 41.23310291 41.3171568 41.38730303 41.44584277 41.49469646 41.53546675 41.56949114 41.59788581 41.62158226 41.64135787 41.65786138 41.6716342 41.68312815 41.6927203 41.70072532 41.70740581 41.71298095 41.71763361 41.72151644 41.72475682 41.72746103 41.7297178 41.73160117 41.73317291 41.73448459 41.73557923 41.73649276 41.73725513 41.73789136 41.73842232 41.73886542 41.73923521 41.73954381 41.73980135 41.74001628 41.74019565 41.74034533 41.74047025 41.74057451 41.74066151 41.74073411 41.7407947 41.74084527 41.74088747 41.74092269 41.74095208 41.74097661 41.74099708 41.74101416 41.74102841 41.74104031 41.74105024 41.74105852 41.74106544 41.74107121 41.74107603 41.74108004 41.7410834 41.7410862 41.74108853 41.74109048 41.74109211 41.74109347 41.7410946 41.74109555 41.74109633 41.74109699 41.74109754 41.741098 41.74109838 41.7410987 41.74109897 41.74109919 41.74109938 41.74109953 41.74109966] png Here, we see that the algorithm converges at the estimated \\(\\beta\\) little over 40. Let’s print out the best beta from gradient descent and the true parameter for comparison. print(f&quot;True parameter of the univariate model: {coef} \\n \\n &quot;) print(f&quot;Best estimate from gradient descent: {beta} \\n \\n&quot;) True parameter of the univariate model: 41.7411003148779 Best estimate from gradient descent: [[41.74109966]] See that the estimate obtained from gradient descent is close to the true parameter. Multivariate model The gradient descent works similarly in case of multivariate model specification except that we’ll have a vector of partial derivatives. The multivariate model specified at the very begining is: \\[ \\begin{aligned} Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_i \\\\ MSE(\\theta) = \\frac{1}{m} (Y - X \\theta)^{T}(Y - X \\theta) \\end{aligned} \\] I’ve expressed MSE in matrix form, where \\(\\theta\\) incorportates the vector of parameters: \\(\\theta = [\\alpha, \\; \\beta_1, \\; \\beta_2]^{T}\\). The gradient vector is given as: \\[ \\frac{\\partial MSE}{\\partial \\theta} = \\frac{1}{m} 2 X^{T}(Y - X \\theta) \\] The gradient vector is of dimension \\(3 \\times 1\\) and stacks all partials of MSE with respect to \\(\\alpha\\), \\(\\beta_1\\) and \\(\\beta_2\\). Let’s code the gradient descent algorithm and print out both the true parameters and their estimates. # Use the gradient descent algorithm m = X.shape[0] y = y.reshape((m, 1)) theta = np.random.randn(3, 1) eta = 0.1 for i in range(100): gradient = -2 / m * X.T @ (y - X @ theta) theta = theta - eta * gradient # True coefficients print(f&quot;True coefficients: {np.hstack([10, coefficient])}&quot;) print(f&quot;Coefficients from gradient descent: {theta}&quot;) True coefficients: [10. 40.71064891 6.60098441] Coefficients from gradient descent: [[10.00156879] [40.74650076] [ 6.62076533]] Note that these coefficients are exactly similar to those obtained from the normal equation method. We can also make sure that we’ve done the estimation correctly by comparing the estimates with those obtained from built in module in sklearn used for purposes of estimating linear regression models. # sklearn reg = linear_model.LinearRegression(fit_intercept=True) reg.fit(X, y) best_theta_coef = reg.coef_ best_int_coef = reg.intercept_ best_theta_coef = np.concatenate([best_int_coef, best_theta_coef[:, 1:3].ravel()], axis = 0) print(f&quot;coefficients from sklearn: {best_theta_coef}&quot;) coefficients from sklearn: [10.00156877 40.74650082 6.62076534] Not too bad!! Standard Error # ---------------------------------------------- # Standard errors # ---------------------------------------------- # 1. Get the standard error of the regression error = (X @ theta - y) error_sq = error.T @ error sigma_sq = 1 / (m -3) * error_sq se_reg = np.sqrt(sigma_sq) print(f&quot;standard error of the regression is: {se_reg}&quot;) # 2. get standard errors of the respective coefficients var_cov = np.linalg.inv(X.T @ X) * sigma_sq manual_se = np.sqrt(np.diag(var_cov)) print(f&quot;standard errors of coefficients (manual estimation): {manual_se}&quot;) # se from stats model X_sm = sm.add_constant(X) # add intercept model = sm.OLS(y, X_sm).fit() print(f&quot;coefficients from statmodels: {model.params}&quot;) # coefficients print(f&quot;standard errors from statmodels: {model.bse}&quot;) # standard errors standard error of the regression is: [[0.98562573]] standard errors of coefficients (manual estimation): [0.03123574 0.03242909 0.03072431] coefficients from statmodels: [10.00156877 40.74650082 6.62076534] standard errors from statmodels: [0.03123574 0.03242909 0.03072431] Course: Causal Inference Topic: Standard Errors .jp-Notebook, .jp-NotebookPanel-notebook { max-width: 900px; margin: auto; } .jp-Cell { padding-left: 40px; padding-right: 40px; } @media print { body { margin: 1in; } } Standard Error # ---------------------------------------------- # Standard errors # ---------------------------------------------- Standard Errors As previously mentioned, we start with a specification oriented towards the population and use a sample to estimate the population parameters. The \\(\\hat{\\beta}\\) are the sample estimates that inform us about the population parameters \\(\\beta\\). In this sense, sampling variability – if you were to take say 1,000 samples from the population and re-estimate the parameter – will give you different estimates of \\(\\beta\\). Just as the sample mean is a random variable with its own distribution, so is \\(\\hat{\\beta}\\). The standard error of \\(\\hat{\\beta}\\) plays the exact same role as the standard error of the mean, which we motivate below. Consider the example of mean height. Say the population distribution is normal with mean 175 cm and standard deviation of 7.6 cm. You first take a sample of 1,000 individuals and estimate the mean height; then re-take the next sample, re-estimate the mean, and so on for 1,000 different samples. This will give you a distribution of mean height estimates, which will itself be normal with a variance driven entirely by sampling variability. It is important to note that in practice you only ever have one sample – the simulation below is a thought experiment to build intuition for what would happen under repeated sampling. This is the frequentist conception of uncertainty. By the Central Limit Theorem (CLT), the sampling distribution of the mean is: \\[\\hat{X} \\sim \\mathcal{N}\\left(\\mu,\\ \\frac{\\sigma}{\\sqrt{n}}\\right)\\] where \\(\\sigma\\) is the population standard deviation (fixed but unknown in practice) and \\(\\frac{\\sigma}{\\sqrt{n}}\\) is the standard error – the standard deviation of the sampling distribution of the mean, not of the population itself. Note the distinction: \\(\\sigma\\) describes variability in individual heights; \\(\\frac{\\sigma}{\\sqrt{n}}\\) describes variability in the estimate of the mean across repeated samples. Let’s simulate height coming from a normal distribution with mean 175 cm and standard deviation 7.6 cm. import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import add_dummy_feature from sklearn import linear_model from scipy import stats mean_store = [] iter = 10000 mean_height = 175 std_height = 7.6 n = 1000 for i in range(0, iter): height = np.random.normal(mean_height, std_height, n) # sample height from a normal distribution mean_store.append(height.mean()) # plot the histogram of mean height mean_store = np.array(mean_store).ravel() # overlay theoretical normal dist x_space = np.linspace(mean_store.min(), mean_store.max(), 1000) theo_nd = stats.norm.pdf(x_space, mean_height, std_height/np.sqrt(n)) plt.figure(figsize=(8, 5)) plt.hist(mean_store, bins=30, color=&#39;steelblue&#39;, edgecolor=&#39;black&#39;, alpha=0.3, density=&#39;True&#39;) plt.plot(x_space, theo_nd, color = &#39;red&#39;, linewidth = 2, label = &#39;Theoretical Dist.&#39;) plt.title(f&#39;Distribution of mean height from {iter} different samples&#39;) plt.xlabel(&#39;mean height&#39;) plt.ylabel(&#39;density&#39;) plt.legend() plt.grid(False) print(f&#39;average of mean height is {mean_store.mean().round(4)} and std is {mean_store.std().round(4)}&#39;) # %%[markdown] # It is clear that the standard deviation of the mean height depends on: i) the standard deviation of the population height; and ii) # n -- the number of observations. The standard deviation of the mean height will be lower if the population standard deviation is lower # (meaning that height is relatively more homogeneous). Next, one can lower it by increasing the sample size. # # Just as the sample mean has a measure for deviation due to sampling variability (standard deviation), $\\hat{\\beta}}$ too has a measure that we know as standard errors. Simply put, the standard errors measure how fluctuating the estimates of $\\beta$ can be given different samples. # # Right off the start, it should be mentioned that reported standard errors are mostly incorrect. This could be due to several unknown reasons including the functional form of the specified model. But rather than dwelling on why the reported standard errors are incorrect, I want to discuss some known ways to fix the standard errors. # # First, (recall) we start with the assumption on the error term. We discussed the i.i.d. assumtion of the error term. Again, the i.i.d. assumption states that error term are *independent* and *identically* distributed. The former means that errors are not correlated, whereas the latter means that errors are extracted from the same distribution with the same mean and variance. # # Under the i.i.d. assumption, errors are homoskedastic. Estimation of standard errors take the following steps. # # 1. First estimate the regression standard error as: $\\sigma_{reg}^2 = \\frac{1}{n}\\epsilon^{T} \\epsilon$. # # 2. The standard error of estimates then is: $\\sqrt{Var(\\hat{\\beta})} = (X^{T}X)^{-1} \\times \\sigma_{reg}$, where $X^{T}X$ is the variance-covariance matrix with variance terms in the diagonal. # # However, both of these assumptions (identical and independently distributed) will mostly likely fail in practice. This means that we&#39;d need to adjust the standard errors appropriately. # # Let&#39;s take a classic example between education and earnings. We&#39;ll also consider &#39;ability&#39; as a control variable in the DGP. Of course, this is only a simulation exercise to clarify the context of standard errors. # average of mean height is 175.0002 and std is 0.2389 png # 1. generate ability score with a mean of 50 and sd of 20. m = 10000 ability = np.random.normal(50, 20, m) ability_scaled = (ability - ability.mean()) / ability.std() # ability scaled # plot histogram of ability score plt.figure(figsize=(8,5)) plt.hist(ability, bins=30, color=&#39;steelblue&#39;, edgecolor=&#39;black&#39;, alpha=0.3) plt.title(&#39;Distribution of ability&#39;) plt.xlabel(&#39;ability score&#39;) plt.ylabel(&#39;frequency&#39;) # 2. education is positively dependent on ability educa = 7 + 0.1 * ability + np.random.normal(0, 4, m) educa[educa&lt;0] = 0 educa_scaled = (educa - educa.mean()) / educa.std() # education scaled #plt.hist(educa) # 3. generate income # Income comes from distribution with varying standard deviation # this marks heteroskedasticity error_term = np.random.normal(0, 500 + (200*educa)) # generate income using the following DGP income = 40000 + 10000 * educa_scaled + 700 * ability_scaled + np.array(error_term).ravel() true_coefficients = np.array([40000, 10000, 700]) # scatter plot between schooling and education plt.figure(figsize=(8,5)) plt.scatter(educa, income) plt.title(&#39;Scatter plot of education and income&#39;) plt.grid(True, linestyle =&#39;dashed&#39;, alpha = 0.3) plt.xlabel(&#39;Schooling&#39;) plt.ylabel(&#39;Earnings&#39;) # scatter plot between schooling and error error_true = income - (40000 + 10000 * educa_scaled + 700 * ability_scaled) plt.figure(figsize=(8, 5)) plt.scatter(educa, error_term, alpha = 0.3, color = &#39;red&#39;) plt.title(&#39;Relationship between schooling and error term \\n using the true parameters&#39;) plt.xlabel(&#39;Schooling&#39;) plt.ylabel(&#39;Error&#39;) plt.grid(True, linestyle=&#39;dashed&#39;) png png png The plot above shows that although there exist a positive correlation between schooling and earnings. However, variation in earnings is higher for greater values of education. Using the true coefficients (we have these since this is a simulated DGP) we extract error and plot the relationship between schooling and errors. Here, we see a funnel shaped scatter plot with the mean of error aligned at 0. This simple example breaks the homoskedasticity assumption. The main point is that the estimation of standard errors need to reflect the fact that error term comes from distribution with different variances. We now have what is known as heteroskedasticity. Let’s start with estimates using the gradient descent. # features X = np.concatenate((educa_scaled.reshape(m,1), ability_scaled.reshape(m,1)), axis = 1) # Note: no need to scale the features as they already are scaled X = add_dummy_feature(X) # intercept term y = income.reshape((m, 1)) p = X.shape[1] # number of features # initialize theta theta = np.random.uniform(0, 1, p).reshape((p, 1)) # learning rate eta = 1e-2 # number of iterations epochs = 2000 for i in range(0, epochs): # get the gradient and adjust theta against the gradient using the learning rate gradient = -2/m * X.T@(y - X@theta ) theta = theta - eta*gradient # Use sklearn module to check reg = linear_model.LinearRegression(fit_intercept=False) reg.fit(X, y) print(f&#39;The estimates from sklearn is: {reg.coef_.round(4)}&#39;) print(f&#39;The estimates from manual GD is: {theta.reshape((1, 3)).round(4)}&#39;) The estimates from sklearn is: [[40015.7547 10015.2551 714.2199]] The estimates from manual GD is: [[40015.7547 10015.2551 714.2199]] We area simply running gradient descent in the code above. This should be familiar to you from previous lectures. Now, we’d want to move on to the standard errors to assess precision of these estimates. First, let’s explore the graphical relationship between education and residuals. Note that I’ve distinguised error vs. residuals – the former is what you get from using true coefficients (you never see this in practice), while the latter use the estimates. # Get residuals (note that we are using the estimated thetas) residual = y - X @ theta plt.figure(figsize=(8,5)) plt.scatter(educa, residual, alpha = 0.3, color=&#39;red&#39;) plt.title(&#39;Relationship between education and residuals&#39;) plt.xlabel(&#39;education&#39;) plt.ylabel(&#39;residuals&#39;) plt.grid(True, linestyle=&#39;dashed&#39;) # %%[markdown] # As you can see we&#39;ve got a funnel shaped relationship between education and residual, again signaling heteroskedasticity. This is precisely coming from differing variance of error term based on education. This won&#39;t affect the estimates but will affect the standard errors. Note that there are many tests for homoskedasticity vs heteroskedasticity. But in practice, the case of homoskedasticity almost always fails. The simple way to see it is to plot the residuals with the variable of concern. If you have a funnel looking shape, then you&#39;ve got the case of heteroskedasticity. # First, let&#39;s do some benchmarking by estimating the standard errors under the homoskedasticity assumption -- error terms have the same variance (which is incorrect in this case and in practice). png # 1. Get the standard error of the regression error = (X @ theta - y) error_sq = error.T @ error sigma_sq = 1 / (X.shape[0] -3) * error_sq se_reg = np.sqrt(sigma_sq) print(f&quot;standard error of the regression is: {se_reg} \\n \\n&quot;) # 2. get standard errors of the respective coefficients var_cov = np.linalg.inv(X.T @ X) * sigma_sq manual_se = np.sqrt(np.diag(var_cov)) # compare it with standard errors from statsmodels import statsmodels.api as sm # se from stats model X_sm = sm.add_constant(X) # add intercept model = sm.OLS(y, X_sm).fit() print(f&quot;The estimates from statmodels: {model.params.round(4)}&quot;) # coefficients print(f&#39;The estimates from sklearn is: {reg.coef_.round(4)}&#39;) print(f&#39;The estimates from manual GD is: {theta.reshape((1, 3)).round(4)} \\n \\n&#39;) print(f&quot;standard errors from statmodel under homoskedasticity: {model.bse.round(4)}&quot;) # standard errors print(f&quot;standard errors (manual estimation) under homoskedasticity: {manual_se.round(4)} \\n \\n&quot;) # %%[markdown] # Now that we have estimated standard errors under the homoskedasticity assumption let&#39;s see how we can fix this. Before we move on, I want to reiterate that the origin of heteroskedasicity is due to error terms coming from the distribution of different variance. In that regard, we want to account for this in our estimation of standard errors. # # To do so, we&#39;ll use a sandwich method, which you should&#39;ve heard from you previous classes. So what does it entail? Basically, we&#39;d want to form weights using the size of the error term. # # Let&#39;s just derive the sandwich estimator: # # \\begin{align} # \\hat{\\beta} = (X^{T}X)^{-1}X^{T}Y \\\\ # \\hat{\\beta} = (X^{T}X)^{-1}X^{T}(X\\beta + \\epsilon) \\\\ # \\hat{\\beta} = \\beta + (X^{T}X)^{-1}X^{T}\\epsilon # \\end{align} # The end line is the starting point. Note that $Var(a)=0$ and $Var(ax) = a^{2}Var(x)$, where $a$ is a constant and $x$ is a random variable. Let&#39;s then take the variance of $\\hat{\\beta}$: # # \\begin{align} # Var(\\hat{\\beta}) = Var((X^{T}X)^{-1}X^{T}\\epsilon) \\\\ # = (X^{T}X)^{-1}Var(X^{T}\\epsilon)(X^{T}X)^{-1} \\\\ # = (X^{T}X)^{-1}X^{T}Var(\\epsilon)X(X^{T}X)^{-1} \\\\ # = (X^{T}X)^{-1}X^{T} \\Omega X(X^{T}X)^{-1} # \\end{align} # # Here, $\\Omega$ is the scaling term and it is $\\epsilon\\epsilon^{T}.I$, which is a $n \\times n$ diagonal matrix. We don&#39;t observe this, so replace this with the sample counterpart $\\hat{\\Omega} = \\hat{\\epsilon}\\hat{\\epsilon}^{T}I$. Notice that under the case of homoskedasticity $\\Omega = \\sigma^2 I$, which collapses $Var{\\hat{\\beta}}$ to $(X^{T}X)^{-1} \\sigma^2$. This is what we used to estimate standard errors under homoskedasticity. # # # Let&#39;s estimate the standard errors accounting for heteroskedasicity. I&#39;m going to divide this into bun and stuffing part as I write the code. standard error of the regression is: [[3001.23166629]] The estimates from statmodels: [40015.7547 10015.2551 714.2199] The estimates from sklearn is: [[40015.7547 10015.2551 714.2199]] The estimates from manual GD is: [[40015.7547 10015.2551 714.2199]] standard errors from statmodel under homoskedasticity: [30.0123 33.6297 33.6297] standard errors (manual estimation) under homoskedasticity: [30.0123 33.6297 33.6297] bun = np.linalg.inv(X.T@X) omega = np.diag((residual**2).ravel()) # n * n diagonal matrix stuff = X.T @ omega @ X # get the variance covariance matrix var_cov = bun @ stuff @ bun robust_se = np.sqrt(np.diag(var_cov)) print(f&#39;heteroskedasticity robust standard error: {robust_se.round(4)}&#39;) print(f&#39;se under homoskedastic assumption: {manual_se.round(4)}&#39;) # robust se using sm model2 = sm.OLS(y, X).fit(cov_type=&#39;HC0&#39;) print(f&#39;standard errors from stats model under heteroskedasticity: {model2.bse.round(4)}&#39;) heteroskedasticity robust standard error: [30.0078 35.5333 33.4547] se under homoskedastic assumption: [30.0123 33.6297 33.6297] standard errors from stats model under heteroskedasticity: [30.0078 35.5333 33.4547] If you compare the standard errors under homoskedasticity versus heteroskedasticity, you will see that standard errors under heteroskedasticity are larger, particularly for the education estimate. Note that there are various forms of heteroskedasicity robust standard error including “HC1”, “HC2”, and “HC3”. Here, we’ve used the “H0” type – you can dig deeper according to your need. Let’s now discuss the case where error terms are not independent and are correlated. One can think of this in a geospatial form – the unexplained portion of income for people living in a particular area can be correlated. For example, if you have people living in Des Moines, Iowa and NYC, you probably will think that income is spatially correlated due to local market conditions, taste, cost of living and other unobserved factors attributing to spatial clustering. In the following simulation exercise, we’ll incorporate this cluter-type correlation in the error terms. Specifically, we’ll build error as: \\[\\begin{equation} \\epsilon_{ic} = u_{c} + v_{i} \\end{equation}\\] Essentially, the error term consists of: i) \\(u_c\\) – the cluster specific shock (all units within a specific cluster gets the same value); and ii) individual specific term. Both \\(u_c\\) and \\(v_i\\) will come from a normal distribution with mean 0 and standard deviation 3,000 and 1,000, respectively. # number of clusters nc = 50 # number of units within cluster ni = 200 # total number of units m = nc * ni # get the cluster shock cluster_shock = np.random.normal(0, 3000, nc) cluster_index = np.repeat(np.linspace(1, nc), ni).astype(&#39;int&#39;) u_shock = cluster_shock[cluster_index-1] # get the individuals shock # in this case cluster shock dominates individual shock v_shock = np.random.normal(0, 1000, m) error = u_shock + v_shock Let’s estimate the model and plot the relationship between education and residuals by some clusters. X = np.concatenate((educa_scaled.reshape((m, 1)), ability_scaled.reshape((m,1))), axis=1) X = add_dummy_feature(X) y_c = 40000 + 10000 * educa_scaled + 700 * ability_scaled + error.ravel() y_c = y_c.reshape((m, 1)) # initialize theta theta = np.random.uniform(0, 1, p).reshape((p, 1)) # learning rate eta = 1e-2 # number of iterations epochs = 20000 for i in range(0, epochs): # get the gradient and adjust theta against the gradient using the learning rate gradient = -2/m * X.T@(y_c - X@theta) theta = theta - eta*gradient print(theta) resid_clus = y_c - X@theta # then re-generate income error_clus = income - (40000 + 10000 * educa_scaled + 700 * ability_scaled) plt.figure(figsize=(8,5)) plt.scatter(educa[cluster_index==1], resid_clus[cluster_index==1], color = &#39;red&#39;, alpha = 0.3, label=&#39;Cluster 1&#39;) plt.scatter(educa[cluster_index==10], resid_clus[cluster_index==10], color = &#39;blue&#39;, alpha = 0.3, label=&#39;Cluster 10&#39;) plt.scatter(educa[cluster_index==20], resid_clus[cluster_index==20], color = &#39;green&#39;, alpha = 0.3, label=&#39;Cluster 20&#39;) plt.legend() plt.title(&#39;Relationship between schooling and residuals \\n by some chosen cluster&#39;) plt.xlabel(&#39;schooling&#39;) plt.ylabel(&#39;residuals&#39;) [[39649.37831353] [10027.34709599] [ 695.56370296]] Text(0, 0.5, &#39;residuals&#39;) png In the figure above, we see that residuals across different clusters are grouped at certain residual values. This depicts the clustering problem. Now, we need to adjust our the standard errors to account for this kind of clustering. The variance now takes the form: \\[\\begin{equation} Var(\\hat \\beta) = (X^{T}X)^{-1} \\bigg(\\sum_{c=1}^{G} X^{T}_c \\epsilon_c \\epsilon^{T}_c X_c \\bigg) (X^{T}X)^{-1} \\end{equation}\\] Let’s estimate this!! bun = np.linalg.inv(X.T@X) stuff = np.zeros(X.shape[1]*X.shape[1]).reshape((X.shape[1], X.shape[1])) for i in range(1, nc+1): X_c = X[cluster_index==i] res_c = resid_clus[cluster_index==i] mid = X_c.T@res_c@res_c.T@X_c stuff += mid var_cov_clus = bun@stuff@bun se_clus = np.sqrt(np.diag(var_cov_clus)) print(f&#39;Cluster robust standard error is: {se_clus}&#39;) # small sample correction term correction = ((nc/(nc-1)) * (m-1)/(m-p)) var_cov_clust_corr = var_cov_clus * correction se_clus_correct = np.sqrt(np.diag(var_cov_clust_corr)) model_clus = sm.OLS(y_c, X).fit(cov_type=&#39;cluster&#39;, cov_kwds={&#39;groups&#39;: cluster_index}) model_noclus = sm.OLS(y_c, X).fit() print(f&#39;Clustered standard errors (not small sample corrected): {se_clus.round(4)}&#39;) print(f&#39;Clustered se corrected for small sample: {se_clus_correct.round(4)}&#39;) print(f&#39;Clustered standard error from statsmodel: {model_clus.bse.round(4)}&#39;) print(f&#39;standard error without clustering from statsmodel: {model_noclus.bse.round(4)}&#39;) Cluster robust standard error is: [421.13627575 28.40377396 32.01738254] Clustered standard errors (not small sample corrected): [421.1363 28.4038 32.0174] Clustered se corrected for small sample: [425.4544 28.695 32.3457] Clustered standard error from statsmodel: [425.4544 28.695 32.3457] standard error without clustering from statsmodel: [31.4168 35.2034 35.2034] We’ve now computed the clustered standard errors and compared it with those from statsmodel. They are very exactly the same. However, clustered standard errors are larger compared to unclustered standard errors. Bootstrapped standard errors Bootstapping can lead to a convenient way of obtaining standard errors. The bootstrapping process assumes the data as the population and resamples \\(k\\) number of times from the population (with replacement) and re-estimates the coefficients on every sample. You’ll then have \\(k\\) number of estimates. The standard error is simply the standard deviation of the estimates. import random rep = 199 # learning rate eta = 1e-2 # number of iterations epochs = 20000 for k in range(0, rep): boot_index = np.random.choice(X.shape[0], size=X.shape[0], replace=True).astype(&#39;int&#39;) boot_index = np.array(boot_index) - 1 X_boot = X[boot_index] y_boot = y_c[boot_index] # initialize theta theta = np.random.uniform(0, 1, p).reshape((p, 1)) for i in range(0, epochs): # get the gradient and adjust theta against the gradient using the learning rate gradient = -2/X_boot.shape[0] * X_boot.T@(y_boot - X_boot@theta) theta = theta - eta*gradient if k == 0: theta_store = theta.ravel() else: theta_new = theta.ravel() theta_store = np.vstack((theta_store, theta_new)) print(f&#39;Rep {k} estimate: {theta}&#39;) boot_se = theta_store.std(axis=0) print(f&#39;bootstrapped non-clustered version of se: {boot_se.round(4)}&#39;) print(f&#39;standard error without clustering from statsmodel: {model_noclus.bse.round(4)}&#39;) Rep 0 estimate: [[39634.20759692] [ 9972.15137597] [ 652.72915823]] Rep 1 estimate: [[39621.9140577 ] [10046.57035788] [ 672.42939652]] Rep 2 estimate: [[39623.08461529] [ 9981.58228619] [ 670.98263527]] Rep 3 estimate: [[39637.55183125] [10001.7489246 ] [ 662.05036779]] Rep 4 estimate: [[39695.44764121] [10041.16930021] [ 647.56837512]] Rep 5 estimate: [[39615.34205097] [10017.89522589] [ 686.87890025]] Rep 6 estimate: [[39678.63656974] [10036.74032889] [ 707.02820496]] Rep 7 estimate: [[39635.53503934] [ 9995.33859027] [ 690.70481666]] Rep 8 estimate: [[39600.00610224] [10028.68218574] [ 702.75449706]] Rep 9 estimate: [[39707.94055818] [10059.68129534] [ 654.26712762]] Rep 10 estimate: [[39612.10414709] [ 9981.12202919] [ 736.70234899]] Rep 11 estimate: [[39681.05918001] [ 9975.17250646] [ 709.55558489]] Rep 12 estimate: [[39647.47856378] [ 9981.73243325] [ 753.13733332]] Rep 13 estimate: [[39667.34485868] [10036.61524871] [ 687.09562445]] Rep 14 estimate: [[39659.16998691] [10000.52337148] [ 710.01139155]] Rep 15 estimate: [[39639.29382108] [10082.30415622] [ 742.44583502]] Rep 16 estimate: [[39625.32899474] [ 9987.89864738] [ 748.9561938 ]] Rep 17 estimate: [[39667.92479251] [10000.07091612] [ 682.58096323]] Rep 18 estimate: [[39647.64304062] [10065.09464912] [ 672.04108689]] Rep 19 estimate: [[39610.70165961] [10018.89623775] [ 729.73636443]] Rep 20 estimate: [[39617.45842608] [10069.92475955] [ 685.70389042]] Rep 21 estimate: [[39588.24995471] [10013.35529898] [ 689.32223213]] Rep 22 estimate: [[39658.37619605] [10072.95445493] [ 696.82674721]] Rep 23 estimate: [[39687.52536611] [10040.72734011] [ 649.30189093]] Rep 24 estimate: [[39666.07008584] [10043.61662354] [ 666.85956965]] Rep 25 estimate: [[39603.38815643] [10047.87591532] [ 632.34129952]] Rep 26 estimate: [[39657.30936341] [10031.24586445] [ 715.40765174]] Rep 27 estimate: [[39615.99259115] [ 9957.31296536] [ 696.13705344]] Rep 28 estimate: [[39643.66488259] [10008.39182557] [ 730.86219762]] Rep 29 estimate: [[39675.34722975] [10081.34286247] [ 676.3526735 ]] Rep 30 estimate: [[39614.46013655] [10001.8207831 ] [ 719.41863241]] Rep 31 estimate: [[39640.25866004] [10021.40479851] [ 704.30228288]] Rep 32 estimate: [[39612.09199402] [10033.38174397] [ 663.34553222]] Rep 33 estimate: [[39683.79571144] [ 9973.46820969] [ 723.40389677]] Rep 34 estimate: [[39686.12970807] [ 9956.55977931] [ 744.53260858]] Rep 35 estimate: [[39632.88902075] [10036.84480922] [ 689.57898674]] Rep 36 estimate: [[39713.00878458] [10015.81757479] [ 715.55775228]] Rep 37 estimate: [[39671.48133869] [10107.90762039] [ 671.13913505]] Rep 38 estimate: [[39643.65792854] [10058.90937064] [ 693.25339089]] Rep 39 estimate: [[39703.82590057] [10076.31928723] [ 654.39904957]] Rep 40 estimate: [[39643.81718312] [ 9964.54652872] [ 698.7648128 ]] Rep 41 estimate: [[39689.97372164] [10000.7727396 ] [ 687.52259812]] Rep 42 estimate: [[39612.87625102] [10004.74660384] [ 709.37496101]] Rep 43 estimate: [[39671.32538866] [10028.43288591] [ 700.32820727]] Rep 44 estimate: [[39630.74865614] [10100.3991206 ] [ 611.18128762]] Rep 45 estimate: [[39622.90791883] [10071.05706214] [ 673.05439047]] Rep 46 estimate: [[39604.76657256] [10057.85387121] [ 628.8394797 ]] Rep 47 estimate: [[39641.07825529] [10049.36557045] [ 730.63707784]] Rep 48 estimate: [[39691.79768444] [10002.91419186] [ 693.30898309]] Rep 49 estimate: [[39662.42792761] [10037.23799975] [ 713.29822611]] Rep 50 estimate: [[39663.69589865] [10074.31186539] [ 645.45476732]] Rep 51 estimate: [[39680.76408045] [ 9987.93311561] [ 760.82821243]] Rep 52 estimate: [[39619.97082001] [10037.60657886] [ 661.40091985]] Rep 53 estimate: [[39624.9279004 ] [10024.43239989] [ 745.91481201]] Rep 54 estimate: [[39586.35708561] [10076.95450602] [ 629.20304524]] Rep 55 estimate: [[39679.83165912] [10071.96718542] [ 644.02298151]] Rep 56 estimate: [[39601.42306921] [10091.46830495] [ 664.50994212]] Rep 57 estimate: [[39738.84348681] [10028.95597784] [ 695.86288615]] Rep 58 estimate: [[39647.87951559] [ 9998.21533346] [ 661.00115565]] Rep 59 estimate: [[39647.8723745 ] [10016.47193961] [ 639.26241395]] Rep 60 estimate: [[39642.84507487] [10034.21172881] [ 653.12708525]] Rep 61 estimate: [[39648.71749172] [10053.26102583] [ 676.84555042]] Rep 62 estimate: [[39654.7967103 ] [10045.72267797] [ 666.79901184]] Rep 63 estimate: [[39612.28274296] [10011.06062689] [ 697.86622241]] Rep 64 estimate: [[39614.60384791] [10021.96923194] [ 722.94352736]] Rep 65 estimate: [[39612.37330471] [10064.7123475 ] [ 669.34877168]] Rep 66 estimate: [[39673.08721685] [10012.1679277 ] [ 745.4580107 ]] Rep 67 estimate: [[39618.32940955] [10013.97419028] [ 668.39873329]] Rep 68 estimate: [[39628.51457672] [10011.12953266] [ 696.81355033]] Rep 69 estimate: [[39658.02181461] [10068.86929084] [ 689.81993823]] Rep 70 estimate: [[39637.89056608] [10052.13019266] [ 715.32750376]] Rep 71 estimate: [[39679.5330422 ] [10100.65291174] [ 677.75745628]] Rep 72 estimate: [[39692.65248798] [10000.46060365] [ 702.46626574]] Rep 73 estimate: [[39658.81539007] [10024.15467442] [ 645.60990573]] Rep 74 estimate: [[39642.66877888] [10023.86808413] [ 723.51478569]] Rep 75 estimate: [[39695.40950835] [10019.72084127] [ 664.74411942]] Rep 76 estimate: [[39637.94858277] [10012.92585604] [ 709.17713849]] Rep 77 estimate: [[39622.50035905] [10046.02128644] [ 675.20862677]] Rep 78 estimate: [[39628.14242031] [10080.18874293] [ 622.71631863]] Rep 79 estimate: [[39682.81735629] [10041.9898932 ] [ 692.45776192]] Rep 80 estimate: [[39675.97498997] [10039.89993909] [ 694.07466724]] Rep 81 estimate: [[39638.87090673] [10077.1659821 ] [ 686.19713262]] Rep 82 estimate: [[39630.96885157] [10022.10255276] [ 726.14058914]] Rep 83 estimate: [[39636.3769543 ] [10030.72330636] [ 705.44437295]] Rep 84 estimate: [[39624.18801433] [10010.07055701] [ 675.1970448 ]] Rep 85 estimate: [[39707.51948677] [10071.49412137] [ 701.92964802]] Rep 86 estimate: [[39615.4515855 ] [10017.99562684] [ 716.18090475]] Rep 87 estimate: [[39608.54965838] [10050.41049065] [ 697.41637554]] Rep 88 estimate: [[39691.98713524] [10098.09824757] [ 713.48827376]] Rep 89 estimate: [[39595.55443741] [10047.83576716] [ 668.21668283]] Rep 90 estimate: [[39731.69916921] [10016.69969976] [ 668.1809089 ]] Rep 91 estimate: [[39652.0587333 ] [10013.15558147] [ 666.57907701]] Rep 92 estimate: [[39625.19746259] [10006.67630543] [ 676.34971174]] Rep 93 estimate: [[39623.1732082 ] [10056.00513643] [ 657.67129513]] Rep 94 estimate: [[39669.56381442] [10011.80050511] [ 755.80095212]] Rep 95 estimate: [[39611.31590999] [10039.15137472] [ 702.07703054]] Rep 96 estimate: [[39612.57842562] [10051.82340555] [ 703.45902349]] Rep 97 estimate: [[39692.78602923] [10042.04713415] [ 677.38841659]] Rep 98 estimate: [[39661.76137101] [10059.24880542] [ 682.59772266]] Rep 99 estimate: [[39606.1170304 ] [10060.23053653] [ 670.5534201 ]] Rep 100 estimate: [[39713.9395932 ] [10042.55208298] [ 716.8405365 ]] Rep 101 estimate: [[39679.54740889] [10023.6023723 ] [ 675.70266313]] Rep 102 estimate: [[39669.29585361] [10075.18985606] [ 670.30338812]] Rep 103 estimate: [[39617.53461811] [10025.62430093] [ 657.2055975 ]] Rep 104 estimate: [[39630.54394129] [10016.80633441] [ 674.27638597]] Rep 105 estimate: [[39675.24635594] [10051.38483056] [ 655.39525373]] Rep 106 estimate: [[39650.47580971] [10015.50862261] [ 744.81596155]] Rep 107 estimate: [[39681.66575401] [10009.71142048] [ 635.38642252]] Rep 108 estimate: [[39646.16404817] [10098.75866793] [ 656.33659447]] Rep 109 estimate: [[39673.45866978] [ 9980.98604561] [ 709.54582486]] Rep 110 estimate: [[39683.01458711] [10060.47250086] [ 659.34579579]] Rep 111 estimate: [[39615.95093122] [10085.35188812] [ 642.85064756]] Rep 112 estimate: [[39685.07117305] [10073.87300929] [ 675.62633986]] Rep 113 estimate: [[39700.80508502] [10025.39673853] [ 723.95671698]] Rep 114 estimate: [[39654.13705739] [10062.99956193] [ 705.49724421]] Rep 115 estimate: [[39642.98033672] [10046.23254871] [ 669.82054311]] Rep 116 estimate: [[39673.93569365] [10044.52343701] [ 730.61526628]] Rep 117 estimate: [[39567.36170446] [10079.10737707] [ 702.04875241]] Rep 118 estimate: [[39640.99781573] [10049.8079462 ] [ 696.77124729]] Rep 119 estimate: [[39652.924091 ] [ 9966.1126205 ] [ 690.08012937]] Rep 120 estimate: [[39640.84772593] [10017.88419631] [ 680.06462807]] Rep 121 estimate: [[39648.81492862] [10085.39461703] [ 631.33819534]] Rep 122 estimate: [[39617.66740925] [10001.81992616] [ 738.36649983]] Rep 123 estimate: [[39663.81801586] [10016.87701287] [ 719.76040622]] Rep 124 estimate: [[39577.83973312] [ 9996.32517714] [ 670.45491691]] Rep 125 estimate: [[39694.78878319] [ 9952.60675322] [ 718.64306284]] Rep 126 estimate: [[39660.56138757] [10004.64619447] [ 656.04524912]] Rep 127 estimate: [[39651.64358692] [10048.61973202] [ 654.07486899]] Rep 128 estimate: [[39689.03339121] [10032.29573575] [ 738.45004651]] Rep 129 estimate: [[39691.52021343] [10052.08173475] [ 659.96646377]] Rep 130 estimate: [[39652.85760954] [ 9986.7354964 ] [ 709.03049442]] Rep 131 estimate: [[39632.99566456] [ 9966.98461718] [ 732.63392343]] Rep 132 estimate: [[39668.93435262] [10036.96414159] [ 649.0258689 ]] Rep 133 estimate: [[39670.47972999] [10089.33536851] [ 644.24850734]] Rep 134 estimate: [[39640.55040963] [10053.44319661] [ 674.20945602]] Rep 135 estimate: [[39623.85153201] [ 9994.22996327] [ 695.09818855]] Rep 136 estimate: [[39645.97319946] [ 9971.11629625] [ 721.15848313]] Rep 137 estimate: [[39658.9619191 ] [10019.48005858] [ 661.48300874]] Rep 138 estimate: [[39642.57914846] [ 9997.19070144] [ 667.73498415]] Rep 139 estimate: [[39637.4290495 ] [10034.32711427] [ 661.76117443]] Rep 140 estimate: [[39628.80713831] [10017.68117611] [ 680.75140228]] Rep 141 estimate: [[39674.93078111] [10013.53733556] [ 721.24820465]] Rep 142 estimate: [[39651.40792942] [10025.72045083] [ 709.04701078]] Rep 143 estimate: [[39639.10392042] [10016.2437193 ] [ 724.07604731]] Rep 144 estimate: [[39646.46862892] [10025.67431022] [ 688.40242255]] Rep 145 estimate: [[39650.14696411] [10005.51665637] [ 712.88464213]] Rep 146 estimate: [[39646.92820979] [ 9954.15976451] [ 776.37739858]] Rep 147 estimate: [[39620.3282916 ] [10042.6185706 ] [ 654.70339839]] Rep 148 estimate: [[39650.55655917] [ 9991.17781519] [ 645.11118853]] Rep 149 estimate: [[39667.55051198] [ 9990.73834714] [ 738.68975989]] Rep 150 estimate: [[39671.71292371] [ 9977.57126474] [ 728.57423806]] Rep 151 estimate: [[39637.46442206] [10060.23241637] [ 663.34468336]] Rep 152 estimate: [[39681.52863218] [10054.86696935] [ 655.32975744]] Rep 153 estimate: [[39647.28083936] [10116.25440095] [ 639.00191274]] Rep 154 estimate: [[39620.82929953] [10047.83682386] [ 717.52890504]] Rep 155 estimate: [[39647.51544097] [10036.1348693 ] [ 759.02598785]] Rep 156 estimate: [[39658.71758628] [10057.79740778] [ 673.53520113]] Rep 157 estimate: [[39652.42180026] [10039.6446241 ] [ 652.37900513]] Rep 158 estimate: [[39622.38554988] [ 9991.14210226] [ 766.88430685]] Rep 159 estimate: [[39651.29699042] [10065.91893929] [ 689.47118359]] Rep 160 estimate: [[39675.69878036] [10068.01885768] [ 687.51871023]] Rep 161 estimate: [[39631.29501096] [ 9912.34553365] [ 767.78971857]] Rep 162 estimate: [[39653.05962809] [10024.96701508] [ 717.29791508]] Rep 163 estimate: [[39666.19510328] [10005.02761342] [ 706.49793799]] Rep 164 estimate: [[39656.32635251] [10024.47016931] [ 695.98769732]] Rep 165 estimate: [[39702.17892756] [10052.94497995] [ 706.22741806]] Rep 166 estimate: [[39606.48521272] [10010.67628793] [ 714.82999807]] Rep 167 estimate: [[39652.91873686] [10028.93993568] [ 672.76873611]] Rep 168 estimate: [[39641.79602234] [10000.60435574] [ 685.80996805]] Rep 169 estimate: [[39696.57476929] [ 9945.83651273] [ 720.77406998]] Rep 170 estimate: [[39607.16983671] [10027.29761694] [ 708.6226797 ]] Rep 171 estimate: [[39589.956391 ] [ 9976.67312017] [ 709.95922716]] Rep 172 estimate: [[39680.0983667 ] [10011.88421355] [ 642.28909189]] Rep 173 estimate: [[39623.31804032] [10082.066283 ] [ 656.84713371]] Rep 174 estimate: [[39726.15662337] [ 9963.56427099] [ 685.90386222]] Rep 175 estimate: [[39678.70850114] [10023.98807359] [ 661.27963856]] Rep 176 estimate: [[39612.72240517] [10013.51792832] [ 701.60793869]] Rep 177 estimate: [[39651.28878098] [10032.53114015] [ 718.29337211]] Rep 178 estimate: [[39651.79886099] [10025.78545733] [ 711.18636276]] Rep 179 estimate: [[39668.81176963] [ 9950.64830118] [ 707.17029946]] Rep 180 estimate: [[39664.55033915] [10025.69574577] [ 682.17967506]] Rep 181 estimate: [[39678.45863918] [10000.94471527] [ 714.3528562 ]] Rep 182 estimate: [[39623.72938936] [10009.48496587] [ 793.69376095]] Rep 183 estimate: [[39638.09616265] [10031.80512906] [ 714.30847955]] Rep 184 estimate: [[39658.07866713] [10045.5638743 ] [ 702.43641255]] Rep 185 estimate: [[39651.57617443] [10039.40055719] [ 696.51313447]] Rep 186 estimate: [[39644.5923245 ] [ 9999.10838089] [ 703.16412766]] Rep 187 estimate: [[39667.01121342] [ 9994.92130014] [ 707.2300142 ]] Rep 188 estimate: [[39655.6793376 ] [ 9993.46295671] [ 667.46454071]] Rep 189 estimate: [[39674.40927094] [10038.5271482 ] [ 689.60601406]] Rep 190 estimate: [[39670.97584095] [ 9989.99235183] [ 689.55471574]] Rep 191 estimate: [[39656.01227867] [ 9993.23920429] [ 763.2792197 ]] Rep 192 estimate: [[39632.08646723] [10037.74031865] [ 709.21037716]] Rep 193 estimate: [[39672.99028294] [10050.50258356] [ 735.72762695]] Rep 194 estimate: [[39725.59632485] [ 9997.65628356] [ 732.66952791]] Rep 195 estimate: [[39684.14531155] [10002.98869804] [ 717.85523503]] Rep 196 estimate: [[39674.37506247] [10014.25014105] [ 720.57318313]] Rep 197 estimate: [[39616.73965371] [10013.33283978] [ 631.645646 ]] Rep 198 estimate: [[39638.25336916] [10025.39517299] [ 707.12836366]] bootstrapped non-clustered version of se: [30.7524 34.9706 33.2355] standard error without clustering from statsmodel: [31.4168 35.2034 35.2034] There you have it – the bootstrapped version of the standard errors. Note that we’ve not accounted for the clusters and one can do that by sampling clusters (with replacement) rather than units. Logistic regression Course: Causal Inference Topic: Logistic Regression Say, you need to predict the probability of someone being in a good vs. bad health (binary outcome) given a set of inputs: i) college education (yes or no); ii) income (high vs. low); iii) insured vs uninsured; and iv) stress level (continuous variable). For the sake of simplicity, we are going to assume a super simple DGP as follows: College education has a positive effect on health (coefficient = 0.1) High income has a positive effect on health (coefficient = 0.2) 40 percent more people from higher income households have college education Insurance has a positive effect on health (coefficient = 0.05) 60 percent more people from low income households are likely to be stressed Stress has a negative effect on health (coefficient = -1) Note: We know from the previous lecture that LPM estimates directly gives us the marginal effects – the coefficients (at least theoretically) are interpreted as the effect of marginal changes in \\(X\\)s on \\(y\\). LPM estimates are straight forward and easy to interpret as most often we are concerned with marginal effects from a policy standpoint. However, the coefficients pertaining to logistic regression are not marginal effects. This will be clearer as we proceed. We looked at the Linear Probability Model (LPM) in the previous lecture – the estimates from a properly specified model were close to the true parameters. What if we have to estimate probability of someone being in good health? In this case, LPM does not gurantee that the probabilities are restricted between 0 and 1. Logistic regression is a tool that is used to model binary outcome and used for classification purposes. It uses a logistic function to restrict probabilities between values of 0 and 1. So how does it work? Essentially, the primary goal here is to predict probabilities of a binary event. The true probability is written as a function of \\(\\theta\\) and \\(X\\): \\[ p = h_\\theta(X) = \\sigma(\\theta X) \\] \\(\\theta\\) is a vector of true parameters – they govern the DGP, and probabilites are the function of the true coefficients and inputs. Specifically, \\(\\sigma(.)\\) is the logistic function, defined as: \\[ \\sigma(z) = \\frac{1}{(1 + exp(-z))} \\] This \\(\\sigma(.)\\) is also known as the sigmoid function and \\(z=\\theta X\\) is often known as the logit. The logit is a linear combination of \\(\\theta\\) and \\(X\\). By nature of the logistic function, the output is restricted between 0 and 1. To see the logistic function closely, let’s take a look at the following graph. # import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import add_dummy_feature from pathlib import Path # generate numbers from -5 to 5 z = np.linspace(-5, 5, 1000) print(z[0:20]) # compute logistic values (note that these are probabilities) sigma_z = 1/(1 + np.exp(-z)) plt.figure(figsize=(8, 5)) plt.plot(z, sigma_z) plt.xlabel(&#39;z&#39;) plt.title(&#39;A sigmoid function&#39;) plt.ylabel(&#39;probability&#39;) plt.show() [-5. -4.98998999 -4.97997998 -4.96996997 -4.95995996 -4.94994995 -4.93993994 -4.92992993 -4.91991992 -4.90990991 -4.8998999 -4.88988989 -4.87987988 -4.86986987 -4.85985986 -4.84984985 -4.83983984 -4.82982983 -4.81981982 -4.80980981] png Note that the logistic function is S-shaped – negative logit (z) values will have probabilities less than 0.5, whereas the positive z values will have probablities greater than 0.5. Also, probabilities on the vertical axis are constrained between 0 and 1, as they should be. The inputs in the sigmoid function is: \\(z=\\theta X\\), which will help us attain probabilities. \\(X\\) and \\(\\theta\\) are the features (covariates) and the parameters of interest, respectively. Using these probability values, one can classify. For example: \\(y_i = 1\\) if \\(\\hat{p_i}\\geq 0.5\\) or else 0. Our goal is to come up with the estimates of \\(\\theta\\). After we have \\(\\hat{\\theta}\\), we can obtain probabilities, perform classification based on them, or use probability estimates for downstream analysis. The Loss Function To do so, we will start with a loss function. Consider the following: \\(C = -\\log(\\hat{p_i})\\) if \\(y_i=1\\) \\(C = -\\log(1-\\hat{p_i})\\) if \\(y_i=0\\) Generally speaking, you want the model to come up with higher probabilities for observations with \\(y_i=1\\) and lower probabilities for \\(y_i=0\\). With this in mind, consider what might happen if \\(\\hat{p_i}\\) is small vs large (say, 0.05 vs 0.95) when \\(y_i=1\\). This will inflate the loss in the former case but reduce it in the latter. The case is reversed for \\(y_i=0\\); higher probabilities will yield higher loss; whereas, lower probabilities will yield lower loss values. So, lower probabilities are ‘shunned’ for observations with \\(y_i=1\\), and higher probabilities are penalized more for observations with \\(y=0\\). We put this logic together and come up with the following cross-entropy loss function: \\[ C_{\\theta} = -\\frac{1}{n} \\sum_i^{n} [y_i \\times \\log(\\hat{p_i}) + (1-y_i) \\times \\log(1-\\hat{p_i})] \\] Recall: \\[ p = \\sigma(\\theta X) = \\frac{1}{(1 + exp(-\\theta X))} \\] The objective is to get the estimates of \\(\\theta\\) that minimizes the loss function. Turns out that the loss function above don’t have an analytical or a closed form solution. However, the function is convex, which means that we can use gradient descent to estimate \\(\\theta\\). Using Gradient Descent Let’s first simulate the data following the DGP mentioned above. Note that the functional that’s used to simulate the outcome variable (health) will depend on probability values obtained from the logistic function. # ---------------------------- # A. Simulate data # ---------------------------- # 1. College education has a positive effect on health (coefficient = 0.1) # 2. High income has a positive effect on health (coefficient = 0.2) # 3. 40 percent more people from higher income households have college education # 4. Insurance has a positive effect on health (coefficient = 0.05) # 5. Stress has a negative effect on health (coefficient = -1) # number of obs n = 100000 # 1. income income_log = np.random.lognormal(0, 1, n) income = income_log * 20000 ln_income = np.log(income) # categorize high vs low income based on median income high_income = (income&gt;=np.median(income)).astype(&#39;int&#39;) low_income = (income&lt;np.median(income)).astype(&#39;int&#39;) # 2. college def gen_college(prob): col = np.random.binomial(1, prob, 1) return col college = [] for i in range(n): # 40% more people from high income group will have college degree college_i = gen_college(0.2 + 0.4*high_income[i]) college.append(college_i) college = np.array(college).ravel() print(f&quot;mean of college: {college.mean()}&quot;) print(f&quot;share college for high income: {np.mean(college[high_income == 1])}&quot;) print(f&quot;share college for low income: {np.mean(college[high_income == 0])}&quot;) # 3. Stress def gen_stress(prob): p = np.random.binomial(1, prob, 1) return p stress = [] for i in range(n): # 60% more people in low income will be stressed stress_i = gen_stress(0.6*low_income[i]) stress.append(stress_i) # a continuous stress variable dependent on income status stress = np.array(stress).ravel() + np.random.normal(3, 1, n)*low_income + np.random.normal(0, 1, n) # histogram of the stress index plt.figure(figsize=(8, 5)) plt.hist(stress, bins=30, color=&quot;steelblue&quot;, edgecolor=&quot;black&quot;, alpha=0.3) plt.xlabel(&#39;stress index&#39;) plt.ylabel(&#39;frequency&#39;) plt.title(&#39;Histogram of stress index&#39;) plt.show() print(f&quot;average stress index for low income group: {stress[high_income==0].mean().round(4)}&quot;) print(f&quot;average stress index for high income group: {stress[high_income==1].mean().round(4)}&quot;) # 4. Insurance (exogeneous -- does not depend on other Xs) insurance = np.random.binomial(1, 0.3, n) print(f&quot;fraction insured: {insurance.mean()}&quot;) # 5. health (Y variable) def gen_health(prob): h = np.random.binomial(1, prob, 1) # these probabilities are going to come from the logistic function return h # ---------------------------------------------------- # Logistic regression using the gradient descent # ---------------------------------------------------- # define the logistic function def sigma(input): logistic = 1/(1 + np.exp(-input)) return logistic # true thetas governing the DGP theta_true = np.array([0.3, 0.1, 0.2, 0.05, -1]) sigma(theta_true) X = np.concatenate((college.reshape((n, 1)), high_income.reshape((n, 1)), insurance.reshape((n, 1)), stress.reshape((n, 1))), axis=1) X = add_dummy_feature(X) # this adds 1 in the first column (intercept) z = X @ theta_true.reshape((5, 1)) prob_logit = sigma(z) # output true probabilities # NOTE: PROBABILITIES COME FROM THE LOGISTIC FUNCTION. THIS IS THE KEY TO SIMULATE LOGISTIC REGRESSION. # Step 1: Calculate linear combination (logit): z = X @ theta # Step 2: Transform to probabilities: p = sigma(z) = 1/(1 + exp(-z)) # Step 3: Generate binary outcomes using these probabilities using a binomial dist. plt.figure(figsize=(8, 5)) plt.hist(prob_logit, bins=30, color=&quot;steelblue&quot;, edgecolor=&quot;black&quot;) plt.grid(True, alpha=0, linestyle=&#39;--&#39;) plt.title(&#39;True probabilities using true theta values&#39;) plt.show() # generate health using probabilities health = [] for i in range(n): health_i = gen_health(prob_logit[i]) # AGAIN, THESE PROBABILITIES COME FROM THE LOGISTIC FUNCTION health.append(health_i) health = np.array(health).ravel() mean of college: 0.40291 share college for high income: 0.603 share college for low income: 0.20282 png average stress index for low income group: 3.6107 average stress index for high income group: 0.0004 fraction insured: 0.30029 png Since, this is a simulation, we know the true probabilities generated using the true coefficients and the DGP. From a practitioner’s standpoint, we won’t know the true probabilities in non-experimental settings, since we don’t know the true DGP to begin with. We have to estimate them. Let’s print out our some summary measures on health. #print(f&quot;y variable: {health} \\n&quot;) #print(f&quot;X matrix: {X}&quot;) print(f&quot;fraction with good health: {health.mean()}&quot;) # create a stress band around the mean for no college, low income and uninsured mean_stress_baseline = stress[(college==0) &amp; (high_income==0) &amp; (insurance==0)].mean() stress_tolerance = 0.5 # within ±0.5 of mean stress_band = (np.abs(stress - mean_stress_baseline) &lt;= stress_tolerance) print(f&quot;fraction with good health among no school, low income, and uninsured: {np.mean(health[(college==0) &amp; (high_income==0) &amp; (insurance==0) &amp; (stress_band)]).round(4)}&quot;) fraction with good health: 0.34948 fraction with good health among no school, low income, and uninsured: 0.0353 The true \\(\\theta\\) values are \\([\\theta_0=0.3, \\theta_1=0.1, \\theta_2=0.2, \\theta_3=0.05, \\theta_4=-1]\\). 0.3 is the intercept coefficient, representing people in no college, low income, and uninsured group. 0.1 corresponds to college coefficient. 0.2 corresponds to high income coefficient. 0.05 corresponds to insurance coefficient. -1 corresponds to stress coefficient. Note that 3.61 percent of people who have no schooling, are of low income, are uninsured and around the mean stress index are in good health. This pertains to true \\(\\theta\\) of 0.3. Let’s convert this value into probability using: p_gh = 1/(1 + np.exp(-0.3 + np.mean(stress[(college==0) &amp; (high_income==0) &amp; (insurance==0)]))) print(f&quot;the conversion of theta = 0.3 + mean stress value to prob: {p_gh.round(4)} \\n&quot;) the conversion of theta = 0.3 + mean stress value to prob: 0.035 Notice that according to the DGP, around 3.55 percent of people in the population with no college, low income, uninsured, and of the mean stress value are in good health. This is close to what we have in our sample. Hence, it is important to recognize that \\(\\theta\\) values are coefficients and in the case of logistic regression; they are different from probabilities. Gradient Descent Let’s move on to the gradient descent and its usage in estimating \\(\\theta\\). Simply put, gradient is a vector of the partial derivatives of the loss function with respect to each \\(\\theta\\) stacked together. \\[ \\text{gradient} = \\begin{bmatrix} \\frac{\\partial C}{\\partial \\theta_0} \\\\ \\frac{\\partial C}{\\partial \\theta_1} \\\\ \\frac{\\partial C}{\\partial \\theta_2} \\\\ \\frac{\\partial C}{\\partial \\theta_3} \\end{bmatrix} \\] Before we get to the gradient of the logistic kind, let’s stack the loss function using matrices: \\[C_{\\theta} = -\\frac{1}{n} [Y^{t} \\log(p) + (1-Y^{t}) \\log(1- p)]\\] Replacing \\(p= \\sigma(\\theta X)\\), we have: \\[C_{\\theta} = -\\frac{1}{n} [Y^{t} \\log(\\sigma(X\\theta)) + (1-Y^{t}) \\log(1-\\sigma(X\\theta)]\\] \\(C\\) will be a scalar. Next, get \\(\\frac{\\partial C}{\\partial \\theta}\\). But first, here are the dimensions of terms in the RHS: \\(X: (n\\times 5)\\) \\(Y^{T}: (1\\times n)\\) \\(\\theta : (5\\times 1)\\) \\(X\\theta : (n\\times 1)\\) \\(Y^{t} \\log(\\sigma(X\\theta)): scalar\\) Taking the partial derivative of the newly formatted cost function \\(C\\) with respect to \\(\\theta\\), you get the gradient vector as follows: \\(\\frac{\\partial C}{\\partial \\theta} = \\frac{1}{n} X^{T}(\\sigma(X \\theta) - Y)\\). where, \\(X^{T}\\) is a \\(5\\times n\\) matrix and \\((X^{T}\\sigma(X \\theta) - Y)\\) is a \\(n \\times 1\\) matrix. I solved for the partial using the brute force chain rule. One thing to note while solving is a small trick below: \\(\\frac{exp(\\theta X)}{1 + exp(\\theta X)} = \\frac{1 + exp(\\theta X) -1}{1 + exp(\\theta X)}\\). This results to: \\(1 - \\frac{1}{1 + exp(\\theta X)} = 1 - \\sigma(\\theta X)\\). This is getting into minute little details. You can escape this and just take the word for the gradient or you could try it all out. Upto you! Now that we are through with all this, the gradient descent algorithm is straight forward. Gradient Descent Algorithm Initialize the \\(\\theta_{gd}\\) values. I’ve used values from the normal distribution. Initialize the learning rate – \\(\\eta\\) and the number of interations (epochs). We set \\(\\eta = 0.5\\) and epochs=50. Compute the gradient. Call this \\(gd_i\\). Adjust \\(\\theta\\) using the gradient and the learning rate as: \\(\\theta_{gd} = \\theta_{gd} - \\eta \\times gd_i\\). Note that we have to move against the gradient; hence, the negative. Iterate steps 3 and 4 for \\(iter=epochs\\) number of times or until the algorithm converges. # Gradient Descent theta_gd = np.random.normal(0, 1, 5) # initial theta values from the normal dist. epsilon = 1e-15 # to prevent overflow coming from logit values close to 0. epochs = 50 # number of iterations eta = 0.5 # learning rate loss = [] for i in range(epochs): z = np.clip(X @ theta_gd.reshape((5, 1)), -500, 500) gradient_i = ((sigma(z) - health.reshape((n, 1))).transpose() @ X) / n # caculate the gradient theta_gd = theta_gd - eta*gradient_i # adjust theta by moving opposite to the gradient loss_i = np.mean(health*(-np.log(sigma(z)+epsilon).ravel()) + # calculate loss (1-health)*(-np.log(1-sigma(z)+epsilon).ravel())) loss.append(loss_i) # append loss print(f&quot;theta estimates from gradient descent: {theta_gd.round(4)} \\n \\n&quot;) plt.figure(figsize=(8, 5)) plt.plot(np.linspace(1, epochs, epochs), np.array(loss).ravel()) plt.xlabel(&#39;epochs&#39;) plt.ylabel(&#39;cross-entropy loss&#39;) plt.title(&quot;Loss with respect to iterations&quot;) plt.show() theta estimates from gradient descent: [[-0.296 -0.2989 1.1594 -0.1458 -0.8166]] png We’ve now estimated the \\(\\theta\\) using gradient descent. Let’s verify our results using the in-built library in sklearn that estimates the Logistic Regression. # compare estimates from sklearn mod = LogisticRegression(max_iter=epochs, fit_intercept=False, C=np.inf) mod_fit = mod.fit(X, health) print(f&quot;Estimates from sklearn: {mod_fit.coef_}&quot;) results = { &quot;GD&quot;: theta_gd.ravel().round(3), &quot;SK&quot;: mod_fit.coef_.ravel().round(3) } pd.DataFrame(results) Estimates from sklearn: [[ 0.32179444 0.08030071 0.20651284 0.01809187 -0.99105091]] /home/vinish/Dropbox/Machine Learning/myenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1170: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters warnings.warn( .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } GD SK 0 -0.296 0.322 1 -0.299 0.080 2 1.159 0.207 3 -0.146 0.018 4 -0.817 -0.991 Let’s estimate the model using the LPM – note that this is a wrong functional form at use. # linear regression mod_linear = LinearRegression(fit_intercept=False) # we dont want to double fit the intercept; X already contains it mod_linear = mod_linear.fit(X, health) print(f&quot;Estimates from linear reg: {mod_linear.coef_}&quot;) Estimates from linear reg: [ 0.44695463 0.01138116 0.16453216 0.0015579 -0.10234894] The estimates from the gradient descent and sklearn are virtually similar. Note that the interpretation of \\(\\theta\\) estimates are not equivalent to marginal effects as they are in the LPM set up. Recall, in the case of logistic regression: \\(\\hat{p} = \\frac{1}{(1 + exp(-\\theta X))}\\). Hence, we need to translate \\(\\theta\\) into marginal effects before comparing them with LPM’s estimates. Calculation of marginal effect needs to be with respect to a given benchmark. There is nothing wrong with creating a benchmark. Say, we consider person A: with no college, low income, uninsured, and stress level around the mean (for the group with no college, uninsured, and low income) as this benchmark person and the marginal effects are computed with respect to this person. The following code translates \\(\\theta\\) into marginal effect. def fun_me(theta_vals, person): logit = (theta_vals @ person).ravel() p = sigma(logit) return p # create person A: without college, low income, and uninsured # NOTE: This will be our benchmark person. person_A = np.array([1, 0, 0, 0, mean_stress_baseline]).reshape((5, 1)) prob_health_A = fun_me(theta_gd, person_A) print(f&quot;The probability that person A is in good health is:{prob_health_A} \\n&quot;) # Person B: with college but low income and uninsured person_B = np.array([1, 1, 0, 0, mean_stress_baseline]).reshape((5, 1)) prob_health_B = fun_me(theta_gd, person_B) print(f&quot;The probability that person B is in good health is: {prob_health_B} \\n&quot;) # Person C: with high income but without college and uninsured person_C = np.array([1, 0, 1, 0, mean_stress_baseline]).reshape((5, 1)) prob_health_C = fun_me(theta_gd, person_C) print(prob_health_C) # Person D: with insurance but without college and low income person_D = np.array([1, 0, 0, 1, mean_stress_baseline]).reshape((5, 1)) prob_health_D = fun_me(theta_gd, person_D) print(prob_health_D) # Person E: Same as Person A but one unit increase in stress for marginal effects person_E = np.array([1, 0, 0, 0, mean_stress_baseline + 1]).reshape((5, 1)) prob_health_E = fun_me(theta_gd, person_E) print(prob_health_E) The probability that person A is in good health is:[0.03732477] The probability that person B is in good health is: [0.02795158] [0.11001232] [0.0324238] [0.01684625] Since we are using Person A as the benchmark, we can compute marginal probabilities simply by substracting probabilities. me_B_A = prob_health_B - prob_health_A me_C_A = prob_health_C - prob_health_A me_D_A = prob_health_D - prob_health_A me_E_A = prob_health_E - prob_health_A me = np.array([prob_health_A, me_B_A, me_C_A, me_D_A, me_E_A]) results_me_lpm = { &quot;ME:logistic&quot;: me.ravel().round(3), &quot;ME:LPM&quot;: mod_linear.coef_.ravel().round(3) } pd.DataFrame(results_me_lpm) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ME:logistic ME:LPM 0 0.037 0.447 1 -0.009 0.011 2 0.073 0.165 3 -0.005 0.002 4 -0.020 -0.102 The marginal effects from LPM and logistic regression are shown in the table. Let’s take a look at predicted probabilities from both LPM and logistic regression models: # prediction from LPM prob_linear = mod_linear.predict(X) # LPM directly gives probabilites print(prob_linear) plt.figure(figsize=(8, 5)) plt.hist(prob_linear, bins = 30, color=&quot;red&quot;, edgecolor=&quot;black&quot;, alpha=0) plt.xlabel(&#39;probability&#39;) plt.ylabel(&#39;frequency&#39;) plt.title(&#39;Predicted probabilities from LPM&#39;) plt.grid(True, alpha=0.3, linestyle=&quot;--&quot;) plt.show() # predictions from logistic prob_logit_predict = mod_fit.predict_proba(X) # need to call .predict_proba() to get probabilities from logistic model plt.figure(figsize=(8,5)) plt.hist(prob_logit_predict[:,1], bins=30, color=&quot;steelblue&quot;, edgecolor=&quot;black&quot;, alpha=0.3) #plt.hist(prob_logit, bins=30, color=&quot;red&quot;, edgecolor=&quot;black&quot;, alpha=0.3) plt.xlabel(&#39;probability&#39;) plt.ylabel(&#39;frequency&#39;) plt.grid(True, alpha=0.3, linestyle=&#39;--&#39;) plt.title(&#39;Predicted probabilities from Logistic Regression&#39;) plt.show() [ 0.29927004 0.64684742 0.74602907 ... -0.02085397 0.80991622 0.74443065] png png We see that the predicted probabilities from LPM are negative (this can’t happen), whereas those from the logistic model closely mimic the actual probabilities. Hence, if the goal is to attain probabilities then logistic regression is clearly better than LPM. LPM vs Logistic Regression From a practitioner’s perspective, one can get by using LPM if the goal is to infer causality alone and you aren’t concerned about predicting probabilities. It is simple, easy to interpret, and will require low computational power. It does mean that you should, but you can get by. However, if the goal is to predict, say probability of the binary outcome, then LPM is a no-go. In the world of causality, the importance of estimating the probability of someone being treated (vs untreated) cannot be overstated. We know this as propensity scores. Logistic regression can be a good starting model while estimating propensity scores. "],["causal-inference.html", "5 Causal Inference", " 5 Causal Inference “Correlation is not causality” is one of the most frequently used lines in social science. In a lab experiment, a researcher can perform controlled experiments to determine whether A causes B by controlling for confounders. However, the complexities and interrelations of human behavior create a setting starkly different from the controlled environment of a lab, making things much more convoluted. Causal inference, therefore, can be seen as a process to determine whether A causes B in both lab settings and out-of-lab scenarios. A simple example. Say, we are interested in evaluating the effects of a tutoring program on exam scores for an introductory course. To begin, in this simple example, we assume that the treatment is (completely) randomly assigned. The class is randomly divided into two groups: one group receives the treatment (treatment group) and the other group does not receive the treatment (control group). Proper randomization means that each individual has an equal probability of receiving the treatment or not receiving it. This approach with an arbitrarily high probability ensures balance in both observed and unobserved factors as the sample size grows such that any differences in outcomes between the treatment and control groups can be attributed to the treatment itself, rather than to pre-existing differences between the groups.1 Balance here is defined as an instance when all pre-treatment covariates between the treatment and control groups are similar. If this is attained then it increases confidence that the treatment and the control units are comparable. Set up. We use \\(W\\) to denote the treatment status such that \\(W_i \\in \\{0, \\; 1\\}\\), \\(Y_i\\) is the exam score following the treatment assignment, and \\(X_i\\) are the covariates (e.g., gender, race). The subscript \\(i\\) indicates an individual or unit of observation. Of course, balance is not guranteed and in such cases one should think hard whether differences in covariates matter, and if they do, adjustment should be applied.↩︎ "],["potential-outcome-framework-neyman-rubin-causal-model.html", "5.1 Potential Outcome Framework: Neyman-Rubin Causal Model", " 5.1 Potential Outcome Framework: Neyman-Rubin Causal Model We are going to use the potential outcome framework to describe the impacts of the treatment following the Neyman-Rubin causal model (Splawa-Neyman, Dabrowska, and Speed 1923 [1990]; Rubin 1974). Define \\(Y_i(0)\\) and \\(Y_i(1)\\) as the potential outcomes for an individual \\(i\\) in the case of treatment and without treatment, respectively. The potential outcomes are not realized yet. As such it is wrong to say that \\(Y_{i}(0) = Y_i\\). Let’s spend some time discussing various formats of the potential outcome in relation to what is observed versus what is not. \\([Y_{i}(0)|W_i = 0].\\) Here, the expression in the bracket is read as the outcome of an unit \\(i\\) in the no-treatment state conditional upon \\(i\\) actually not receiving the treatment. This is an observed outcome. \\([Y_{i}(0)|W_i = 1].\\) Here, the expression is asking for what the outcome of an unit \\(i\\) who received the treatment \\((W_i = 1)\\) would be in absence of the treatment. This is not observed and is termed as the counterfactual. The same goes with the potential outcome \\(Y_{i}(1)\\) – the outcome if \\(i\\) were to be treated. The observed variable, \\(Y_i\\), can be written as a function of the potential outcomes as follows: \\[\\begin{equation} Y_i = W \\times Y_i(1) + (1-W)\\times Y_i(0) \\end{equation}\\] The fundamental problem is that one cannot observe both \\(Y_i(0)\\) and \\(Y_i(1)\\) at the same time. As such, the causal inference through the lens of Neyman-Rubin causal framework can be seen as the missing data problem. If one has the data for \\(Y_i(1)\\) then the \\(Y_i(0)\\) counterpart is missing and vice-versa. Much of causal inference is finding ways to deal with the missing-data problem. The independence assumption allows us to proceed further with causality. Formally, a complete random assignment of treatment means: \\(W_i \\perp Y_i(0), Y_i(1)\\). This states that the treatment assignment is independent of potential outcomes. Quite literally, this means that the treatment assignment is not related to the potential outcome. In other words, the treatment assignment is completely random (probability of being treated is 0.5 in the case of binary treatment). The independence assumption also states that the treatment assignment is independent of any covariates \\(X_i\\). In our particular example, this means that the probability of receiving the treatment is the same for different groups defined by these covariates, such as gender and race. Specifically, females are equally likely to get treated compared to males, and Blacks are equally likely to be treated compared to Whites. Within both the treatment and control groups, it is highly likely that the proportion of Blacks and Whites, as well as males and females, will be similar – an attribute known as balance. The independence assumption is one of the necessary assumptions to proceed further but it is not sufficient. Additional two assumptions are needed to proceed ahead: overlap and Stable Unit Treatment Value Assumption (SUTVA). The overlap assumption states that observations in both the treatment and control groups fall within the common support. For instance, this assumption is violated if the treatment group consist of all females and the control group consist of all males as one would not be able to attain balance in covariates. The independence and overlap assumption together constitute a property known as stong ignorability of assignment, which is necessary for the identification of the treatment effect. The SUTVA assumption is the no interference assumption defining that the treatment status of one unit should not affect the potential outcome for other units. In our example, tutoring treatment for a unit in the treatment group should not change the potential outcome for other units. This assumption breaks down if there is a spillover effect, for example, if the a student in the treatment group helps her friend in the control group. References "],["average-treatment-effect-ate.html", "5.2 Average treatment effect (ATE)", " 5.2 Average treatment effect (ATE) Our target is to estimate the effects of the treatment. For a brief moment, let’s assume the presence of a parallel universe that includes Alia, Ryan, Shrey, Samaira, and Rakshya in the course. In one universe (actual universe) the treatment for these individuals are randomly allocated: \\(W_{Alia} = 1\\), \\(W_{Ryan} = 0\\), \\(W_{Shrey} = 0\\), \\(W_{Samaira} = 1\\), \\(W_{Rakshya} = 0\\). In the other (parallel) universe, everything is similar to the actual universe except that the treatment status is exactly opposite. In this case, individual specific treatment effect can be estimated by taking the difference in individual specific outcomes across two universes. For example, the treatment effect for Alia is \\(Y_{Alia}(1) - Y_{Alia}(0).\\) This is feasible since a perfect counterfactual is available for all the units given the parallel universe. The average of such individual treatment effect gives the average treatment effect, ATE. The target is to estimate average treatment effect (ATE), which is defined as: \\[\\begin{equation} ATE = E(Y_i(1)) - E(Y_i(0)) \\end{equation}\\] \\(Y_i(1)\\) denotes the outcome for an unit \\(i\\) in presence of treatment, whereas \\(Y_i(0)\\) is the realization for the same unit \\(i\\) in absence of the treatment. As we know, it is impossible to measure the unit \\(i\\) in two different states (with and without treatment). A major difficulty is that one cannot observe units simultaneously with and without treatment in reality. This means that the perfect counterfactual does not really exist. This again emphasizes causal inference as a missing data problem – when estimating the treatment effect of an unit \\(i\\), \\(Y_i(0)\\) is not observed if \\(Y_i(1)\\) is and vice-versa. This unfortunately does not allow us to estimate individualized treatment effect. The best we can do (as of yet) is use the independence assumption as well as overlap assumption together and evalute ATE. Note that the independence condition, \\(W_i \\perp Y_i(0), \\; Y_i(1)\\), gives: \\(E(Y_i(0)|W_i = 1) = E(Y_i(0)|W_i = 0)\\). The term, \\(E(Y_i(0))\\), in ATE equation is replaced by \\(E(Y_i|W_i = 0)\\). In the case of a pure randomized experiment, the ATE is given as: \\[\\begin{equation} ATE = E(Y_i | W_i = 1) - E(Y_i | W_i = 0) \\end{equation}\\] ATE evaluates treatment effect for the whole population by comparing the treated units to the control units. "],["rct.html", "5.3 RCT", " 5.3 RCT Randomized Controlled Trials (RCTs) are the cornerstone of causal inference and are often referred to as the gold standard. The quality of non-experimental studies is frequently assessed by comparing how closely the observational setting approximates an RCT. In an RCT, the Average Treatment Effect (ATE) is identified through the randomization of treatment assignment. This process ensures that the treatment and control groups are comparable, making RCTs a straightforward yet immensely powerful tool for establishing causal relationships. In a simple RCT setting the treatment is binary – the units are either assigned to the treatment group \\((W_i = 0)\\) or the control group \\((W_i = 1)\\). The implicit assumption in this design is that each unit has an equal probability of being treated. The treatment assignment for the RCT setting can be attained using a Bernoulli process, where each unit has an independent probability \\(\\pi\\) of receiving treatment. Specifically, each unit is assigned to the treatment group with probability \\(\\pi\\) and to the control group with probability \\(1 - \\pi\\). # a bernoulli process of treatment assignment library(ggplot2) fun_treat_assign &lt;- function(N, prob, treat.type){ treatment &lt;- rbinom(N, size = 1, p = prob) dat &lt;- data.frame(treatment = treatment, type = treat.type) return(dat) } # p = 0.5 for each unit dat1 &lt;- fun_treat_assign(N = 10000, prob = 0.5, treat.type = &quot;p = 0.5&quot;) # p = 0.3 for each unit dat2 &lt;- fun_treat_assign(N = 10000, prob = 0.2, treat.type = &quot;p = 0.2&quot;) dat.assign &lt;- rbind(dat1, dat2) # plot ggplot(dat.assign, aes(x = treatment)) + geom_histogram(fill= &quot;skyblue&quot;, color = &quot;black&quot;) + facet_wrap(~ type) + theme_minimal() ## `stat_bin()` using `bins = 30`. Pick better value `binwidth`. A practical example of this is an unbiased coin toss used to determine treatment assignment. In this case, a head could correspond to the treatment group (e.g., \\(W_i = 1\\)), and a tail to the control group (e.g., \\(W_i = 0\\)). This method exemplifies Bernoulli-randomization, where the assignment is determined by a random process, ensuring that each unit has an equal probability of being assigned to either group. In an RCT setting, the difference-in-means estimator is given as: \\[\\begin{equation} \\hat{\\tau} = \\frac{1}{N_t}\\sum_{W_i =1} Y_i - \\frac{1}{N_c}\\sum_{W_i =0} Y_i \\end{equation}\\] The difference-in-mean estimator is unbiased and consistent for the average treatment effect. "],["average-treatment-effect-on-the-treated-att.html", "5.4 Average treatment effect on the treated (ATT)", " 5.4 Average treatment effect on the treated (ATT) The average treatment effect on the treated is concerned with the evaluation of treatment effects for only those units that are treated. Formally, it is defined as: \\[\\begin{equation} ATT = E(Y_i(1) - Y_i(0) | W_i = 1) \\end{equation}\\] ATE is only concerned with a subset of the population who received the treatment, \\(E[.|W_i = 1]\\). Here, ATT is comparing outcomes among the treated units in presence of the treatment versus what the outcomes would have been in absence of the treatment only for units receiving the treatment. Hence, only one segment of the counterfactual (potential outcome) is required. In our example, the counterfactual for Alia and Samaira would allow estimation of ATT, whereas ATE requires counterfactual for everyone. The independence condition states that on average the potential outcomes for the treated group in absence of the treatment would be similar to the average outcome for the control group, i.e. \\(E(Y_i(0)|W_i = 1) = E(Y_i(0)|W_i = 0)\\). This allows re-writing ATT as the following: \\[\\begin{align} ATT = E\\{E(Y_i | W_i = 1) - E(Y_i | W_i = 0) | W_i = 1\\} \\\\ ATT = E(Y_i | W_i = 1) - E(Y_i | W_i = 0) \\end{align}\\] The second line follows from the independence assumption which allows this: \\(E\\{E(Y_i | W_i = 0) | W_i = 1\\} = E(Y_i | W_i = 0).\\) Under the independence assumption this means that we can estimate ATT by substrating the averages of exam score across the treated and control units. In purely randomized experiments, if there is a perfect case of compliance, then the ATT will be similar to ATE. "],["an-estimation-example.html", "5.5 An estimation example", " 5.5 An estimation example # create a function to estimate the treatment effects fun_ATE &lt;- function(tau, N){ # @arg tau: true treatment effect # @arg N: sample size # Return tau_hat: estimate of the treatment effect gender &lt;- rbinom(N, size = 1, p = 0.5) race &lt;- rbinom(N, size = 1, p = 0.5) W &lt;- rbinom(N, size = 1, p = 0.5) Y &lt;- 50 + tau * W + gender * 5 + race * 10 + rnorm(n = N, mean = 5, sd = 5) tau_hat &lt;- mean(Y[which(W == 1)]) - mean(Y[which(W == 0)]) return(tau_hat) } # print treatment effect print(paste(&quot;the ATE estimate is: &quot;, fun_ATE(tau = 10, N = 2000))) ## [1] &quot;the ATE estimate is: 10.4120024165428&quot; # run 2000 replications to get a distribution of tau_hats reps &lt;- 2000 tau.hats &lt;- rep(0, reps) for(i in 1:reps){ tau.hats[i] &lt;- fun_ATE(tau = 10, N = 2000) } # histogram of tau hats hist(tau.hats, breaks = 30) # obtaining the standard error print(paste(&quot;the mean of tau hats : &quot;, mean(tau.hats))) ## [1] &quot;the mean of tau hats : 9.99488871337007&quot; print(paste(&quot;the standard error of tau hats : &quot;, sd(tau.hats))) ## [1] &quot;the standard error of tau hats : 0.337461879683807&quot; "],["unconfoundedness-assumption.html", "5.6 Unconfoundedness assumption", " 5.6 Unconfoundedness assumption Most of the time the treatment assignment may not be fully random but can be driven by some selective covariates. Referring to the tutoring example, it may be unethical to disallow someone in the control group who wants to attend the tutoring sessions. As such, tutoring sessions may be voluntarily held, where students can select whether to attend the session. Say, females and Blacks are more likely to attend the tutoring session and both of these variables are also likely to yield higher potential outcome. This means that females and Blacks are more likely to have higher exam score in absence of the treatment compared to males and Whites. It is easy to see that the treatment assignment is correlated with the potential outcomes and the independence assumption is violated. This is true in many cases of observational settings and even in randomized experiments. We require adjustments before being able to estimate treatment effects in such cases. If we understand the treatment mechanism fairly well then we can still proceed further to estimate the treatment effects. For example, suppose the treatment assignment is (voluntarily) more tilted towards females than males and Blacks than Whites. In this case, we would want to invoke unconfoundedness (conditional independence) assumption.2 Formally, this states that \\(Y_i(0), \\; Y_i(1) \\perp W_i | X_i\\). This means that conditional upon the covariates the treatment assignment is random. To estimate ATT one would want to first estimate ATT within each strata: \\(i)\\) female-Black, \\(ii)\\) female-White, \\(iii)\\) male-Black, and \\(iv)\\) male-White, and take a weighted average of the strata-specific ATEs by using the proportion of the sample in the given strata as weights. The conditional independence assumption means that within each strata treatment assignment is random. The following code first estimates the strata specific ATTs and then summarizes them using the weighted average. Note that the true treatment effect is 10. fun_ATE2 &lt;- function(N, tau){ # @arg tau: true treatment effect # @arg N: sample size # Return tau_hat: estimate of the treatment effect using conditional randomness assumption # Return tau_hat2: estimate of the treatment effect wrongly using unconditional independence assumption # Return reg_tau: estimate from conditioning using regression but from a misspecified model # create pseudo data gender &lt;- rbinom(N, size = 1, p = 0.5) race &lt;- rbinom(N, size = 1, p = 0.5) W &lt;- rbinom(N, size = 1, p = 0.2 + 0.4 * (gender &gt; 0) + 0.2 * (race &gt; 0)) Y &lt;- 40 + 10 * W + gender * 2 + race * 5 + 25 * race * gender + rnorm(n = N, mean = 5, sd = 5) # female-Blacks tau_hat1 &lt;- mean(Y[which(W == 1 &amp; gender == 1 &amp; race == 1)]) - mean(Y[which(W == 0 &amp; gender == 1 &amp; race == 1)]) w1 &lt;- sum(gender == 1 &amp; race == 1) / N # female-Whites tau_hat2 &lt;- mean(Y[which(W == 1 &amp; gender == 1 &amp; race == 0)]) - mean(Y[which(W == 0 &amp; gender == 1 &amp; race == 0)]) w2 &lt;- sum(gender == 1 &amp; race == 0) / N # male-Blacks tau_hat3 &lt;- mean(Y[which(W == 1 &amp; gender == 0 &amp; race == 1)]) - mean(Y[which(W == 0 &amp; gender == 0 &amp; race == 1)]) w3 &lt;- sum(gender == 0 &amp; race == 1) / N # male-Whites tau_hat4 &lt;- mean(Y[which(W == 1 &amp; gender == 0 &amp; race == 0)]) - mean(Y[which(W == 0 &amp; gender == 0 &amp; race == 0)]) w4 &lt;- sum(gender == 0 &amp; race == 0) / N tau_hat &lt;- tau_hat1 * w1 + tau_hat2 * w2 + tau_hat3 * w3 + tau_hat4 * w4 tau_hat2 &lt;- mean(Y[W == 1]) - mean(Y[W == 0]) # a mis-specified regression model reg &lt;- lm(Y ~ W + gender) reg_tau &lt;- coefficients(reg)[[2]] return(list(table(gender[W == 1]), table(race[W == 1]), tau_hat, tau_hat2, reg_tau)) } ATE2_results &lt;- fun_ATE2(N = 20000, tau = 10) print(paste(c(&quot;treated males: &quot;, &quot;treated females: &quot;) , ATE2_results[[1]])) ## [1] &quot;treated males: 2978&quot; &quot;treated females: 7070&quot; print(paste(c(&quot;treated Whites: &quot;, &quot;treated Blacks: &quot;) , ATE2_results[[2]])) ## [1] &quot;treated Whites: 3997&quot; &quot;treated Blacks: 6051&quot; print(paste(&quot;ATE conditioned on Xs is :&quot;, ATE2_results[[3]])) ## [1] &quot;ATE conditioned on Xs is : 9.97465533525352&quot; print(paste(&quot;ATE not conditioned on Xs is :&quot;, ATE2_results[[4]])) ## [1] &quot;ATE not conditioned on Xs is : 19.6663959822765&quot; # get tau_hats from replications store &lt;- rep(0, reps) store2 &lt;- store store.reg &lt;- store for(i in 1:reps){ ATE.results &lt;- fun_ATE2(N = 20000, tau = 10) store[i] &lt;- ATE.results[[3]] store2[i] &lt;- ATE.results[[4]] store.reg[i] &lt;- ATE.results[[5]] } # histogram of tau_hat conditioned hist(store, main = &quot;tau hats conditioned&quot;) print(paste(&quot;The standard error from the conditioned approach is:&quot;, sd(store))) ## [1] &quot;The standard error from the conditioned approach is: 0.0817898258403735&quot; hist(store2, main = &quot;tau hats not conditioned&quot;) The ATT estimate is much closer to the true parameter, 10, when conditioned upon the covariates as compared to an unconditional approach (where the distribution of ATT estimate is centered around 19.3). This example highlights the importance of conditioning on \\(X\\)s when evaluating the treatment effects if the treatment assignment is correlated with the potential outcomes. In this case, Blacks and females are more likely to have higher scores in general even without the treatment and both of these subgroups are also more likely to be treated. Treatment is not only non-random but is systematically correlated with the outcomes. Since we have the perfect information on the treatment assignment mechanism, after conditioning for the covariates the treatment assignment is essentially random. In other words, within Black vs. White race groups, for example, the treatment assignment is randomly allocated. This allows estimation of ATE for each subgroup or strata. After estimating ATE for each strata, the ATEs are averaged using the sample size of the strata as weights. One problem with the approach highlighted above is that in complex settings, with many determinants (multi-dimensionality) of treatment or in presence of continuous covariates, the sub-space required for the analyses highlighted above will be thinned out too soon. As an alternative, regression framework has been rigorously used as a tool-kit to control for covariates. While there are benefits of using a regression framework, it is by nomeans a panacea. This is especially true if the regression models are misspecified. Below we will use a misspecified version of the regression model to see if we can recover the treatment estimate close to the true value. print(paste(&quot;ATE estimated from misspecified regression model:&quot;, ATE2_results[[5]])) ## [1] &quot;ATE estimated from misspecified regression model: 14.4967149956478&quot; print(paste(&quot;The standard error is:&quot;, sd(store.reg))) ## [1] &quot;The standard error is: 0.177872732457276&quot; hist(store.reg, main = &quot;Treatment effects using regression&quot;) The terms unconfoundedness and conditional independence (\\(heart\\; disease \\perp age \\; | \\; cholestrol\\)) are used interchangebly in causal inference literature. Conditional independence is a broader term that relates to the general field of probability and statistics, whereas unconfoundedness is more specific to causal inference. Unconfoundedness implies a specific kind of conditional independence, specific to causal inference.↩︎ "],["discussion-1.html", "5.7 Discussion", " 5.7 Discussion In our discussion, we explored the causal effect through the lens of the potential outcome framework, emphasizing the crucial assumptions needed for its accurate identification. We particularly focused on the independence assumption and the unconfoundedness assumption, alongside the Stable Unit Treatment Value Assumption (SUTVA) and the overlap assumption. These assumptions play a pivotal role in ensuring valid causal inference, with the conditional independence assumption being highly effective in randomized controlled trials. However, in observational studies where randomization is not possible, the conditional independence assumption can be quite stringent and challenging to meet as there might be unobserved variables driving the treatment assignment. Consequently, it is essential to leverage alternative methodologies designed for observational settings. Exploring approaches such as propensity score matching, instrumental variables, regression discontinuity design, and difference-in-differences can help overcome these challenges and improve the robustness of causal effect estimation when randomization is not feasible. By employing these methods, we can better navigate the complexities of observational data and draw more reliable causal inferences. "],["reference.html", "5.8 Reference", " 5.8 Reference "],["ipw-and-aipw.html", "6 IPW and AIPW", " 6 IPW and AIPW The target is to estimate the average treatment effect (ATE): \\[\\begin{equation} \\label{eq:ATE} ATE = E[Y_i(1) - Y_i(0)] \\tag{6.1} \\end{equation}\\] Note that using the following two assumptions: \\(W_i \\perp \\{Y_i(0), \\; Y_i(1)\\}\\) (independence assumption) \\(Y_i(W) = Y_i\\) (SUTVA) the ATE estimate \\(\\hat{\\tau}\\) can be written as the difference-in-means estimator: \\[\\begin{equation} \\label{eq:ATE_estimator} \\hat{\\tau} = \\frac{1}{N_T} \\sum_{W_i = 1} Y_i - \\frac{1}{N_C} \\sum_{i \\in W_i = 0} Y_i \\end{equation}\\] where \\(N_T\\) and \\(N_C\\) are the number of treated and control units, respectively. In the previous lecture, we disscussed randomized control trial as an ideal approach to estimate ATE. In a randomized controlled trial each unit has an equal probability of receiving the treatment. This means the following: \\[\\begin{equation} P(W_i = 1 \\; | \\; Y_i(0), \\; Y_i(1), \\; n_T) = \\frac{n_T}{n}, \\; \\; i = \\{1, ...., n\\} \\tag{6.2} \\end{equation}\\] In equation (6.2), \\(n_T\\) refers to the number of units that receives the treatment.3 In an easy to understand set-up, if a researcher wants \\(P(W_i = 1) = 0.5\\) (unit is equally likely to be treated or untreated), a coin flip can feasibly be used as a mechanism to assign treatment.4 Although randomized controlled trials (RCTs) are often considered the gold standard in causal inference, they cannot always be used due to ethical, moral, and monetary reasons. Returning to the example we used in the previous chapter, it is not ethical to demarcate who can attend the tutoring session versus who cannot. In real-world scenarios, tutoring sessions are typically voluntary. Students who regularly attend these sessions may have different baseline (pre-treatment) characteristics compared to those who do not attend. These differences can introduce biases that complicate causal inference in observational studies. To proceed further in observational setting (without using RCTs), we require more knowledge about the treatment assignment. In other words, we need to understand which variables determine who attends the tutoring sessions. This information is crucial for identifying potential confounders and for applying methods that can help estimate causal effects in observational settings. In causal inference, confounders are variables that are associated with both the treatment and the outcome. They can introduce bias in the estimation of the causal effect of the treatment on the outcome by providing alternative explanations for any observed relationships. For example, say you are trying to evaluate the efficacy of a new drug on blood pressure level. If smokers are more likey to get treated and if they tend to have higher blood pressure to begin with, the treatment effects are likely to be understated. This brings us to the unconfoundedness assumption. Unconfoundedness: The treatment assignment is as good as random once we control for \\(X\\)s. \\[\\begin{equation} \\{W_i \\perp \\{Y_i(0), \\; Y_i(1)\\} | X_i \\} \\; for \\; all \\; x \\in \\chi. \\tag{6.3} \\end{equation}\\] As with the tutoring example, the independence assumption (discussed in the previous chapter) is highly unlikely to hold in observational settings. Let’s consider the following scenarios: Out of the ten states that are yet to expand Medicaid, eight fall in South. Medicaid expansion is not random. Cigarette taxes are higher in states with higher anti-smoking sentiments. Infrastructure development, such as construction of roads, schools, hopitals, are demand-driven. The list goes on .. However, if we manage to observe all the \\(X\\)s (covariates) that influence the treatment, we can invoke unconfoundedness for causal inference. Although it is generally recommended to assign half of the sample to the treatment group and the other half to the control group, this is not a strict requirement.↩︎ Of course, this is quicky going to be inefficient as the sample size increases. In general, treatment assignment is determinted by a statistical process via a software. For example, if a researcher wants about one-third of the sample treated then a bernoulli trial with the probability of success of 0.33 can be used.↩︎ "],["a-simple-example.html", "6.1 A simple example", " 6.1 A simple example Say, you are interested in evaluating the effect of tutoring program initiated following the first exam on grades at an introductory level course. For simplicity, the possible grades are A and B. However, students who received B on their first exam are more likely to attend the tutoring session. In other words, \\(P(W_i = 1 | Y_{iFE} = A) &lt; P(W_i = 1 | Y_{iFE} = B)\\) (\\(Y_{iFE}\\) is read as unit \\(i&#39;s\\) grade in the first exam). In this case, the treatment assignment is correlated with the past grade, which can predict the grade on the second exam. In other words, if you did well in the first exam, you are likely to perform well in the second exam and so on. Hence, using equation (2) to estimate effects of the tutoring program will result in biased estimate. Since we know that the probability of treatment is influenced by the grade on the first exam, we can estimate the conditional average treatment effect (CATE) and average them using weights to form an estimate of ATE. Let’s take a look at the data. # function to report grade breakdown by the first exam grade (A and B) grade_mat &lt;- function(grade_vec){ grade &lt;- matrix(0, nrow =2, ncol = 3) grade[, 1] &lt;- c(&quot;Treat&quot;, &quot;Control&quot;) grade[ ,2] &lt;- c(grade_vec[1], grade_vec[2]) grade[ ,3] &lt;- c(grade_vec[3], grade_vec[4]) return(grade) } # Y_iFS == A grade &lt;- grade_mat(c(5, 9, 2, 4)) colnames(grade) &lt;- c(&quot; &quot;, &quot;A (2nd Exam)&quot;, &quot;B (2nd Exam)&quot;) # Se grade %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) %&gt;% add_header_above(c(&quot;Table 1.&quot; = 1, &quot;Grade in the 2nd exam | 1st exam = A&quot; = 2)) Table 1. Grade in the 2nd exam | 1st exam = A A (2nd Exam) B (2nd Exam) Treat 5 2 Control 9 4 grade &lt;- grade_mat(c(15, 1, 5, 4)) colnames(grade) &lt;- c(&quot; &quot;, &quot;A (2nd Exam)&quot;, &quot;B (2nd Exam)&quot;) grade %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) %&gt;% add_header_above(c(&quot;Table 2.&quot; = 1, &quot;Grade in the 2nd exam | 1st exam = B&quot; = 2)) Table 2. Grade in the 2nd exam | 1st exam = B A (2nd Exam) B (2nd Exam) Treat 15 5 Control 1 4 \\(~\\) \\(~\\) Estimation \\(\\hat{\\tau}_{FE=A} = \\frac{5}{7} - \\frac{9}{13} = 2.1 \\; pp\\) \\(\\hat{\\tau}_{FE=B} = \\frac{15}{20} - \\frac{1}{5} = 55 \\; pp\\) \\(\\hat{\\tau}_{AGG} = \\frac{20}{45} \\hat{\\tau}_{FE=A} - \\frac{25}{45} \\hat{\\tau}_{FE=B} = 31.48 \\; pp\\). The first two are CATEs for the group that recived A and B in the first exam. The assumption is that once conditioned on the grade in the first exam, treatment (who attends vs. who doesn’t) is random. This allows valid estimation of within group causal effects, which are then averaged to form ATE using appropriate weights on the third line. This simple example using the discrete feature space (grade in the first exam can be A or B) provides intuition that if variables influencing the treatment assignment are observed then ATE estimate can be uncovered by taking weighted average of CATE estimates (these are also group-wise ATE).5 In this case, CATEs are different across the two sub-groups. Sometimes the core interest of analysis can be uncovering the heterogeneous treatment effects, which motivates estimation and inference on CATEs across two or more sub-groups.↩︎ "],["aggregated-estimator.html", "6.2 Aggregated Estimator", " 6.2 Aggregated Estimator As we saw in Section 1, aggregated estimator can be used in when we know what exact variables are determining the treatment assignment. We will consider the discrete case, when a discrete covariate determines the probability of treatment for simplicity. Note that students who received B on their first exam are more likely to attend the tutoring session in our simple example. So grade in the first exam (A or B for simplicity) determines the treatment here. Setup. \\(Y_i\\) is the outcome variable; final exam score. \\(W_i\\) is the treatment variable; whether a student attended the tutoring session. \\(X_i\\) is the covariate; a student’s grade on the first exam. We want to evaluate the effects of attending a tutoring program on a student’s exam score. What we would want to do is to condition on the first exam’s grade, estimate the treatment effects, and aggregate it. The aggregated estimator is given as: \\[\\begin{align} \\hat{\\tau}_{AGG} = \\frac{n_{A}}{n} \\bigg[\\frac{1}{n_{A1}} \\underbrace{\\sum}_{\\substack{i \\in A \\\\ \\textrm{W = 1}}} Y_i - \\frac{1}{n_{A0}} \\underbrace{\\sum}_{\\substack{i \\in A \\\\ \\textrm{W = 0}}} Y_i \\bigg] + \\frac{n_{B}}{n} \\bigg[\\frac{1}{n_{B1}} \\underbrace{\\sum}_{\\substack{i \\in B \\\\ \\textrm{W = 1}}} Y_i - \\frac{1}{n_{B0}} \\underbrace{\\sum}_{\\substack{i \\in B \\\\ \\textrm{W = 0}}} Y_i \\bigg] \\tag{6.4} \\end{align}\\] Note that the first block is the treatment effect for those who received A on their first exam, whereas the second block represents treatment effect for those who received B. After a few steps of simple algebra, equation @ref{eq:AGG0} can be written as: \\[\\begin{equation} = \\frac{1}{n} \\bigg[ \\frac{1}{\\frac{n_{A1}}{n_{A}}} \\underbrace{\\sum}_{\\substack{i \\in A}} Y_i \\times W_i - \\frac{1}{\\frac{n_{A0}}{n_{A}}} \\underbrace{\\sum}_{\\substack{i \\in A}} Y_i \\times (1 - W_i) \\bigg] + \\\\ \\frac{1}{n} \\bigg[ \\frac{1}{\\frac{n_{B1}}{n_{B}}} \\underbrace{\\sum}_{\\substack{i \\in B}} Y_i \\times W_i - \\frac{1}{\\frac{n_{B0}}{n_{B}}} \\underbrace{\\sum}_{\\substack{i \\in B}} Y_i \\times (1 - W_i) \\bigg] \\tag{6.5} \\end{equation}\\] Equation (6.5) looks super complicated, buts its not. Let’s breakdown the components of it: \\(\\frac{n_{A1}}{n_A}:\\) Represents the fraction of treated individuals who received \\(A\\) on the first exam. \\(\\frac{n_{A0}}{n_A}:\\) Represents the fraction of untreated individuals who received \\(A\\) on the first exam. \\(\\frac{n_{B1}}{n_B}:\\) Represents the fraction of treated individuals who received \\(B\\) on the first exam. \\(\\frac{n_{B0}}{n_B}:\\) Represents the fraction of untreated individuals who received \\(B\\) on the first exam. Note that \\(\\frac{n_{A1}}{n_A} = e(X = A)\\) and \\(\\frac{n_{A0}}{n_A} = 1 - e(X = A)\\) and the same goes with the segment composed of those who received B on their first exam. Equation (6.5) can be further simplified as: \\[\\begin{equation} \\hat{\\tau}_{AGG} = \\frac{1}{n}\\bigg[ \\sum \\frac{Y_i \\times W_i}{e(X)} - \\sum \\frac{Y_i \\times (1-W_i)}{1 - e(X)} \\bigg] \\tag{6.6} \\end{equation}\\] Equation @ref{eq:AGG2} takes the form of an inverse probability-weighted estimator. We’ll find out that Aggregate Estimator \\((\\hat{\\tau}_{AGG})\\) is a special case of Inverse Probability Weighting later on in this section. n &lt;- 2000 p &lt;- 10 true_effect &lt;- 10 set.seed &lt;- 12570 agg.means &lt;- replicate(1000, { X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) prob &lt;- 1 / (1 + exp(- (X[, 1] &lt; 1 + rnorm(n)))) W &lt;- rbinom(n, 1, prob) Y &lt;- 60 + true_effect * W + 5 * pmax(X[, 1], 1) + rnorm(n) group &lt;- ifelse( X[, 1] &lt; 1, &quot;A&quot;, &quot;B&quot; ) # estimates for those receiving A and B on the first exam att.A &lt;- mean(Y[group == &quot;A&quot; &amp; W == 1]) - mean(Y[group == &quot;A&quot; &amp; W == 0]) att.B &lt;- mean(Y[group == &quot;B&quot; &amp; W == 1]) - mean(Y[group == &quot;B&quot; &amp; W == 0]) prop.A &lt;- mean(group == &quot;A&quot;) prop.B &lt;- mean(group == &quot;B&quot;) prop.A * att.A + prop.B * att.B } ) # plot(X[, 1], X[, 2], col = as.factor(W)) hist(agg.means, freq = F, main = &quot;&quot;, col= rgb(0, 0, 1, 1/8), xlab = &quot;ATT estimates&quot;, las = 1) abline(v = true_effect, lwd = 3, lty = 2) legend(&quot;topright&quot;, &quot;True effect&quot;, lwd = 3, lty = 2, bty = &quot;n&quot;) paste(&quot;mean of AGG estimates: &quot;, round(mean(agg.means), 3)) ## [1] &quot;mean of AGG estimates: 9.959&quot; paste(&quot;standard error of AGG estimates: &quot;, round(sd(agg.means), 3)) ## [1] &quot;standard error of AGG estimates: 0.063&quot; Let’s compare this to a naive estimator. set.seed &lt;- 12570 naive.means &lt;- replicate(1000, { X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) prob &lt;- 1 / (1 + exp(- (X[, 1] &lt; 1 + rnorm(n)))) W &lt;- rbinom(n, 1, prob) Y &lt;- 60 + true_effect * W + 5 * pmax(X[, 1], 1) + rnorm(n) # naive estimates that does not take voluntary selection into account mean(Y[ W == 1]) - mean(Y[ W == 0]) } ) # plot(X[, 1], X[, 2], col = as.factor(W)) hist(naive.means, freq = F, main = &quot;&quot;, col= rgb(0, 0, 1, 1/8), xlab = &quot;ATT naive estimates&quot;, las = 1) abline(v = true_effect, lwd = 3, lty = 2) legend(&quot;topright&quot;, &quot;True effect&quot;, lwd = 3, lty = 2, bty = &quot;n&quot;) paste(&quot;mean of naive estimates: &quot;, round(mean(naive.means), 3)) ## [1] &quot;mean of naive estimates: 9.762&quot; paste(&quot;standard error of naive estimates: &quot;, round(sd(naive.means), 3)) ## [1] &quot;standard error of naive estimates: 0.083&quot; "],["propensity-score.html", "6.3 Propensity score", " 6.3 Propensity score Previously we discussed the setting of a discrete feature in which case we estimate group-wise ATEs and use the weighted average to obtain an overall ATE estimate. When there are many features (covariates), this approach is prone to the curse of dimensionality.6 Moreover, if features are continuous, we won’t be able to estimate ATE at each value of \\(x \\in \\chi\\) due to lack of enough sample size. Instead of estimating group-wise ATE and averaging them, we would want to use a more indirect approach. This is when propensity score comes in. The implicit assumption is that we have collected enough features (discrete, continuous, interaction terms, higher degree polynomials) to back unconfoundedness. This again means that the treatment assignment is as good as random after controlling for \\(X_i\\). More formally, this us back to equation (6.3). But in actuality we are not interested in splitting groups to estimate group-wise treatment effects in the case when covariates are continuous and there are many characteristics determining the treatment assignment. Propensity score: \\(e(x)\\). The probability of being treated given a set of covariates \\(X\\)s. \\[\\begin{equation} e(x) = P(W_i = 1 | X_i = x) \\tag{6.7} \\end{equation}\\] The key property of the propensity score is that it balances units in the treatment and control groups. If unconfoundedness assumption holds, we can write the following: \\[\\begin{equation} W_i \\perp \\{Y_i(0), \\; Y_i(1)\\} | \\; e(X_i) \\tag{6.8} \\end{equation}\\] What equation (6.8) says is that instead of controlling for \\(X\\) one can control for the probability of treatment \\((e(X))\\) to establish the desired property that the treatment is as good as random. The propensity scores are mainly used for balancing purposes. One straight-forward implication of equation (6.8) is that if we partition observations into groups with similar propensity score then we can estimate group-wise treatment effects and aggregate them to form an estimate for ATE. This can be done using the propensity score stratification method. The argument here is that when units with similar propensity scores are compared, the covariates are approximately balanced, mimicking a randomized experiment. As the number of covariates increases the domain space shrinks quite rapidly making it infeasible to estimate ATE within the given domain due to thinning out data.↩︎ "],["estimation-of-propensity-score.html", "6.4 Estimation of propensity score", " 6.4 Estimation of propensity score Propensity scores can be estimated using various statistical or machine learning models. We will first estimate propensity score using a logistic regression model, where the treatment assignment \\(W\\) is regressed on the covariates \\(X\\). Next, we will estimate propensity score using random forest model built within the GRF framework in Athey et al.  Logistic Regression Using a linear regression framework to predict probabilities when the outcome is binary \\(\\{0, \\; 1\\}\\) falls short since the predicted values can go beyond 0 and 1. Many models contain values within the range of 0 and 1, which can be used to model a binary response. The logistic regression uses a logistic function given as: \\[\\begin{equation} p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p}} \\tag{6.9} \\end{equation}\\] It is easy to see that \\(lim_{a \\rightarrow - \\inf}[\\frac{e^a}{1+e^a}] = 0\\) and \\(lim_{a \\rightarrow \\inf}[\\frac{e^a}{1+e^a}] = 1\\). Equation @ref{eq:logit} can be transformed using the logit transformation given as: \\[\\begin{equation} g(X) = ln[\\frac{p(X)}{1-p(X)}] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + .... \\beta_p X_p \\end{equation}\\] We want to fit a logistic regression in order to predict the probability. For now, we will use simulated data. # helper packages library(dplyr) # data wrangling library(ggplot2) # plots library(rsample) # data splitting ## ## Attaching package: &#39;rsample&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## calibration library(tidyr) # for reshaping, pivot_wider # Modeling package library(caret) # for logistic regression modeling # Model interpretability library(vip) ## ## Attaching package: &#39;vip&#39; ## The following object is masked from &#39;package:utils&#39;: ## ## vi set.seed(194) # for replicability # Generate simulated Data n &lt;- 2000 # number of obsevations p &lt;- 10 # number of covariates X &lt;- matrix(rnorm(n * p), n, p) # data matrix true_effect &lt;- 2.5 W &lt;- rbinom(n, 1, 0.1 + 0.4 * (X[, 1] &gt; 0) + 0.2 * (X[, 2] &gt; 0)) prob &lt;- 0.1 + 0.4 * (X[, 1] &gt; 0) + 0.2 * (X[, 2] &gt; 0) # oracle propensity score Y &lt;- true_effect * W + X[, 2] + pmax(X[, 1], 0) + rnorm(n) #plot(X[, 1], X[, 2], col = as.factor(W)) dat &lt;- data.frame(cbind(W, Y, X)) colnames(dat) &lt;- c(&quot;W&quot;, &quot;Y&quot;, paste0(&quot;X&quot;, seq(1, 10))) dat &lt;- dat %&gt;% mutate(W = as.factor(W)) # create 70% training and 30% test data churn_split &lt;- initial_split(dat, prop = 0.7) dat_train &lt;- training(churn_split) dat_test &lt;- testing(churn_split) # dimension of training and testing data print(dim(dat_train)) ## [1] 1400 12 print(dim(dat_test)) ## [1] 600 12 # let&#39;s compare two different models # using X1 as the predictor cv_model1 &lt;- train( W ~ X1 + X2 + X3 + X4, data = dat_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # misses out on X1 cv_model2 &lt;- train( W ~ X2 + X3 + X4, data = dat_train, method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) # print the sample performance measures sum_performance &lt;- summary( resamples( list( model1 &lt;- cv_model1, model2 &lt;- cv_model2 ) ) ) sum_performance$statistics$Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Model1 0.6043165 0.6446429 0.6678571 0.6706292 0.6964286 0.7285714 0 ## Model2 0.5642857 0.5785714 0.5892392 0.5935621 0.6160714 0.6285714 0 # use the confusion matrix # predict class threshold &lt;- 0.5 pred_prob &lt;- predict(cv_model1, dat_train, type = &quot;prob&quot;) pred_class_manual &lt;- rep(0, 1400) pred_class_manual[pred_prob[, 2] &gt;= 0.5] &lt;- 1 pred_class &lt;- predict(cv_model1, dat_train) # print the confusion matrix confusionMatrix( data = relevel(pred_class, ref = &quot;1&quot;), # predictions reference = relevel(dat_train$W, ref = &quot;1&quot;) # reference or the true value ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 270 151 ## 0 299 680 ## ## Accuracy : 0.6786 ## 95% CI : (0.6534, 0.703) ## No Information Rate : 0.5936 ## P-Value [Acc &gt; NIR] : 3.164e-11 ## ## Kappa : 0.3053 ## ## Mcnemar&#39;s Test P-Value : 4.219e-12 ## ## Sensitivity : 0.4745 ## Specificity : 0.8183 ## Pos Pred Value : 0.6413 ## Neg Pred Value : 0.6946 ## Prevalence : 0.4064 ## Detection Rate : 0.1929 ## Detection Prevalence : 0.3007 ## Balanced Accuracy : 0.6464 ## ## &#39;Positive&#39; Class : 1 ## # if predict all yes still get an accuracy of 0.5936 table(dat_train$W) %&gt;% prop.table() ## ## 0 1 ## 0.5935714 0.4064286 Looking at the confusion matrix, the values on the downward diagonal ([1, 1] and [2, 2] in matrix) are correctly idenfified by the model, while the upward diagonal values ([2, 1] and [1, 2]) are incorrectly classified. If all of the observations were assigned the value of 0, the accuracy would still be 0.5936%. This is termed as the no information rate. The model performs quite well in predicting True Negatives (classify as 0, when the value is actually 0). However, it does not perform so well in classifying the True Positives – more than 50% of the positive cases are classified as negative. Next, two measures of importance are sensitivity and specificity. The sensitivity measure tracks the true positive rate from the model, while the specificity measure tracks the true negative rate. \\(sensitivity = \\frac{True \\; positives}{True \\; positives + False \\; negatives} = 0.8790\\). \\(specificity = \\frac{True \\; negatives}{True \\; negatives \\; + \\; False \\; positives} = \\frac{604}{604 + 85} = 0.8766\\). How are the observations classified? A threshold value is used to transform the raw prediction of probabilities into classification such that \\(P(Y_{i} &gt; p_{threshold})=1.\\) The implicit \\(p_{threshold}\\) used is 0.5. Varying the threshold from 0 to 1, one can calculate the relationship between the False Positive Rate (the prediction is positive when in actual the outcome is negative) and True Positive Rate at each threshold value. If the threshold value \\((p_{threshold})\\) is 1, then all observations are classified as 0, which means that the False Positive Rate is 0 but so is the True Positive Rate. Similarly, if the threshold is 0, then both True and False positive rates are 1. This gives the Receiver Operating Characteristic (ROC). library(ROCR) # compute probabilities m1_prob &lt;- predict(cv_model1, dat_train, type = &quot;prob&quot;)[, 2] m2_prob &lt;- predict(cv_model2, dat_train, type = &quot;prob&quot;)[, 2] # AUC metrics perf1 &lt;- prediction(m1_prob, dat_train$W) %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) perf2 &lt;- prediction(m2_prob, dat_train$W) %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) # plot ROC curves plot(perf1, col = &quot;red&quot;) plot(perf2, add = TRUE, col = &quot;green&quot;) legend(0.8, 0.2, legend = c(&quot;cv_model1&quot;, &quot;cv_model2&quot;), lty = c(1,1), col = c(&quot;red&quot;, &quot;green&quot;), cex = 0.6) Figure 6.1: ROC The figure above plots the ROC for two models that we tested using cross-validation. The cv_model2 produces a diagonal line, which means that this model is as good as a random guess. Next, cv_model1 performs a whole lot better since a large gains in True positive rate can be achieved with a relatively small increase in False positive rate at the start. The ROC curve pertaining to cv_model1 helps pick a threshold to balance the sensitivity (True Positive Rate) and specificity (1 - False Positive Rate). The histogram of the estimated propensity scores using the logistic regression is as: hist(pred_prob[, 2], main = &quot;Histogram of P(W=1|X) \\n using Logistic Regression&quot;, xlab = &quot;probabilities&quot;) Now, let’s take a look at the confusion matrix using the test data. pred_class_test &lt;- predict(cv_model1, dat_test) # print the confusion matrix this time for the test sample confusionMatrix( data = relevel(pred_class_test, ref = &quot;1&quot;), # classification from the prediction reference = relevel(dat_test$W, ref = &quot;1&quot;) # ground truth ) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 94 69 ## 0 131 306 ## ## Accuracy : 0.6667 ## 95% CI : (0.6274, 0.7043) ## No Information Rate : 0.625 ## P-Value [Acc &gt; NIR] : 0.01882 ## ## Kappa : 0.2474 ## ## Mcnemar&#39;s Test P-Value : 1.608e-05 ## ## Sensitivity : 0.4178 ## Specificity : 0.8160 ## Pos Pred Value : 0.5767 ## Neg Pred Value : 0.7002 ## Prevalence : 0.3750 ## Detection Rate : 0.1567 ## Detection Prevalence : 0.2717 ## Balanced Accuracy : 0.6169 ## ## &#39;Positive&#39; Class : 1 ## The measures of accuracy, sensitivity, and specificity are similar for both the training and testing sample. "],["using-cross-fitting-to-predict-propensity-score.html", "6.5 Using cross-fitting to predict propensity score", " 6.5 Using cross-fitting to predict propensity score Here, we will be using 10-fold cross-folding to predict propensity score. fun_probit_predict &lt;- function(predictfold){ # @Arg predictfold: number of the fold to avoid for model traning # but used for prediction cv_model1 &lt;- train( W ~ X1 + X2 + X3 + X4, data = dat[-predictfold, ], method = &quot;glm&quot;, family = &quot;binomial&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10) ) predict_logit &lt;- predict(cv_model1, dat[predictfold, ], type = &quot;prob&quot;) return(predict_logit[, 2]) } ############################## # # cross-fitting # ############################## k &lt;- 10 # number of folds len &lt;- nrow(dat) ind &lt;- sample(1:len, replace = FALSE, size = len) fold &lt;- cut(1:len, breaks = k, labels = FALSE) # create 10 folds fold &lt;- fold[ind] # randomly allocate the folds by ind # container to store the predicted values store &lt;- c() true_index &lt;- c() # do the cross-fitting and store for(i in 1:k){ # which(fold == i) is used as an index, if 8th observation receives the 1st fold for the first time, # then the 1st prediction value corresponds to the 8th obs store_new &lt;- fun_probit_predict(predictfold = which(fold == i)) store_new &lt;- as.numeric(as.character(store_new)) true_index_new &lt;- which(fold == i) store &lt;- c(store, store_new) true_index &lt;- c(true_index, true_index_new) } # create a dataframe with index that maps the predictions with the actual data store &lt;- data.frame(pscore = store, index = true_index) # sort by index store &lt;- store[order(store[, 2]), ] # propensity score dat &lt;- dat %&gt;% mutate(pscore = store$pscore) # histogram of propensity score hist(dat$pscore, main = &quot;propensity score \\n from cross-fitting&quot;) "],["propensity-score-stratification.html", "6.6 Propensity score stratification", " 6.6 Propensity score stratification Propensity scores are super important as they can be used in various different approaches to enchance the validity of causal inference in observational settings. These include but are not limited to inverse probability weighting, matching estimates, weight adjustments in regression (for better balancing), trimming, and propensity score stratification. These methods will be discussed in detail as we move on with the course. First, let’s take a look at propensity score stratification to get a gist of how propensity scores contribute in comparing treatment units with control units. The simple idea is given by the cliché that we want to compare oranges with oranges and not apples. To bring focus back into our context, it simply means that it is no good comparing a treated unit with an extremely high probability of receiving the treatment with a control unit with super low probability of receiving the treatment. But what if (yes, what if) we compare units with similar treatment probabilities? Let’s run a quick thought experiment. We run the logistic regression and estimate the propensity score. Say, we have two units, each from the treatment and control group, with the propensity score of 0.6. The assumption here is, conditional on the similar propensity score, the treatment assignment is random. This follows from the unconfoundedness assumption: \\(Y_i^{0}, \\; Y_i^{1} \\; \\perp \\; W_i \\; | X_i\\). Propensity score stratification divides the estimates of propensity scores into several segments and estimates the ATE within each segment. Finally, these segment-specific ATE estimates are averaged to obtain the overall estimate of ATE. Steps for ATE estimation using propensity score stratification Order observations according to their estimated propensity score. \\(\\hat{e}(X)_{i1}, \\; \\hat{e}(X)_{i2}, ... \\; \\hat{e}(X)_{iN}\\) Form \\(J\\) strata of equal size and take the simple difference in mean between the treated and control units within each strata. These are \\(\\hat{\\tau}_j\\) for \\(j = \\{1, \\; 2, \\; ..., \\; N\\}\\). Form the ATE, \\(\\hat{\\tau}_{Strat} = \\frac{1}{J} \\sum_{j = 1}^{J} \\hat{\\tau}_j\\) Here, \\(\\hat{\\tau}_{Strat}\\) is consistent for \\(\\tau\\), meaning that \\(\\hat{\\tau}_{Strat} \\rightarrow_p \\tau\\) given that \\(\\hat{e}(x)\\) is consistent for \\(e(x)\\) and the number of strata grows appropriately with \\(N\\). However, one needs to set the number of strata, which can be a bit ad-hoc. Demo of propensity score stratification # order data by the propensity score: low to high dat &lt;- dat[order(dat$pscore), ] # cut to form ventiles strata &lt;- cut(dat$pscore, breaks = quantile(dat$pscore, seq(0, 1, 0.05)), labels = 1:20, include.lowest = TRUE) dat &lt;- dat %&gt;% mutate(strata = strata) # compare across strata dat_sum &lt;- dat %&gt;% group_by(W, strata) %&gt;% summarize(mean_Y = mean(Y)) %&gt;% pivot_wider(names_from = W, values_from = mean_Y) ## `summarise()` has grouped output by &#39;W&#39;. You can override using the `.groups` argument. colnames(dat_sum) &lt;- c(&quot;strata&quot;, &quot;mean_control&quot;, &quot;mean_treat&quot;) dat_sum &lt;- dat_sum %&gt;% mutate(diff = mean_treat - mean_control) print(paste(&quot;ATE Estimation from propensity score stratification is: &quot;, mean(dat_sum$diff), sep = &quot;&quot;)) ## [1] &quot;ATE Estimation from propensity score stratification is: 2.50135397640408&quot; print(paste(&quot;raw difference is :&quot;, mean(dat$Y[dat$W == 1]) - mean(dat$Y[dat$W == 0]), sep = &quot;&quot;)) ## [1] &quot;raw difference is :3.04098842187519&quot; print(paste(&quot;And the true treatment effect is :&quot;, true_effect, sep = &quot;&quot;)) ## [1] &quot;And the true treatment effect is :2.5&quot; We see that the estimate from stratification gets closer to the true effect compared to the mean difference estimator. Looks like given that we know and observe what variables determine the treatment assignment, propensity score stratification approach performs well in estimating the ATE. "],["inverse-probability-weighting-ipw.html", "6.7 Inverse Probability Weighting (IPW)", " 6.7 Inverse Probability Weighting (IPW) A more natural way to exploit the condition of unconfoundedness is to weight observations by their propensity score, which is known as the inverse probability weighting. As before \\(\\hat{e}(x)\\) is defined as an estimated propensity score. \\[\\begin{equation} \\hat{\\tau}_{IPW} = \\frac{1}{N}\\sum_{i = 1}^{N} \\Bigg(\\frac{Y_i . W_i}{\\hat{e}(X_i)} - \\frac{Y_i . (1-W_i)}{1 - \\hat{e}(X_i)}\\Bigg) \\tag{6.10} \\end{equation}\\] Intuitively, observations with high propensity score within the treated group are weighted down, while observations with higher propensity score in the control group are weighted more. In this way, propensity score is used to balance the differences in covariates across the treatment and control groups. Note that the validity of \\(\\hat{\\tau}\\) still hinges on the unconfoundedness assumption. Any inference that you make is only good if your assumption holds. Limitation of IPW Estimate. One way to analyze the accuracy of \\(\\hat{\\tau}_{IPW}\\) is to compare it with the oracle IPW estimate, \\(\\hat{\\tau}_{IPW}^{*}\\). The oracle estimate is obtained from the known propensity score. Briefly, comparison between \\(\\hat{\\tau}_{IPW}^{*}\\) and \\(\\hat{\\tau}_{AGG}\\) suggests that the oracle IPW under-performs \\(\\hat{\\tau}_{AGG}\\). In other words, the variance of the oracle estimate is larger than that of \\(\\hat{\\tau}_{AGG}\\). Algorithmically, we can form score as: \\((\\frac{Y_i \\times W_i}{\\hat{e}(X_i)} - \\frac{Y_i \\times (1-W_i)}{1 - \\hat{e}(X_i)})\\) The mean of it results to \\(\\hat{\\tau}\\) and the standard error of the estimate is simply \\(\\frac{\\hat{\\sigma}_{score}}{\\sqrt{N}}\\). Estimating IPW. In the example below we will simulate a dataset where the treatment assignment is made to be correlated with the outcome. This means that the independence assumption does not hold. However, since this is a simulated data, we know exactly what covariates influence the treatment assignment. Hence, we can invoke the unconfoundedness assumption. We estimate the propensity score using random forest based on honest splitting. For this, we use GRF package from . Note that \\(e(x)\\) is estimated via cross-fitting. The data is divided into \\(K\\)-folds. For each fold \\(k\\), model building is administered using \\(-k\\) folds. Using Step 2, predictions are generated for units in the \\(k^{th}\\) fold. Steps 2 and 3 are repeated until all \\(K\\) folds are exhausted. Estimation The following example uses 10 fold cross-fitting. ################################# # Author: VS # Last Revised: Jan 16, 2024 # Keywords: IPW, AIPW, GRF # # # ################################# set.seed(194) # Generate Data n &lt;- 2000 p &lt;- 10 true_effect &lt;- 15 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) prob &lt;- 1 / (1 + exp(- (X[, 1] &gt; 1 + rnorm(n)))) W &lt;- rbinom(n, 1, prob) Y &lt;- 60 + true_effect * W + 5 * pmax(X[, 1], 0) + rnorm(n) plot(X[, 1], X[, 2], col = as.factor(W)) #paste0(&quot;average treatment effect is: &quot;, round(mean(pmax(X[, 1], 0)), 3)) ################################# ################################# # # Inverse Probability Weighting # ################################# ################################# # use the random forest to get the propensity score dat &lt;- data.frame(W, X, Y) n_features &lt;- length(setdiff(names(dat), &quot;W&quot;)) # A. ranger (probability tree) rf1_ranger &lt;- ranger( W ~ ., data = dat, mtry = min(ceiling(sqrt(n_features) + 20), n_features), num.trees = 2000, probability = TRUE ) # OOB predictions from ranger p.ranger &lt;- rf1_ranger$predictions[, 1] # B. probability tree using GRF # cross-fitting index K &lt;- 10 ind &lt;- sample(1:length(W), replace = FALSE, size = length(W)) folds &lt;- cut(1:length(W), breaks = K, labels = FALSE) index &lt;- matrix(0, nrow = length(ind) / K, ncol = K) for(f in 1:K){ index[, f] &lt;- ind[which(folds == f)] } # Build RF using GRF P(W = 1 | X) fun.rf.grf &lt;- function(X, W, predictkfold){ rf_grf &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) p.grf &lt;- predict(rf_grf, predictkfold)$predictions return(p.grf) } # storing predict.mat &lt;- matrix(0, nrow = nrow(index), ncol = K) tauk &lt;- rep(0, K) tauk_oracle &lt;- rep(0, K) weighttau &lt;- rep(0, K) score &lt;- list() score_oracle &lt;- list() # for each fold i use other folds for estimation for(i in seq(1:K)){ predict.mat[, i] &lt;- fun.rf.grf(X = X[c(index[, -i]), ], W = W[index[, -i]], predictkfold = X[c(index[, i]), ]) # fold-specific treatment effect score[[i]] &lt;- ((W[index[, i]] * Y[index[, i]]) / (predict.mat[, i])) - (((1 - W[index[, i]]) * Y[index[, i]]) / (1 - predict.mat[, i])) tauk[i] &lt;- mean(score[[i]]) } # ipw using oracle propensity score and propensity score estimated from grf alpha &lt;- 0.05 # 5 percent level of significance #ipw.ranger &lt;- mean(((W * Y) / (p.ranger)) - (((1 - W) * Y) / (1 - p.ranger))) ipw.grf &lt;- mean(unlist(score)) score_oracle &lt;- ((W * Y) / (prob)) - ((1 - W) * Y / (1 - prob)) ipw.oracle &lt;- mean(score_oracle) sd.ipw &lt;- sd(unlist(score)) sd.oracle &lt;- sd(score_oracle) ll &lt;- ipw.grf - (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ul &lt;- ipw.grf + (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ll_oracle &lt;- ipw.oracle - (sd.oracle / sqrt(length(score_oracle))) * qnorm(1 - alpha/2) ul_oracle &lt;- ipw.oracle + (sd.oracle / sqrt(length(score_oracle))) * qnorm(1 - alpha/2) result.ipw &lt;- c(&quot;IPW estimate&quot; = round(ipw.grf, 3), &quot;se&quot; = round(sd.ipw / (sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll, 3), &quot;upper bound&quot; = round(ul, 3)) result.oracle.ipw &lt;- c(&quot;IPW Oracle estimate&quot; = round(ipw.oracle, 3), &quot;se&quot; = round(sd.oracle / (sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll_oracle, 3), &quot;upper bound&quot; = round(ul_oracle, 3)) print(result.ipw) ## IPW estimate se lower bound upper bound ## 15.508 3.092 9.448 21.568 print(result.oracle.ipw) ## IPW Oracle estimate se lower bound upper bound ## 16.496 3.165 10.294 22.699 What? Despite having true propensity score, the Oracle IPW underperforms in accuracy compared to the IPW estimate with unknown propensity score. Why is it so? "],["comparing-ipw-with-aggregated-estimate.html", "6.8 Comparing IPW with Aggregated Estimate", " 6.8 Comparing IPW with Aggregated Estimate We know that the probablity of the treatment increases with \\(X1\\), as shown below. plot(X[, 1], prob) par(new = T) abline(v = 0, col = &quot;red&quot;, lty = &quot;dashed&quot;) Next, we would want to divide \\(X1\\) into segments such that the probability of treatment remains more or less similar in each segment. Since we generated the data ourselves, we know that the probability of treatment increases for observations with \\(X1 &gt; 0\\). However, lets take 10 segments, which cuts the distribution of \\(X1\\) in decile. quant &lt;- quantile(X[, 1], p = seq(0, 1, 0.1)) group &lt;- cut(X[, 1], quant, include.lowest = TRUE) dat$group &lt;- group Next, we want to estimate the treatment effects for each segment and then aggregate it. dat_sum &lt;- data.frame(dat %&gt;% mutate(status = ifelse(W == 1, &quot;treatment&quot;, &quot;control&quot;)) %&gt;% group_by(group) %&gt;% summarize(num_treat = sum(W), n_group = n())) %&gt;% mutate(prop_treat = num_treat / n_group) %&gt;% dplyr::select(c(group, prop_treat)) dat &lt;- dat %&gt;% merge(dat_sum, by = &quot;group&quot;, all.x = T) %&gt;% mutate(score = Y * ((W / prop_treat) - ((1 - W) / (1 - prop_treat))) ) paste(&quot;aggregated treatment effect is: &quot;, round(mean(dat$score), 3)) ## [1] &quot;aggregated treatment effect is: 15.019&quot; paste(&quot;se of aggregated treatment effect is: &quot;, round(sd(dat$score) / sqrt(length(W)), 3)) ## [1] &quot;se of aggregated treatment effect is: 3.122&quot; Now, let’s estimate the oracle IPW with known propensity score. score_oracle &lt;- ((W * Y) / (prob)) - ((1 - W) * Y / (1 - prob)) ipw.oracle &lt;- mean(score_oracle) se.oracle &lt;- sd(score_oracle) / sqrt(length(score_oracle)) paste(&quot;oracle IPW estimate: &quot;, round(ipw.oracle, 3)) ## [1] &quot;oracle IPW estimate: 16.496&quot; paste(&quot;standard error: &quot;, round(se.oracle, 3)) ## [1] &quot;standard error: 3.165&quot; "],["aipw-and-estimation.html", "6.9 AIPW and Estimation", " 6.9 AIPW and Estimation Augmented Inverse Probability Weighting (AIPW) provides a robust way to estimate ATE by alleviating the limitation of IPW estimate. Following the IPW approach, estimation of ATE is given in equation (6). The other approach to estimate \\(\\tau\\) is to think of it from the conditional response approach. Write \\(\\mu_{w}(x) = E[Y_i| \\; X_i = x, W_i = w]\\). Then: \\(\\tau(x) = E[Y_i| \\; X_i = x, W_i = 1] - E[Y_i| \\; X_i = x, W_i = 0]\\) This is the regression outcome approach, where \\(\\tau = E[\\mu_{1}(x) - \\mu_{0}(x)]\\). The consistent estimator can be formed by using: \\(\\hat{\\tau}(x) = N^{-1} \\sum_{i = 1}^{N} \\mu_{1}(X_i) - \\mu_{0}(X_i)\\). AIPW approach combines both IPW approach as well as regression outcome approach to estimate \\(\\tau\\). \\(\\hat{\\tau}_{AIPW} = \\frac{1}{N} \\sum_{i = 1}^{N} (\\mu_{1}(X_i) - \\mu_{0}(X_i) + \\frac{(Y_i - \\hat{\\mu}_1(X_i)). W_i}{\\hat{e}(X_i)} - \\frac{(Y_i - \\hat{\\mu}_0(X_i)). (1-W_i)}{1 - \\hat{e}(X_i)})\\) ML approach using cross-fitting is used to estimate both \\(\\hat{e}(x)\\) and \\(\\hat{\\mu}_{w}(x)\\). Following the cross-fitting structure, we can formally write the estimate for \\(\\tau\\) as: \\(\\hat{\\tau}_{AIPW} = \\underbrace{\\frac{1}{N} \\sum_{i = 1}^{N} (\\mu_{1}^{-k(i)}(X_i) - \\mu_{0}^{-k(i)}(X_i)}_{consistent \\; estimate \\; of \\; \\tau} + \\frac{(Y_i - \\hat{\\mu}_1^{-k(i)}(X_i)). W_i}{\\hat{e}^{-k(i)}(X_i)} - \\frac{(Y_i - \\hat{\\mu}_0^{-k(i)}(X_i)). (1-W_i)}{1 - \\hat{e}^{-k(i)}(X_i)})\\) The AIPW approach can be thought of estimating ATE taking the difference across conditional responses. Next, the residuals are adjusted using weights given by the propensity score. There are two attractive features of AIPW estimate. First, \\(\\hat{\\tau}_{AIPW}\\) is consistent as long as \\(\\hat{e}(x)\\) or \\(\\hat{\\mu}_{w}(x)\\) is consistent. This is because \\(E[(Y_i - \\hat{\\mu}_{W_i}(X_i)) \\approx 0\\). Second, \\(\\hat{\\tau}_{AIPW}\\) is a good approximation to oracle \\(\\hat{\\tau}_{AIPW}^{*}\\) as long as \\(\\hat{\\mu}(.)\\) and \\(\\hat{e}(.)\\) are reasonably accurate. If one estimate is highly accurate, then it can compensate lack of accuracy on the other estimate. If both \\(\\hat{\\mu}(.)\\) and \\(\\hat{e}(.)\\) are \\(\\sqrt{n}\\)-consistent7, then the following holds. \\(\\sqrt{n}(\\hat{\\tau}_{AIPW} - \\hat{\\tau}_{AIPW}^{*}) \\rightarrow_p 0\\). ####################### # # Augmented IPW (aipw) # ####################### #n_features2 &lt;- length(setdiff(names(dat2), &quot;Y&quot;)) # ranger #funrf_ranger &lt;- function(dat){ # rf2 &lt;- ranger( # Y ~ ., # data = dat, # mtry = min(ceiling(sqrt(n_features) + 20), n_features), # respect.unordered.factors = &quot;order&quot;, # seed = 123, # num.trees = 2000 # ) # return(rf2) #} # storing predict.mat2a &lt;- matrix(0, nrow = nrow(index), ncol = K) predict.mat2b &lt;- predict.mat2a aipwK &lt;- rep(0, K) weightaipK &lt;- rep(nrow(index) / length(index), K) for(i in seq(1:K)){ # E(Y | X, W = 1) using cross-fitting predict.mat2a[, i] &lt;- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], predictkfold = cbind(X[c(index[, i]), ], 1)) # E(Y | X, W = 0) using cross-fitting predict.mat2b[, i] &lt;- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], predictkfold = cbind(X[c(index[, i]), ], 0)) noise &lt;- ((W[index[, i]] * (Y[index[, i]] - predict.mat2a[, i])) / (predict.mat[, i])) - (((1 - W[index[, i]]) * (Y[index[, i]] - predict.mat2b)) / (1 - predict.mat[, i])) score[[i]] &lt;- predict.mat2a[, i] - predict.mat2b[, i] + noise aipwK[i] &lt;- mean(score[[i]]) } aipw.grf &lt;- weighted.mean(aipwK, weights = weightaipK) sd.aipw &lt;- sd(unlist(score)) ll &lt;- aipw.grf - (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) ul &lt;- aipw.grf + (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2) result.aipw &lt;- c(&quot;AIPW Est.&quot; = round(aipw.grf, 3), &quot;se&quot; = round(sd.aipw/(sqrt(length(W))), 3), &quot;lower bound&quot; = round(ll, 3), &quot;upper bound&quot; = round(ul, 3)) ###################### # grf ###################### # Train a causal forest tau.forest &lt;- causal_forest(X, Y, W) # Estimate the conditional average treatment effect on the full sample (CATE). grf_ate &lt;- average_treatment_effect(tau.forest, target.sample = &quot;all&quot;) grf_att &lt;- average_treatment_effect(tau.forest, target.sample = &quot;treated&quot;) ## PRINT ALL #print(paste0(&quot;average treatment effect is: &quot;, round(mean(pmax(X[, 1], 0)), 3))) print(paste(&quot;treatment effects according to naive estimator:&quot;, round(mean(Y[which(W == 1)]) - mean(Y[which( W == 0)]), 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to naive estimator: 15.558&quot; print(paste(&quot;treatment effects according to IPW using&quot;, K, &quot;fold cross-fittin:&quot;, round(ipw.grf, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to IPW using 10 fold cross-fittin: 15.508&quot; print(paste(&quot;treatment effects according to IPW oracle:&quot;, round(ipw.oracle, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to IPW oracle: 16.496&quot; print(paste(&quot;treatment effects according to AIPW using&quot;, K, &quot;fold cross-fitting:&quot;, round(aipw.grf, 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to AIPW using 10 fold cross-fitting: 15.037&quot; print(paste(&quot;treatment effects according to GRF:&quot;, round(grf_ate[[1]], 3), sep = &quot; &quot;)) ## [1] &quot;treatment effects according to GRF: 15.031&quot; print(result.ipw) ## IPW estimate se lower bound upper bound ## 15.508 3.092 9.448 21.568 print(result.aipw) ## AIPW Est. se lower bound upper bound ## 15.037 0.031 14.975 15.098 print(grf_ate) ## estimate std.err ## 15.03144837 0.05209014 This means that \\(\\hat{\\mu}(.)\\) converges to \\(\\hat{\\mu}\\) at the ↩︎ "],["assessing-balance.html", "6.10 Assessing Balance", " 6.10 Assessing Balance ########################################## # # # Assessing Balance # # ########################################## XX &lt;- X[c(index), ] YY &lt;- Y[c(index)] WW &lt;- W[c(index)] e.hat &lt;- c(predict.mat) # unadjusted means.treat &lt;- apply(XX[WW == 1, ], 2, mean) means.control &lt;- apply(XX[WW == 0, ], 2, mean) abs.mean.diff &lt;- abs(means.treat - means.control) var.treat &lt;- apply(XX[WW == 1, ], 2, var) var.control &lt;- apply(XX[WW == 0, ], 2, var) std &lt;- sqrt(var.treat + var.control) # adjusted means.treat.adj &lt;- apply(XX*WW / e.hat, 2, mean) means.control.adj &lt;- apply(XX*(1 - WW) / (1 - e.hat), 2, mean) abs.mean.diff.adj &lt;- abs(means.treat.adj - means.control.adj) var.treat.adj &lt;- apply(XX * WW / e.hat, 2, var) var.control.adj &lt;- apply(XX * (1 - WW) / (1 - e.hat), 2, var) std.adj &lt;- sqrt(var.treat.adj + var.control.adj) # plot unadjusted and adjusted differences par(oma=c(0,4,0,0)) plot(-2, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, xlim=c(-.01, 1.01), ylim=c(0, ncol(XX)+1), main=&quot;&quot;) axis(side=1, at=c(-1, 0, 1), las=1) lines(abs.mean.diff / std, seq(1, ncol(XX)), type=&quot;p&quot;, col=&quot;blue&quot;, pch=19) lines(abs.mean.diff.adj / std.adj, seq(1, ncol(XX)), type=&quot;p&quot;, col=&quot;orange&quot;, pch=19) legend(&quot;topright&quot;, c(&quot;Unadjusted&quot;, &quot;Adjusted&quot;), col=c(&quot;blue&quot;, &quot;orange&quot;), pch=19) abline(v = seq(0, 1, by=.25), lty = 2, col = &quot;grey&quot;, lwd=.5) abline(h = 1:ncol(XX), lty = 2, col = &quot;grey&quot;, lwd=.5) mtext(paste0(&quot;X&quot;, seq(1, ncol(XX))), side=2, cex=0.7, at=1:ncol(XX), padj=.4, adj=1, col=&quot;black&quot;, las=1, line=.3) abline(v = 0) hist(e.hat, breaks = 100, freq = FALSE) "],["cross-fitting.html", "6.11 Cross-fitting", " 6.11 Cross-fitting What is cross-fitting? Divide the data into K folds randomly. Train the model using \\(-k\\) folds (all folds except the \\(k^{th}\\) one). Generate a fit of fold k on the model trained using \\(-k\\) folds Repeat steps 2 and 3 to generate fit for all \\(K\\) number of folds. This is illustrated using the figure below. The data is randomly divided into 5 folds (segments). This is an example of a five-fold cross-fitting. In the first round, the blue segments are used for model building, while responses are constructed for observations in the green segment of the data. Next, we move into the second round and so on; again the blue segments are used for model building and responses are constructed for the green segment. In this way, each observation is used for model building. # cross-fitting illustration colorcode &lt;- diag(5) # this creates a coding colorcode &lt;- c(colorcode) # Create data for the boxes boxes &lt;- data.frame( x = rep(seq(2, 10, 2), 5), y = rep(seq(5, 1, by = -1), each = 5), label = rep(paste(&quot;fold&quot;, seq(1, 5), sep = &quot; &quot;), 5), colorcode = colorcode ) boxes &lt;- boxes %&gt;% mutate(fill = ifelse(colorcode == 1, &quot;lightgreen&quot;, &quot;lightblue&quot;)) %&gt;% dplyr::select(-c(colorcode)) # Create the plot ggplot() + geom_rect(data = boxes, aes(xmin = x , xmax = x + 2, ymin = y - 0.3, ymax = y + 0.5, fill = fill), color = &quot;black&quot;, alpha = 0.5) + xlim(0, 14) + ylim(-1, 6) + theme_void() + scale_fill_identity() + annotate(&quot;text&quot;, x = c(seq(3, 11, 2), rep(0.5, 5)), y = c(rep(0.3, 5), seq(5, 1, -1)), label = c(paste(&quot;fold&quot;, seq(1, 5, 1), sep = &quot; &quot;), paste(&quot;round&quot;, seq(1, 5, 1), sep = &quot; &quot;)), color = rep(c(&quot;red&quot;, &quot;black&quot;), each = 5) ) What does it do? Simply put, cross-fitting assures that the same observations are not used for modeling building as well as to estimate the response (e.g., predictions). In this way, we would want to alleviate concerns of over-fitting. "],["difference-in-differences.html", "7 Difference in Differences ", " 7 Difference in Differences "],["a-quick-introduction.html", "7.1 A Quick Introduction", " 7.1 A Quick Introduction Let’s consider that there are two groups: group \\(i)\\) receives the treatment and group \\(ii)\\) does not receive the treatment. We term group \\(i)\\) as the treatment group and group \\(ii)\\) as the control or untreated group. We want to compare the two groups in the pre-treatment period versus the post-treatment period. While doing so we will be utilizing both within and across variation in outcomes between the two groups. "],["set-up.html", "7.2 Set up", " 7.2 Set up For example, you observe units A and B in periods 1 and 2. Unit A is treated in period 2, whereas unit B does not receive treatment. The variation in treatment between periods 1 and 2 is the within variation in treatment. The comparison in treatment across units A and B is the across variation. Goal. We would want to compare the difference in average outcomes between two periods for unit A with the difference in average outcomes between the two periods pertaining to unit B. "],["an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html", "7.3 An example: Evaluating the impact of Medicaid expansion on uninsured rate", " 7.3 An example: Evaluating the impact of Medicaid expansion on uninsured rate Let’s take a concrete example. Say, we are interested in evaluating the impacts of ACA-Medicaid expansion on insurance outcomes. Following the supreme court decision in 2012 that deemed Medicaid expansion voluntary, 26 states expanded Medicaid in 2014, while the rest did not. In fact, 19 states did not expand Medicaid until 2018. Data for this example comes from one of my projects and can be downloaded from github. We are looking at the county-level data, where our outcome variable is the uninsured rate. We want to evaluate the ATT. What is the effect of ACA-Medicaid expansion reform on uninsured rates? We will draw our attention to the states that expanded Medicaid in 2014 plus the 19 states that did not expand Medicaid until 2018. States that expanded Medicaid between 2014 and 2018 are dropped from the sample. This is to avoid the case of bad comparison, an issue that arises from comparing early treated units with later treated units. We will reflect on this topic later. For now, let’s take a quick look at the data. user = Sys.info()[&quot;nodename&quot;] if(user == &quot;vinish-Legion-Pro-7-16IRX9H&quot;){ source(&quot;/home/vinish/Dropbox/Medicaid_South/code/filepath.r&quot;) }else{ source(&quot;/Users/user1/Dropbox/Medicaid_South/code/filepath.r&quot;) } library(pacman) p_load(fixest, dplyr, ggplot2, tidyverse, patchwork, arrow) # load in county level uninsured rate data merged with other variables mort_allcauses &lt;- read_feather( file.path(datapath, &quot;NVSS_data_county_2010to2017_merged_allcauses.feather&quot;)) %&gt;% mutate(treat = ifelse(is.na(treat) == T, &quot;control 3&quot;, treat)) %&gt;% filter(yearexpand == 2014 &amp; age == 0 &amp; race_name == &quot;white&quot;) %&gt;% dplyr::select(&quot;countyfips&quot;, &quot;year&quot;, &quot;state.abb&quot;, &quot;expand&quot;, &quot;yearexpand&quot;, &quot;sahieunins138&quot;, &quot;GovernorisDemocrat1Yes&quot;) %&gt;% filter(duplicated(.)) %&gt;% arrange(countyfips, year) # sort by countyfips and year # the select() function is masked # by other packages, so use dplyr::select() instead # only keep the years 2013 and 2014 for the canonical case dat_canonical &lt;- mort_allcauses %&gt;% filter(year %in% c(2013, 2014)) head(dat_canonical) ## # A tibble: 6 × 7 ## countyfips year state.abb expand yearexpand sahieunins138 GovernorisDemocrat1Yes ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1001 2013 AL 0 2014 39.6 0 ## 2 1001 2014 AL 0 2014 31.9 0 ## 3 1003 2013 AL 0 2014 45.1 0 ## 4 1003 2014 AL 0 2014 43.8 0 ## 5 1005 2013 AL 0 2014 37.3 0 ## 6 1005 2014 AL 0 2014 34 0 dat_canonical %&gt;% tabyl(state.abb, expand) ## state.abb 0 1 ## AL 126 0 ## AR 0 144 ## AZ 0 30 ## CA 0 112 ## CO 0 106 ## CT 0 8 ## DE 0 6 ## FL 134 0 ## GA 291 0 ## IA 0 196 ## ID 84 0 ## IL 0 196 ## KS 181 0 ## KY 0 240 ## MA 0 28 ## MD 0 48 ## ME 32 0 ## MI 0 164 ## MN 0 168 ## MO 230 0 ## MS 149 0 ## NC 198 0 ## ND 0 79 ## NE 144 0 ## NH 0 20 ## NJ 0 39 ## NM 0 50 ## NV 0 26 ## NY 0 124 ## OH 0 174 ## OK 148 0 ## OR 0 66 ## RI 0 10 ## SC 88 0 ## SD 98 0 ## TN 190 0 ## TX 409 0 ## UT 54 0 ## VA 248 0 ## VT 0 26 ## WA 0 72 ## WI 136 0 ## WV 0 110 ## WY 46 0 cat(&quot;The expansion states are: \\n&quot;) ## The expansion states are: table(dat_canonical$state.abb[dat_canonical$expand == 1]) ## ## AR AZ CA CO CT DE IA IL KY MA MD MI MN ND NH NJ NM NV NY OH OR RI VT WA WV ## 144 30 112 106 8 6 196 196 240 28 48 164 168 79 20 39 50 26 124 174 66 10 26 72 110 cat(&quot;The non-expansion states are: \\n&quot;) ## The non-expansion states are: table(dat_canonical$state.abb[dat_canonical$expand == 0]) ## ## AL FL GA ID KS ME MO MS NC NE OK SC SD TN TX UT VA WI WY ## 126 134 291 84 181 32 230 149 198 144 148 88 98 190 409 54 248 136 46 length(table(dat_canonical$state.abb[dat_canonical$expand == 1])) ## [1] 25 length(table(dat_canonical$state.abb[dat_canonical$expand == 0])) ## [1] 19 The two groups are as follows: Expansion states (treated): AR, AZ, CA, CO, CT, DE, HI, IA, IL, KY, MA, MD, MI, MN, ND, NH, NJ, NM, NV, NY, OH, OR, RI, VT, WA, WV Non-expansion states (control): AL, FL, GA, ID, KS, ME, MO, MS, NC, NE, OK, SC, SD, TN, TX, UT, VA, WI, WY "],["naive-estimator.html", "7.4 Naive estimator", " 7.4 Naive estimator A naive estimate of ATT would just be the difference in means between the treated and control groups in the period following the expansion. naive &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year &gt;= 2014]) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year &gt;= 2014]) print(naive) ## [1] -13.42305 The naive estimate suggests that uninsured rate dropped by -13.66 percentage points following the Medicaid expansion in 2014. But can we trust this estimate? Not really! Here are some reasons why the naive estimate fails: One way to assess the validity of naive estimate is to compare the (natural) experiment on hand with the randomized control case. Note that we are very far away from the randomized controlled trial in this case. The treatment (decision to expand Medicaid) is not random. Note that states voluntarily decided to expand Medicaid. This means that expansion versus non-expansion states may be very different in terms of pre-treatment characteristics. For example, many of the southern states did not expand Medicaid. Also, pre-treatment uninsured rates of southern states are generally higher compared to non-southern states. The naive comparison can simply be capturing the difference in pre-treatment characteristics correlated with the treatment assignment. The baseline outcome among the treatment group may differ significantly from the control group. For example, southern states have higher population of Blacks compared to non-South. Typically, uninsured rate is higher among Blacks. Hence, it is difficult to disentagle the influence of democraphic composition versus Medicaid expansion. Let’s evaluate the difference in uninsured rate between the expansion versus the non-expansion states in the pre-treatment year (2013). naive_pre &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year &lt; 2014]) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year &lt; 2014]) print(naive_pre) ## [1] -7.669252 Note that treatment units on average have 7.68 percentage points lower uninsured rate compared to the control units even prior to the treatment. Hence, the naive estimator captures the pre-existing differerences in outcome; something that we don’t want. Since the treatment is not randomized a lot of baseline characteristics that influence the outcome across the treated and control groups may differ dramatically. If these variables are thought to influence the dynamics of the outcome variable, then we will be capturing the influence of such variables rather than the treatment itself. "],["canonical-difference-in-differences-framework.html", "7.5 Canonical Difference in Differences Framework", " 7.5 Canonical Difference in Differences Framework We would like to alleviate the aforementioned concerns. One way to address the second concern, i.e., outcomes in pre-treatment period may differ significantly between the treatment and control groups, is to take out the mean difference in outcome during the pre-treatment period from the mean difference in outcome post treatment. This approach uses two groups and two periods, which is termed as the canonical DiD case. The canonical DiD can be seen using a \\(2\\times 2\\) matrix. # Create the data for the 2x2 matrix data &lt;- data.frame( group = rep(c(&quot;Control&quot;, &quot;Treated&quot;), each = 2), time = rep(c(&quot;Pre&quot;, &quot;Post&quot;), times = 2), outcome = c(5, 5, 5, 7), # Example outcomes label = c(&quot;Y_11&quot;, &quot;Y_01&quot;, &quot;Y_10&quot;, &quot;Y_00&quot;) # Labels for matrix cells ) # Base plot ggplot(data, aes(x = time, y = group)) + geom_tile(fill = &quot;lightblue&quot;, color = &quot;black&quot;) + # Create the matrix grid geom_text(aes(label = label), size = 6, fontface = &quot;bold&quot;) + # Add cell labels annotate(&quot;text&quot;, x = 1, y = 2.55, label = &quot;Pre-Treatment&quot;, size = 5, fontface = &quot;italic&quot;) + annotate(&quot;text&quot;, x = 2, y = 2.55, label = &quot;Post-Treatment&quot;, size = 5, fontface = &quot;italic&quot;) + annotate(&quot;text&quot;, x = 0.45, y = 2, label = &quot;Control Group&quot;, angle = 90, size = 5, fontface = &quot;italic&quot;) + annotate(&quot;text&quot;, x = 0.45, y = 1, label = &quot;Treated Group&quot;, angle = 90, size = 5, fontface = &quot;italic&quot;) + labs( title = &quot;2x2 Difference-in-Differences Matrix Illustration&quot;, x = NULL, y = NULL ) + theme_minimal() + theme( axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank(), plot.title = element_text(hjust = 0.5, size = 16) ) Note that the naive estimator simply is: \\(E(Y_{11}) - E(Y_{01})\\). This can be considered as the first difference. Next, we construct the second difference across the two groups during the pre-treatment period as: \\(E(Y_{10}) - E(Y_{00})\\). The difference-in-differences estimate: $_{did} = \\(E[Y_{11} - Y_{01}] - E[Y_{10} - Y_{00}]\\). This defines the term “difference-in-differences” as it involves two differences in means across the treatment and control group; one post treatment and the other prior to the treatment. In the ACA-Medicaid expansion example that involves two groups and two time periods: cat(&quot;did estimate= \\n&quot;, naive - naive_pre) ## did estimate= ## -5.7538 This suggests that uninsured rate dropped by 5.81 percentage points following the Medicaid expansion in year 2014. Let’s formally visit the DiD approach to appreciate some necessary assumptions while connecting it with ATT. The ATT is given as: \\[\\begin{equation} \\tau = E(Y^1(1) - Y^0(1)| D = 1) \\tag{7.1} \\end{equation}\\] Here, \\(Y^1(1)\\) is the outcome following the treatment, and \\(Y^0(1)\\) is the counterfactual, i.e., the outcome without the treatment. \\(E(Y^0(1)| D = 1)\\), the conditional mean outcome of the treatment group in absence of the treatment, is not revealed as it is the counterfactual. Our job still remains to come up with a valid counterfactual. The validity of the difference-in-differences estimate rest on the parallel trend assumption. Let’s consider the canonical case of DiD with two groups (treatment &amp; control) and two periods (before and after treatment). Formally the parallel trend assumption (PTA) is given as: \\[\\begin{equation} E(Y^0(1) - Y^0(0)| D = 1) = E(Y^0(1) - Y^0(0)| D = 0) \\tag{7.2} \\end{equation}\\] Here, we have written the parallel trend assumption using the potential outcomes. \\(Y^0(1)\\) is the potential outcome in the post-treatment period in absence of the treatment, \\(Y^0(0)\\) is the potential outcome in pre-treatment period. All forms of potential outcomes are revealed (observed) except \\(E(Y^0(1) | D = 1)\\), which is average outcome for the treated group in the post-treatment period in absence of the treatment. The parallel trend assumption states that, in absence of the treatment, outcomes for the treatment and control groups after the treatment would follow similar trend to that of the pre-treatment period. How does the parallel trend assumption help in identifying the ATT \\((\\delta)\\)? To see this, lets expand equation @ref{eq:att}. \\[ \\begin{align} \\delta = E(Y^1(1)| D = 1) - E(Y^0(1)| D = 1) \\\\ = E(Y^1(1)| D = 1) - E(Y^0(1)| D = 1) + E(Y^0(0)| D = 1) - E(Y^0(0)| D = 1) \\\\ = \\{E(Y^1(1)| D = 1) - E(Y^0(0)| D = 1)\\} - \\{E(Y^0(1)| D = 1) - E(Y^0(0)| D = 1)\\} \\\\ = \\{E(Y^1(1)| D = 1) - E(Y^0(0)| D = 1)\\} - \\{E(Y^0(1)| D = 0) - E(Y^0(0)| D = 0)\\} \\\\ = \\{E(Y(1)| D = 1) - E(Y(0)| D = 1)\\} - \\{E(Y(1)| D = 0) - E(Y(0)| D = 0)\\} \\end{align} \\] Moving from line 3 to 4 makes use of the parallel trend assumption as given in equation @ref{eq:ptrend1}. So it turns out that under the parallel trend assumption, DiD framework uncovers the ATT. However, the parallel trend assumption cannot be exactly tested since it is impossible to observe the potential outcome of the treated group in absence of the treatment. If we cannot provide a feasible test for the parallel trend assumption, how can we then attest for the validity of the DiD estimate? "],["did-in-multi-period-set-up.html", "7.6 DiD in multi-period set up", " 7.6 DiD in multi-period set up So far we have only considered the canonical DiD (two groups and two period DiD framework). However, data may be available for several periods before and after the treatment implementation. In this case, we will be in the setting of multi-period DiD. For example, for the Medicaid expansion case, we have data spanning from years 2010 to 2017. We’d like to utilize all of these years rather than just years 2013 and 2014 as in the canonical DiD setting. Using the multi-period set up can be helpful for the following reasons: We can trace the dynamic effects of the treatment following the implementation. This means that we can trace the ATT estimate of Medicaid expansion for periods following its implementation, i.e., in 2014, 2015, 2016, and 2017. If the effects of expansion are increasing over time, then this setting will be very helpful in uncovering such dynamics. In other words, we can trace the potential heterogeneous effects of the treatment over time. Next, the multi-period setting allows us to way to check the validity of the parallel trend assumption. As mentioned above, the exact test for the parallel trend assumption is infeasible. However, we can deduce suggestive evidence to assess validity of this assumption. See below. Let’s first plot the mean uninsured rate between the years 2010 and 2017 across the treatment versus the control groups. # estimate the mean uninsured rate by expansion status and year dat_sum &lt;- mort_allcauses %&gt;% group_by(year, expand) %&gt;% summarize(uninsured = mean(sahieunins138)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. post_cf_treat &lt;- dat_sum$uninsured[dat_sum$expand == 0] + naive_pre dat_treat_cf &lt;- data.frame(year = seq(2010, 2017, 1), expand = rep(1, 8), cf = post_cf_treat) dat_sum &lt;- dat_sum %&gt;% left_join(dat_treat_cf, by = c(&quot;year&quot;, &quot;expand&quot;)) f_uninsured &lt;- ggplot(dat_sum, aes(x = year, y = uninsured, group = as.factor(expand), color = as.factor(expand)), shape = as.factor(expand)) + geom_point() + geom_line() + theme_minimal() + geom_vline(xintercept = 2014, linetype = &quot;dashed&quot;) ggplot(dat_sum, aes(x = year, y = uninsured, group = as.factor(expand), color = as.factor(expand)), shape = as.factor(expand)) + geom_point() + geom_line() + theme_minimal() + geom_vline(xintercept = 2014, linetype = &quot;dashed&quot;) + geom_line(data = dat_sum, aes(y = cf), linetype = &quot;dashed&quot;, color = &quot;lightblue&quot;, linewidth = 1) The plot shows that uninsured rate trended parallelly between the treated and control groups prior to the expansion. The rates dropped following the expansion year (2014) across both the treated and control groups, but the magnitude of drop in uninsured rate is higher for the treated group.8 The DiD uses the control group as the counterfactual for the treated group. In other words, we are assuming that in absence of the treatment, outcome for the treated group would have evolved similarly to that of the control group. This is the key assumption of DiD – the parallel trend assumption. Note that, once we difference out the pre-treatment means between the treated and controls groups, we obtain the dashed line, which is the counterfactual for the treated group. In this case, the counterfactual line overlaps the average outcome for the treated group, suggesting that outcome was trending similarly across the two groups during the pre-treatment period. Loosely speaking, ATT is the average gap between the outcome for the treated group (solid blue line) and the counterfactual outcome (dashed line) during the post-treatment period. As previously mentioned, we are unable to provide an actual test for parallel trend assumption due to the missing data on treated observations following the treatment in a state when treatment is absent. This does not mean that we are absolutely helpless. There are a list of things that can be done to provide suggestive evidence in favor of or lack of parallel trend. They are: Check pre-treatment summary statistics across treatment and control groups. Say, treatment and control groups have highly different pre-treatment characteristics. It is unlikely that parallel trend assumption will hold in this case. It is because pre-treatment variables that influence the outcome can induce different dynamics in trends. Hence, the outcome trends between the treated and control groups may vary even in absence of the treatment. Usually trends in outcomes across treated and control groups are assessed to evaluate parallel trends. If outcome is trending parallely between the two groups prior to the treatment, then it provides evidence in favor of PTA. However, concluding that PTA does not hold in a case of non-parallel trend is a narrow assessment at the best, since this approach merely depicts unconditional trends. PTA may have more leverage once (pre-treatment) covariates are accounted for. The next approach that has been used widely is the event-study. We will discuss this approach soon. In most cases, unconditional parallel trend assumption may not be very convincing. Why? The drop in uninsured rate for the control group can be explained by the implementation of other aspects of ACA such as subsidies and employment mandate.↩︎ "],["conditional-parallel-trend-assumption.html", "7.7 Conditional Parallel Trend Assumption", " 7.7 Conditional Parallel Trend Assumption We have discussed the parallel trend assumption already. This is the unconditional parallel trend assumption, which assumes that the parallel trend assumption holds without accounting for any covariates. One way to bolster the credibility of the parallel trend assumption is to claim that it only holds conditional on covariates. For example, since the decision of whether to expand Medicaid was under the states’ discretion, we may want to condition on a state’s partisan leaning, i.e. Democrat versus Republican Governor. In the canonical model, the parallel trend assumption can be modified to obtain the conditional parallel trend assumption. This means we are assuming that the parallel trend only exists once conditioned on covariates. This is given as below. \\[\\begin{equation} E(Y^0(1) - Y^0(0)| D = 1, X) = E(Y^0(1) - Y^0(0)| D = 0, X) \\tag{7.3} \\end{equation}\\] The conditional parallel trend is easy to understand when covariates required are small in number and if covariates are discrete. For example, say, in the case of ACA-Medicaid example, the parallel trend assumption holds once accounting for state’s partisan leaning in 2013 (pre-treatment). What does this mean exactly? This would be equivalent to running two different DiD estimates: one for the group with Democrat Governor and the other for the Republican governor. This gives two DiD estimates. Next, we average these two estimates to get the DiD estimate for the whole sample. GovernorisDemocrat1Yes is a variable that indicates whether a state’s governor is of the Democrat party. # we need to first get the state partisan data for 2013 and merge with the main data # note that it is recommended to conduct all data cleaning in separate files. # we are doing this in a fly, so I break that rule. dat_gov2013 &lt;- dat_canonical %&gt;% filter(year == 2013) %&gt;% dplyr::select(c(countyfips, GovernorisDemocrat1Yes)) %&gt;% rename(GovernorisDemocrat1Yes2013 = GovernorisDemocrat1Yes) dat_canonical &lt;- dat_canonical %&gt;% merge(dat_gov2013, by = c(&quot;countyfips&quot;), all.x = TRUE) # get the first and second differences for states with democrat governor in 2013 first_diff_dem &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year == 2014 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 1], na.rm = T) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year == 2014 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 1], na.rm = T) second_diff_dem &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year == 2013 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 1], na.rm = T) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year == 2013 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 1], na.rm = T) # get the first and second differences for the state with the republican state in 2013 first_diff_rep &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year == 2014 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 0], na.rm = T) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year == 2014 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 0], na.rm = T) second_diff_rep &lt;- mean(dat_canonical$sahieunins138[dat_canonical$expand == 1 &amp; dat_canonical$year == 2013 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 0], na.rm = T) - mean(dat_canonical$sahieunins138[dat_canonical$expand == 0 &amp; dat_canonical$year == 2013 &amp; dat_canonical$GovernorisDemocrat1Yes2013 == 0], na.rm = T) did_dem &lt;- first_diff_dem - second_diff_dem did_rep &lt;- first_diff_rep - second_diff_rep # fraction with Democrat Governor gov_dem &lt;- mean(dat_canonical$GovernorisDemocrat1Yes2013, na.rm = T) # weighted average did_estimate &lt;- (gov_dem * did_dem) + (1 - gov_dem) * did_rep # conditional DiD estimate cat(&quot;did estimate (controling for state partisan leaning in 2013) = \\n&quot;, did_estimate) ## did estimate (controling for state partisan leaning in 2013) = ## -5.041286 # unconditional DiD estimate (note: we already have this from previous estimation) cat(&quot;did estimate (no controls) = \\n&quot;, naive - naive_pre) ## did estimate (no controls) = ## -5.7538 What we observe is that DiD estimate that accounts for a state’s political leaning in 2013 is slightly lower than the unconditional version. The difference is not too large in this case. However, I’d still be inclined to trust the conditional DiD estimate. Why so? This is because the condition forces comparison across the treatment vs. control units with the same political leaning in 2013. For example, treated units under the Democrat regime are compared to control units also under the Democrat regime. The same goes with treated and control units falling under the Republican Governor in 2013. Since, ACA was highly politicized and also since Democrat vs. Republican states are quite different in socio-economic factors, we need to make sure that we are comparing units with similar political leaning. In other words, we are conducting relatively more similar comparison. "],["some-concerns-with-controls.html", "7.8 Some concerns with controls", " 7.8 Some concerns with controls Ok, we sort of argued that conditional DiD may perform better in the real world. Like everything, this does not come easily. Here are some concerns regarding including covariates. First of all, we don’t always clearly know what to control for in the real world. I previously made the case that a state’s political leaning is an important variable and one should account for it. However, there might be other variables that I’m completely missing out. We can use economic reasoning, past studies in the literature, as well as data based methods (e.g., double lasso for variable selection) to decide on controls. There are good controls and there are bad controls. Let’s say you think its important to improve comparability between the treated and control units. To do so, you pick income in 2014 as a control. This, I’d argue, is an example of a bad control. Why? Its because income in 2014 might be affected by the ACA-Medicaid expansion, which can lead to bias in the estimate of interest. We’d want to make sure that the controls are not directly affected by the reform itself. This is why mostly researchers rely on pre-treatment variables rather than post-treatment variables as controls. Earlier, we looked at the case of a binary variable (Republican vs. Democrat Governor in 2013) as a covariate. However, in this setting, if the number of control increases, then the sample space thins out fairly quickly. Say, we add the following binary controls: \\(i)\\) urban|rural, \\(ii)\\) south|non-south, \\(iii)\\) high|low uninsured unit based on 2013 (baseline) uninsured rates. Here, we’d have \\(2^{4}\\) different splitting of the sample. If you decide to add in more controls, the number of subgroups will increase exponentially. Note that we’ve only considered binary controls so far. This issue worsens if you add in continuous variables as controls. This is known as the curse of dimensionality. There are several ways to avoid this curse. One relatively less taxing approach is to incorporate controls in the regression format. However, this leads to its own issues. Firstly, the covariates are being linearly incorporated in the regression, which leads to a linear functional form assumption. Second, if the effect of the reform varies along the covariate, then this might lead to a bias on the estimate of interest. Hence, we’d want to incorporate controls in a more flexible way using the inverse probability weighting for DiD or Doubly Robust framework tailored for DiD. But more on this later! "],["the-2-times-2-difference-in-differences-estimate.html", "7.9 The \\(2 \\times 2\\) Difference-in-Differences Estimate", " 7.9 The \\(2 \\times 2\\) Difference-in-Differences Estimate Now that we’ve disscussed the difference-in-differences framework (mostly canonical DiD), we would like to look at various ways of estimating DiD. As we’ve seen, (a canonical) DiD estimation can simply be done non-parametrically by using the appropriate differences in means. Another way to do it is by using the regression framework, which provides several advantages including but not limited to: \\(i)\\) estimation of standard errors; \\(ii)\\) ease of accounting for necessary covariates; and \\(iii)\\) feasible estimation of DiD with multiple treatment groups with staggered treatment. Let’s begin with the canonical DiD framework using the regression format. I’m going to set it up as the following: \\[\\begin{equation} \\label{eq:DiD_reg} Y_{it} = \\alpha + \\tau Post_{it} \\times D_{i} + \\sigma Post_{it} + \\eta D_{i} + \\epsilon_{it} \\end{equation}\\] Let’s rewrite the DiD estimator from before as: \\(\\tau_{did} = \\underbrace{E[Y_{11} - Y_{10} | D = 1]}_{first\\; difference} - \\underbrace{E[Y_{01} - Y_{00} | D = 0]}_{second\\; difference}\\) We’d want to see whether estimation of \\(\\tau\\) in the regression above uncovers the DiD estimate. Let’s look at the following conditional expectations. 1). expected outcome for treated group post treatment: \\(E(Y | D = 1, Post = 1) = \\alpha + \\tau + \\sigma + \\eta\\) 2). expected outcome for treated group pre treatment: \\(E(Y | D = 1, Post = 0) = \\alpha + \\eta\\) 3). expected outcome for control group post treatment: \\(E(Y | D = 0, Post = 1) = \\alpha + \\sigma\\) 4). expected outcome for control group pre treatment: \\(E(Y | D = 0, Post = 0) = \\alpha\\) If you do the math, the first difference is given as \\(\\tau + \\sigma\\) and the second difference is \\(\\sigma\\). The DiD – the difference between the first and the second difference – is \\(\\tau\\). Hence, the estimation of \\(\\tau\\), in the regression framework above, is the DiD estimate. Let’s apply this to our ACA-Medicaid example. # lets create the post, treat, and the interaction between the post and treat (labeled as did) dat_canonical &lt;- dat_canonical %&gt;% mutate(post = ifelse(year &gt;= 2014, 1, 0), treat = ifelse(expand == 1, 1, 0), did = post * treat) reg_did &lt;- lm(sahieunins138 ~ did + post + treat, data = dat_canonical) summary(reg_did) ## ## Call: ## lm(formula = sahieunins138 ~ did + post + treat, data = dat_canonical) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.0767 -5.0184 -0.7184 4.7586 27.0540 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.1460 0.1931 213.08 &lt;2e-16 *** ## did -5.7538 0.4172 -13.79 &lt;2e-16 *** ## post -4.8046 0.2732 -17.59 &lt;2e-16 *** ## treat -7.6693 0.2946 -26.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.464 on 5224 degrees of freedom ## Multiple R-squared: 0.4322, Adjusted R-squared: 0.4319 ## F-statistic: 1326 on 3 and 5224 DF, p-value: &lt; 2.2e-16 did &lt;- naive - naive_pre print(did) ## [1] -5.7538 Note that the difference in mean did and the regression coefficient on the interaction between \\(Post \\times treat\\) are the same. Plus, we get the standard errors as well. Of course, the standard errors are not adjusted for clustering. The errors of observations within a cluster (the ground at which the treatment is implemented, in our case state) will be correlated, so we need to account for it. The way to do it is to cluster the standard error at the state level. This can be done using the fixest library and feols command. reg_did_cluster &lt;- feols(sahieunins138 ~ did + post + treat, data = dat_canonical, cluster = ~state.abb) summary(reg_did_cluster) ## OLS estimation, Dep. Var.: sahieunins138 ## Observations: 5,228 ## Standard-errors: Clustered (state.abb) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.14598 1.871204 21.98904 &lt; 2.2e-16 *** ## did -5.75380 1.320435 -4.35750 0.00008033 *** ## post -4.80456 0.280312 -17.14003 &lt; 2.2e-16 *** ## treat -7.66925 2.502144 -3.06507 0.00375138 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 7.46094 Adj. R2: 0.431902 Note that the did coefficient remains unchanged. However, the standard error is inflated after clustering the errors at the state level. "],["event-study-model.html", "7.10 Event study model", " 7.10 Event study model As we have discussed, the validity of DiD estimate depends on the parallel trend assumption. Since the direct test for parallel trend requires the potential outcome of treated unit in absence of the treatment, it is not feasible. Although we are not going to be able to directly test the parallel trend, we can provide suggestive evidence in favor of (or lack of) parallel trend. For this, we need multi-period data, particularly for periods prior to the implementation of the treatment. A simple way to assess parallel trend is to evaluate the unconditional means across the treated and untreated units (expansion and non-expansion states in our case). This is shown in Section 4 (DiD in multi-period set up). I’ve included the figure here. f_uninsured Here, we see that the uninsured rates between the expansion and non-expansion states trended similarly (or parallely) prior to the treatment. This allows us to argue that trends in uninsured rate would’ve remained similar (or would not differ systematically) in absence of the ACA-Medicaid reform. This by far is the simplest but yet powerful way to argue parallel trend assumption in practice. However, note that units in non-expansion states on average had higher uninsured rate compared to their counterparts in the expansion states even prior to the reform. Ideally, we would want treated and control units to have similar baseline features. This is where regression comes into play. Using regression, we can evaulate a difference-in-differences model by accounting for necessary covariates.9 Moreover, it allows us to evaluate dynamic effects of the treatment, i.e., the impacts of the treatment over time (1st period, 2nd period, and so on). The event study model can be written as: \\[\\begin{equation} \\label{eq:DiD_eventstudy} Y_{it} = \\alpha + \\underbrace{\\sum_{j = -k}^{k}}_{j \\neq -1} \\tau_j \\times 1(\\underbrace{t - G}_{r} = j) \\times D_i + \\sigma_{t} + \\eta D_{i} + \\epsilon_{it} \\end{equation}\\] So what are these notations here? \\(\\alpha\\) is the intercept \\(1(\\underbrace{t - G}_{r} = j)\\): This is an indicator that turns on (takes the value 1) when the relative time \\(r\\) in the data is equal to \\(j\\), and turns off otherwise (takes the value 0). The relative time, \\(r\\), is simply the difference between the period \\(t\\) and the implementation year of the reform \\(E\\). For simplicity, I have the minimum and maximum of relative time as \\(-k\\) and \\(k\\), respectively. But of course, this can vary in practice. The omitted category is the year before the reform (i.e., when \\(r = -1\\)). We’ll discuss more about the omitted category later on. \\(\\sigma_t\\): Is the time fixed effects. It captures the changes that are common across treatment and control units over time. \\(D_i\\): Is the fixed effects for treated/control units. In practice, treatment and control units can fundamentally differ in several characteristics. Accounting for \\(D_i\\) separately captures the average difference in the outcome between treatment and control units that does not change over time (time invariant). In other words, controling for \\(D_i\\) accounts for time invariant heterogeneity across the treatment vs. control groups. For example, we saw that expansion units on average had lower uninsured rate even prior to the reform compared to the treated units. For instance, this aspect of the difference in outcomes across the two groups is accounted by \\(D_i\\). From a specification perspectice, the main difference between the canonical DiD specification and the event study specification is the incorporation of the term \\(1(\\underbrace{t - G}_{r} = j)\\), which is interacted with the treatment indicator, \\(D_i\\). This allows us to evaluate the effect of the treatment separately for a given period following (or before) the treatment implementation. Such dynamic effects are picked up by \\(\\widehat{\\tau_t}\\). Let’s try and break down whats going on in the event study specification. First, it is important to realize the role of the omitted category. In the event study specification above, I’ve dropped the period prior to the reform. Note that this is essential from a theroretical standpoint, since inclusion of all periods would result to a fully saturated model and create multicollinearity. Dropping the period prior to the treatment implementation uses this period as the relative period. From point 2, we can think of the event-study specification as estimating several DiD type models, where the second difference is fixed and pertains to the omitted period, while the period of interest varies. Let me elaborate on this. To see this, note that when \\(t - G = -1\\), the conditional expectation for the treated group is \\(E(Y | D = 1, t = G - 1) = \\alpha + \\sigma_{(G-1)} + \\eta\\) and and for the control group is: \\(E(Y | D = 0, G-1) = \\alpha + \\sigma_{(G-1)}\\). \\(E(Y | D = 1, G - 1) - E(Y | D = 0, G-1)\\) is synonymous to the second difference: i.e., the difference in conditional means between the treatment and control units in the period before the treatment implementation. For the first difference, let’s look at the relative period \\(t - G = 0\\), the period of the reform implementation. The conditional expectation for the treatment group is: \\(E(Y | D = 1, t = G) = \\alpha + \\tau_0 + \\sigma_{G} + \\eta\\) and that for the control group is: \\(E(Y | D = 0, t = G) = \\alpha + \\sigma_{G}\\). Here, the first difference is: \\(E(Y | D = 1, t = G) - E(Y | D = 0, t = G)\\). The DiD estimand during the period of the reform, \\(r=0\\), is given as: \\[\\begin{equation} \\underbrace{E(Y | D = 1, t = G) - E(Y | D = 0, t = G)}_{first\\; difference} - \\underbrace{E(Y | D = 1, t = G - 1) - E(Y | D = 0, t = G-1)}_{second\\; difference} = \\tau_0 \\end{equation}\\] The DiD estimand for the period following the reform \\((r= 1)\\) is given as: \\[\\begin{equation} \\underbrace{E(Y | D = 1, t = G+1) - E(Y | D = 0, t = G+1)}_{first\\; difference} - \\underbrace{E(Y | D = 1, t = G - 1) - E(Y | D = 0, t = G-1)}_{second\\; difference} = \\tau_1 \\end{equation}\\] Similarly, we can think of the event study model as nesting the DiD estimation pertaining to the relative periods, \\(r=2, \\; 3\\) and so on. This way, the event study specification allows us to estimate period specific treatment effects from relative time period \\(-k\\) to \\(k\\) in relation to the conditional mean difference between the treated and control groups a period prior to the treatment implementation. Following the estimation of the event study model, we will have two sets of estimates: i) \\(\\tau_{-k}, \\; \\tau_{-k+1}, ..., \\; \\tau_{-2}\\), and ii) \\(\\tau_{1}\\), \\(\\tau_{2}\\), …, \\(\\tau_{k}\\). The estimation of \\(\\tau\\)s prior to the treatment allows us to make inference regarding the parallel trend assumption. If \\(\\widehat{\\tau}_j\\) for \\(j&lt;-1\\) is close to zero, then it provides suggestive evidence that the outcomes between the treatment and control units are trending similarly prior to the treatment. Let’s estimate the event study model for the ACA-Medicaid data. Note that we are using the larger data set where year spans from 2010 to 2018. First create the relative time variable. mort_allcauses &lt;- mort_allcauses %&gt;% mutate(yeararound = year - yearexpand) # view table(mort_allcauses$yeararound) ## ## -4 -3 -2 -1 0 1 2 3 ## 2614 2617 2619 2620 2608 2616 2615 2615 Note that the relative time spans from -4 to 3. Since, we are only considering the states that implemented ACA-Medicaid expansion in year 2014, -4 pertains to year 2010, -3 to 2011, and so on. Next, let’s create relative time indicators and interact them with the expansion status. In the model, this pertains to \\(1(\\underbrace{t - G}_{r} = j) \\times D_i\\) part. mort_allcauses &lt;- mort_allcauses %&gt;% mutate(rel_pre4 = ifelse(yeararound == -4, 1, 0), # indicator for r = -4 rel_pre4 = rel_pre4 * expand, # interact the indicator for r = -4 with expansion status rel_pre3 = ifelse(yeararound == -3, 1, 0), rel_pre3 = rel_pre3 * expand, rel_pre2 = ifelse(yeararound == -2, 1, 0), rel_pre2 = rel_pre2 * expand, rel_pre1 = ifelse(yeararound == -1, 1, 0), rel_pre1 = rel_pre1 * expand, rel_post0 = ifelse(yeararound == 0, 1, 0), rel_post0 = rel_post0 * expand, rel_post1 = ifelse(yeararound == 1, 1, 0), rel_post1 = rel_post1 * expand, rel_post2 = ifelse(yeararound == 2, 1, 0), rel_post2 = rel_post2 * expand, rel_post3 = ifelse(yeararound == 3, 1, 0), rel_post3 = rel_post3 * expand) Now thats done, let’s specify the model and estimate it using OLS. # quick look at the data head(mort_allcauses) ## # A tibble: 6 × 16 ## countyfips year state.abb expand yearexpand sahieunins138 GovernorisDemocrat1Yes yeararound rel_pre4 rel_pre3 rel_pre2 rel_pre1 rel_post0 rel_post1 rel_post2 rel_post3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 2010 AL 0 2014 40.6 0 -4 0 0 0 0 0 0 0 0 ## 2 1001 2011 AL 0 2014 41.6 0 -3 0 0 0 0 0 0 0 0 ## 3 1001 2012 AL 0 2014 39.6 0 -2 0 0 0 0 0 0 0 0 ## 4 1001 2013 AL 0 2014 39.6 0 -1 0 0 0 0 0 0 0 0 ## 5 1001 2014 AL 0 2014 31.9 0 0 0 0 0 0 0 0 0 0 ## 6 1001 2015 AL 0 2014 27.5 0 1 0 0 0 0 0 0 0 0 # specify the event study model reg_es &lt;- lm(sahieunins138 ~ rel_pre4 + rel_pre3 + rel_pre2 + rel_post0 + rel_post1 + rel_post2 + rel_post3 + factor(state.abb) + factor(year), data = mort_allcauses ) # print summary of the results summary(reg_es) ## ## Call: ## lm(formula = sahieunins138 ~ rel_pre4 + rel_pre3 + rel_pre2 + ## rel_post0 + rel_post1 + rel_post2 + rel_post3 + factor(state.abb) + ## factor(year), data = mort_allcauses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.0650 -3.2383 -0.3658 2.7562 22.4298 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.4230 0.2496 165.952 &lt; 2e-16 *** ## rel_pre4 0.4025 0.2740 1.469 0.141786 ## rel_pre3 -0.1852 0.2739 -0.676 0.499059 ## rel_pre2 0.1952 0.2738 0.713 0.475988 ## rel_post0 -5.7906 0.2742 -21.115 &lt; 2e-16 *** ## rel_post1 -7.8759 0.2739 -28.757 &lt; 2e-16 *** ## rel_post2 -7.9523 0.2739 -29.032 &lt; 2e-16 *** ## rel_post3 -8.2633 0.2739 -30.165 &lt; 2e-16 *** ## factor(state.abb)AR 2.3846 0.3504 6.806 1.03e-11 *** ## factor(state.abb)AZ -0.6241 0.5306 -1.176 0.239519 ## factor(state.abb)CA -2.3065 0.3669 -6.286 3.32e-10 *** ## factor(state.abb)CO -0.2576 0.3711 -0.694 0.487555 ## factor(state.abb)CT -10.4509 0.7144 -14.629 &lt; 2e-16 *** ## factor(state.abb)DE -9.1957 1.0411 -8.833 &lt; 2e-16 *** ## factor(state.abb)FL 4.2887 0.3050 14.060 &lt; 2e-16 *** ## factor(state.abb)GA 6.7838 0.2625 25.846 &lt; 2e-16 *** ## factor(state.abb)IA -8.9609 0.3341 -26.822 &lt; 2e-16 *** ## factor(state.abb)ID 4.5479 0.3467 13.117 &lt; 2e-16 *** ## factor(state.abb)IL -7.3991 0.3341 -22.147 &lt; 2e-16 *** ## factor(state.abb)KS -1.1298 0.2852 -3.961 7.48e-05 *** ## factor(state.abb)KY -3.5130 0.3256 -10.791 &lt; 2e-16 *** ## factor(state.abb)MA -21.4058 0.5439 -39.357 &lt; 2e-16 *** ## factor(state.abb)MD -7.6416 0.4542 -16.825 &lt; 2e-16 *** ## factor(state.abb)ME -9.4409 0.4860 -19.426 &lt; 2e-16 *** ## factor(state.abb)MI -6.1339 0.3429 -17.886 &lt; 2e-16 *** ## factor(state.abb)MN -13.0886 0.3416 -38.316 &lt; 2e-16 *** ## factor(state.abb)MO -0.6158 0.2726 -2.259 0.023884 * ## factor(state.abb)MS 4.2729 0.2975 14.361 &lt; 2e-16 *** ## factor(state.abb)NC 3.2062 0.2802 11.441 &lt; 2e-16 *** ## factor(state.abb)ND -3.4350 0.3977 -8.637 &lt; 2e-16 *** ## factor(state.abb)NE -3.6131 0.3004 -12.029 &lt; 2e-16 *** ## factor(state.abb)NH -3.3591 0.6179 -5.436 5.50e-08 *** ## factor(state.abb)NJ 1.6554 0.4819 3.435 0.000594 *** ## factor(state.abb)NM 5.0624 0.4486 11.284 &lt; 2e-16 *** ## factor(state.abb)NV 7.6758 0.5513 13.922 &lt; 2e-16 *** ## factor(state.abb)NY -10.9051 0.3598 -30.310 &lt; 2e-16 *** ## factor(state.abb)OH -5.6530 0.3398 -16.634 &lt; 2e-16 *** ## factor(state.abb)OK 7.4075 0.2976 24.887 &lt; 2e-16 *** ## factor(state.abb)OR -1.8950 0.4148 -4.568 4.95e-06 *** ## factor(state.abb)RI -7.5116 0.8262 -9.091 &lt; 2e-16 *** ## factor(state.abb)SC 1.6600 0.3413 4.863 1.16e-06 *** ## factor(state.abb)SD -0.9894 0.3298 -3.000 0.002703 ** ## factor(state.abb)TN -2.3999 0.2825 -8.495 &lt; 2e-16 *** ## factor(state.abb)TX 13.7492 0.2509 54.804 &lt; 2e-16 *** ## factor(state.abb)UT -0.3493 0.3995 -0.875 0.381846 ## factor(state.abb)VA -1.2731 0.2690 -4.733 2.23e-06 *** ## factor(state.abb)VT -16.4906 0.5589 -29.506 &lt; 2e-16 *** ## factor(state.abb)WA -1.6794 0.4056 -4.141 3.47e-05 *** ## factor(state.abb)WI -12.2690 0.3039 -40.366 &lt; 2e-16 *** ## factor(state.abb)WV -2.8616 0.3683 -7.770 8.18e-15 *** ## factor(state.abb)WY 2.8195 0.4239 6.652 2.97e-11 *** ## factor(year)2011 -0.7485 0.1797 -4.166 3.11e-05 *** ## factor(year)2012 -2.0058 0.1797 -11.163 &lt; 2e-16 *** ## factor(year)2013 -2.7830 0.1796 -15.493 &lt; 2e-16 *** ## factor(year)2014 -7.5810 0.1797 -42.191 &lt; 2e-16 *** ## factor(year)2015 -11.5876 0.1798 -64.456 &lt; 2e-16 *** ## factor(year)2016 -13.5230 0.1798 -75.222 &lt; 2e-16 *** ## factor(year)2017 -13.1527 0.1797 -73.187 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 20866 degrees of freedom ## Multiple R-squared: 0.8301, Adjusted R-squared: 0.8296 ## F-statistic: 1788 on 57 and 20866 DF, p-value: &lt; 2.2e-16 Let’s cluster the standard error at the state level. I’m going to do this using feols command from fixest package. Using feols you can automatically create the relative time indicators and interact them with the treatment status as in the coding below. reg_es_cluster &lt;- feols(sahieunins138 ~ i(yeararound, expand, ref = -1) | year + state.abb, data = mort_allcauses, cluster = ~state.abb ) summary(reg_es_cluster) ## OLS estimation, Dep. Var.: sahieunins138 ## Observations: 20,924 ## Fixed-effects: year: 8, state.abb: 44 ## Standard-errors: Clustered (state.abb) ## Estimate Std. Error t value Pr(&gt;|t|) ## yeararound::-4:expand 0.402513 0.510512 0.788449 4.3476e-01 ## yeararound::-3:expand -0.185154 0.381186 -0.485731 6.2962e-01 ## yeararound::-2:expand 0.195173 0.320074 0.609773 5.4522e-01 ## yeararound::0:expand -5.790635 1.324081 -4.373323 7.6407e-05 *** ## yeararound::1:expand -7.875889 1.466538 -5.370394 2.9855e-06 *** ## yeararound::2:expand -7.952329 1.469702 -5.410843 2.6106e-06 *** ## yeararound::3:expand -8.263343 1.506539 -5.484985 2.0408e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## RMSE: 4.89924 Adj. R2: 0.829592 ## Within R2: 0.131658 We see that the relative time estimates from 3 and 2 are equal to one another. However, the clustered standard errors are inflated. We can then plot the event study estimates. # Extract coefficients and confidence intervals event_study_results &lt;- fixest::coefplot(reg_es_cluster, main = &quot;Event Study Estimates&quot;) event_study_results[[1]] ## estimate ci_low ci_high estimate_names estimate_names_raw id x y ## yeararound::-4:expand 0.4025128 -0.6270325 1.4320580 yeararound::-4:expand yeararound::-4:expand 1 1 0.4025128 ## yeararound::-3:expand -0.1851537 -0.9538886 0.5835811 yeararound::-3:expand yeararound::-3:expand 1 2 -0.1851537 ## yeararound::-2:expand 0.1951727 -0.4503186 0.8406640 yeararound::-2:expand yeararound::-2:expand 1 3 0.1951727 ## yeararound::0:expand -5.7906348 -8.4608988 -3.1203708 yeararound::0:expand yeararound::0:expand 1 4 -5.7906348 ## yeararound::1:expand -7.8758890 -10.8334453 -4.9183327 yeararound::1:expand yeararound::1:expand 1 5 -7.8758890 ## yeararound::2:expand -7.9523287 -10.9162658 -4.9883916 yeararound::2:expand yeararound::2:expand 1 6 -7.9523287 ## yeararound::3:expand -8.2633434 -11.3015687 -5.2251181 yeararound::3:expand yeararound::3:expand 1 7 -8.2633434 We see that the estimates on \\(\\tau_j\\) for \\(j&lt;-1\\) is close to zero and statistically insignificant at the conventional levels. This provides a suggestive evidence in favor of the parallel trend assumption. However, the estimates following the reform implementation drops drastically, demonstrating the reduction in uninsured rate due to the ACA-Medicaid expansion. Again what is necessary is a subject to debate, which we will stay away from for now.↩︎ "],["two-way-fixed-effect-twfe-revisited.html", "7.11 Two way fixed effect (TWFE) Revisited", " 7.11 Two way fixed effect (TWFE) Revisited We have already seen the TWFE and its importance in accounting for unobserved heterogeneity. The TWFE is heavily linked to the difference-in-differences setting (perhaps mistakenly). However, note that the TWFE estimator is not equal to the DiD estimator unless the treatment effects are homogeneous across both units and time. \\[\\begin{equation} \\label{eq:TWFE} Y_{it} = \\theta_{t} + \\eta_{i} + \\alpha D_{it} + v_{it} \\;.....TWFE \\end{equation}\\] Here, \\(Y_{it}\\) is the outcome of individual \\(i\\) in period \\(t\\) (\\(t \\in \\{1,\\;2,\\;...,\\;T\\}\\)) \\(\\theta_{t}\\) is the time fixed effects; \\(\\eta_{i}\\) is the unit fixed effect \\(D_{it}\\) captures whether individual \\(i\\) is treated in time \\(t\\) Equation above is the TWFE. In two groups and two-period setting, the above equation can be estimated in a number of different ways. Let’s simulate data to look. Assign treatment effect = 20 Data Arrange 1: Demeaning to get rid of \\(\\eta_i\\) from TWFE equation (Within Estimator) Let’s look at the concept behind the within estimator. In the two-period two-group case, TWFE can be written as: \\[\\begin{equation} Y_{i1} = \\theta_{1} + \\eta_{i} + \\alpha D_{i1} + v_{i1} \\nonumber \\\\ Y_{i2} = \\theta_{2} + \\eta_{i} + \\alpha D_{i2} + v_{i2} \\end{equation}\\] where, \\(i\\) is represented by 1 (treatment group) and 0 (untreated group). Adding the sub-equations and dividing by the number of time period \\((T=2)\\) yields: \\[\\begin{equation} \\frac{Y_{i1}+Y_{i2}}{2} = \\frac{\\theta_{1}+\\theta_{2}}{2} + \\frac{2\\eta_{i}}{2} + \\frac{\\alpha (D_{i1}+D_{i2})}{2} + \\frac{v_{i1}+v_{i2}}{2} \\\\ Y_{i} = \\frac{\\theta_{1}+\\theta_{2}}{2} + \\eta_{i} + \\alpha D_{i} + v_{i} \\nonumber \\end{equation}\\] Substracting the above equation from the TWFE yields the following: \\[\\begin{equation} Y_{it}-Y_{i} = \\theta_{t} - \\frac{\\theta_{1}+\\theta_{2}}{2} + \\alpha (D_{it}-D_i) + (v_{it}-v_i) \\end{equation}\\] The code shows data arranging for the within estimator. ########################### # Treatment group ########################### treat_t &lt;- rep(1, 1000) period_t &lt;- rep(c(0, 1), each = 500) id &lt;- rep(seq(1, 500, 1), 2) #for the panel nature of data y_treat &lt;- 20 * period_t + 7 + rnorm(1000, 0, 5) treatdata &lt;- data.frame(treat = treat_t, period = period_t, Y = y_treat, id = id) treatdata &lt;- treatdata %&gt;% mutate(Ytrans = Y - mean(Y), D = treat * period - mean(treat * period)) ########################## # control group ########################## control_t &lt;- rep(0, 1000) period_c &lt;- rep(c(0, 1), each = 500) id &lt;- rep(seq(501, 1000, 1), 2) y_control &lt;- 3 + rnorm(1000, 0, 5) controldata = data.frame(treat = control_t, period = period_c, Y = y_control, id = id) controldata &lt;- controldata %&gt;% mutate(Ytrans = Y - mean(Y), D = treat * period - mean(treat * period)) data = rbind(treatdata, controldata) Data Arrange 2: First differencing Let’s briefly look at the concept behind first differencing. Write TWFE as: \\[\\begin{equation} Y_{i1} = \\theta_{1} + \\eta_{i} + \\alpha D_{i1} + v_{i1} \\nonumber \\\\ Y_{i2} = \\theta_{2} + \\eta_{i} + \\alpha D_{i2} + v_{i2} \\end{equation}\\] for \\(i \\in \\{0,\\;1\\}\\). Then, \\[\\begin{equation} Y_{i2} - Y_{i1} = \\theta_{2} - \\theta_{1} + \\alpha (D_{i2}-D_{i1}) + (v_{i2} - v_{i1}) \\end{equation}\\] The code shows data arranging for the first difference estimator. # First the treated group fd_treat1 &lt;- treatdata %&gt;% filter(period == 0) %&gt;% dplyr::select(-c(&quot;Ytrans&quot;)) colnames(fd_treat1) &lt;- c(&quot;treat1&quot;, &quot;period1&quot;, &quot;Y1&quot;, &quot;id&quot;) fd_treat2 &lt;- treatdata %&gt;% filter(period == 1)%&gt;% dplyr::select(-c(&quot;Ytrans&quot;)) colnames(fd_treat2) &lt;- c(&quot;treat2&quot;, &quot;period2&quot;, &quot;Y2&quot;, &quot;id&quot;) fd_treat &lt;- merge(fd_treat1, fd_treat2, by = &quot;id&quot;, all.x = T) fd_treat &lt;- fd_treat %&gt;% mutate(Y_FD = Y2 - Y1, D = (period2 * treat2) - (period1 * treat1)) # Then the control group fd_control1 &lt;- controldata %&gt;% filter(period == 0) %&gt;% dplyr::select(-c(&quot;Ytrans&quot;)) colnames(fd_control1) &lt;- c(&quot;treat1&quot;, &quot;period1&quot;, &quot;Y1&quot;, &quot;id&quot;) fd_control2 &lt;- controldata %&gt;% filter(period == 1)%&gt;% dplyr::select(-c(&quot;Ytrans&quot;)) colnames(fd_control2) &lt;- c(&quot;treat2&quot;, &quot;period2&quot;, &quot;Y2&quot;, &quot;id&quot;) fd_control &lt;- merge(fd_control1, fd_control2, by = &quot;id&quot;, all.x = T) fd_control &lt;- fd_control %&gt;% mutate(Y_FD = Y2 - Y1, D = (period2 * treat2) - (period1 * treat1)) FDdata = rbind(fd_treat, fd_control) "],["various-ways-of-estimation.html", "7.12 Various ways of estimation", " 7.12 Various ways of estimation Typical Estimation reg1 &lt;- lm(Y ~ treat:period + treat + period, data) summary(reg1) ## ## Call: ## lm(formula = Y ~ treat:period + treat + period, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.830 -3.455 -0.093 3.467 16.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.95751 0.22379 13.216 &lt;2e-16 *** ## treat 4.26159 0.31648 13.466 &lt;2e-16 *** ## period -0.07377 0.31648 -0.233 0.816 ## treat:period 19.95384 0.44757 44.583 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.004 on 1996 degrees of freedom ## Multiple R-squared: 0.8002, Adjusted R-squared: 0.7999 ## F-statistic: 2665 on 3 and 1996 DF, p-value: &lt; 2.2e-16 Within Estimator reg2 &lt;- lm(Ytrans ~ D + period, data) summary(reg2) ## ## Call: ## lm(formula = Ytrans ~ D + period, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.830 -3.455 -0.093 3.467 16.072 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03689 0.19376 0.190 0.849 ## D 19.95384 0.44746 44.594 &lt;2e-16 *** ## period -0.07377 0.31640 -0.233 0.816 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.003 on 1997 degrees of freedom ## Multiple R-squared: 0.6641, Adjusted R-squared: 0.6637 ## F-statistic: 1974 on 2 and 1997 DF, p-value: &lt; 2.2e-16 First Difference reg3 &lt;- lm(Y_FD ~ D, FDdata) summary(reg3) ## ## Call: ## lm(formula = Y_FD ~ D, data = FDdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.074 -4.372 0.223 4.794 21.751 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.07377 0.31216 -0.236 0.813 ## D 19.95384 0.44146 45.200 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.98 on 998 degrees of freedom ## Multiple R-squared: 0.6718, Adjusted R-squared: 0.6715 ## F-statistic: 2043 on 1 and 998 DF, p-value: &lt; 2.2e-16 Imputation method This traces the counterfactual (untreated potential outcome) of the treated group using paths of the untreated group. Using the first difference (or within transformation): \\[\\begin{equation} Y_{i2}(0) - Y_{i1}(0) = \\theta_{2} - \\theta_{1} + v_{i2}-v_{i1} \\nonumber \\\\ \\Delta Y_{it}(0) = \\theta_t + \\Delta v_{it} \\end{equation}\\] where, \\(\\theta_{t-1}\\) is normalized to 0. Estimate the above equation using only the untreated group and estimate \\(\\hat{\\theta_t}\\). This is the time trend in the untreated group. The parallel trend assumption states that the outcome in treated group would have moved in a similar way to the untreated group in absence of the treatment. So, let’s use \\(\\hat{\\theta_t}\\) to adjust for the pathway in the treated group and find the potential outcome in the treated group in absence of the treatment. Note that \\(\\hat{\\theta_t} = \\frac{1}{n_0}\\sum_1^n (1-D_i) \\Delta Y_{it}\\). \\[\\begin{equation} \\hat{Y_{it}(0)} = Y_{it-1} + \\hat{\\theta_t} \\end{equation}\\] Then write \\(ATT_{imp}\\) as: \\[\\begin{equation} ATT_{imp} = \\frac{1}{n_1}\\sum_{1}^{n}D_{i}(Y_{it}-\\hat{Y_{it}(0)}) \\\\ = \\frac{1}{n_1}\\sum_{1}^{n}D_{i}(Y_{it}-({Y_{it-1}+\\hat{\\theta_{t}}}) ) \\\\ = \\frac{1}{n_1}\\sum_{1}^{n}D_{i}(Y_{it}-({Y_{it-1}}) - \\frac{1}{n_0}\\sum_{1}^{n}(1-D_{i})(Y_{it}-({Y_{it-1}}) \\\\ = \\frac{1}{n_1}\\sum_{1}^{n}D_{i}\\Delta Y_{it} - \\frac{1}{n_0}\\sum_{1}^{n}(1-D_{i}) \\Delta Y_{it} \\end{equation}\\] reg &lt;- lm(Y_FD ~ period2, subset(FDdata, treat1 == 0)) FDdata$yhattreat = FDdata$Y1 + reg[[1]][[1]] FDdata$imp = FDdata$Y2 - FDdata$yhattreat mean(FDdata$imp[FDdata$treat1 == 1]) ## [1] 19.95384 Frisch-Waugh Theorem "],["multi-period-multi-group-and-variation-in-treatment-timing.html", "7.13 Multi Period, Multi Group and Variation in Treatment Timing", " 7.13 Multi Period, Multi Group and Variation in Treatment Timing trueeffect &lt;- 10 intercep &lt;- 10 N &lt;- 5000 T &lt;- 20 early &lt;- 5 late &lt;- 15 datagen &lt;- function(T, N, group){ timeT &lt;- rep(1:T, each = N) treatT &lt;- rep(1, length(timeT)) groupT &lt;- rep(group, length(timeT)) df &lt;- data.frame(time = timeT, treat = treatT, group = groupT) return(df) } dftreat &lt;- datagen(T, N, 1) # early treatment dftreat2 &lt;- datagen(T, N, 2) # late treatment dfuntreat &lt;- datagen(T, N, 3) # untreated data &lt;- rbind(dftreat, dftreat2, dfuntreat) # generating policy variables data &lt;- data %&gt;% mutate(policy = 0, policy = ifelse(group == 1 &amp; time &gt; early, 1, policy), policy = ifelse(group == 2 &amp; time &gt; late, 1, policy), dumtreat1 = ifelse(group == 1, 1, 0), dumtreat2 = ifelse(group == 2, 1, 0), dumtreat3 = ifelse(group == 3, 1, 0)) e &lt;- rnorm(nrow(data), 0, 5) data &lt;- data %&gt;% mutate(Y = 1 + trueeffect*dumtreat1*policy + trueeffect*dumtreat2*policy + time + dumtreat1*2 + dumtreat2*4 + e, Ypot = 1 + 1*dumtreat1*policy + 1*dumtreat2*policy + time + dumtreat1*2 + dumtreat2*4 + e) datasum &lt;- data.frame(data %&gt;% group_by(group, time) %&gt;% summarise(meanY = mean(Y), meanYpot = mean(Ypot)) ) ## `summarise()` has grouped output by &#39;group&#39;. You can override using the `.groups` argument. # data for potential outcome datasumpot &lt;- datasum %&gt;% dplyr::select(c(group, time, meanYpot)) %&gt;% mutate(group = 10*group) datasum &lt;- datasum %&gt;% dplyr::select(-c(meanYpot)) colnames(datasumpot) &lt;- c(&quot;group&quot;, &quot;time&quot;, &quot;meanY&quot;) datasum &lt;- rbind(datasum, datasumpot) vlines &lt;- data.frame(xint = c(6, 16)) datasum$group = factor(datasum$group) ggplot(datasum, aes(x = time, y = meanY, group = group)) + geom_line(aes(linetype = group, color = group),size = 2) + #geom_point(aes(shape = group, size = 3)) + scale_linetype_manual(name = &quot;Linetype&quot;,values = c(&quot;1&quot; = 1, &quot;2&quot; = 1, &quot;3&quot; = 1, &quot;10&quot; = 2, &quot;20&quot; = 2, &quot;30&quot; = 2), guide = &quot;none&quot;) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(color = &quot;black&quot;), legend.position = &quot;none&quot;) + xlab(&quot;period&quot;) + ylab(&quot;Outcome&quot;) + geom_vline(data = vlines, aes(xintercept = xint), linetype = &quot;dashed&quot;) + annotate(x = c(5.2, 15.2, 2, 9, 17), y = c(0, 0, 35, 35, 35), label = c(bquote(t[k]^&quot;*&quot;), bquote(t[l]^&quot;*&quot;), &quot;PRE(k)&quot;, &quot;MID(k,l)&quot;, &quot;POST(l)&quot;), geom = &quot;text&quot;, parse = TRUE) The figure above depicts the case of three groups: \\(i)\\) early treated (treatment starting from the \\(6^{th}\\) period, \\(t_{k}^{*}\\)); \\(ii)\\) late treated (\\(16^{th}\\) period, \\(t_{k}^{*}\\)); and untreated group. The figure provides an example of staggered treatment adoption. This means that units are treated at different points in time and once treated they are always treated. The solid lines represent observed outcomes, whereas the dotted lines are the potential outcomes. For the untreated group and not yet treated periods the observed outcomes are also the potential outcomes. To go further, let’s introduce some notations. \\(T\\) is defined as the number of periods. Groups are defined based on the timing of the treatment of the unit. \\(G_{i}\\) indicates the group of the unit and all set of groups include \\(\\zeta \\in \\{2,\\; ...., T,\\;T+1\\}\\). Units that are treated in period 1 are dropped in this setup. One reason to do so is that no pretreatment outcomes are observed for this group, which means that it is not possible to use the parallel trend assumption. T+1 group is used to denote units that remain untreated throughout the period (never treated group). It is possible that eventually all units are treated. In this case, one can limit the data to time period with not yet treated group. \\(Y_{it}(g)\\) is denoted as the outcome of unit \\(i\\) observed at time \\(t\\) when the unit was treated in period \\(g\\). \\(Y_{it}(0)\\) is the potential outcome of unit \\(i\\) at time \\(t\\) had the unit not been treated. There are a few assumptions that we need to consider. There are more assumputions highlighted in (callaway2022?) paper. But I will focus on two main ones. Staggered Treatment Assignment. For all units and for all \\(t = 2,\\;3,...,\\;T\\), \\(D_{it-1}=1\\) implies \\(D_{it}=1\\). This means that once a unit is treated, it remains treated. Parallel Trend Assumption for Multi Period and Variation in Treatment Timing. For all \\(t = 2,\\;3,...,\\;T\\) \\[\\begin{equation} E[\\Delta Y_{it}(0)|G=g]=E[\\Delta Y_{it}(0)] \\end{equation}\\] This basically says that the average pathway of group g if it was untreated (potential outcome) would be same as the average pathway of other group (untreated) for each time period. This holds for each group \\(g \\in \\zeta\\). In other words, this can be thought as an extension of the parallel trend assumption for \\(2 \\times 2\\), but in this case it should hold for each \\(g \\in \\zeta\\) and for each time period \\(t\\). The parallel trend described in the assumption above is highly general, meaning that it assumes parallel trends for any groups. However, this may not be the case as groups can be very different in observed characteristics and can have different pathways in absence of the treatment. Hence, there are other versions of parallel trend assumption to consider: a. parallel trend holds only for groups with similar observed characteristics b. parallel trend holds only for certain time periods c. parallel trend holds only for those group who ever participate in the treatment "],["problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html", "7.14 Problem with TWFE in Multiple Group with Treatment Timing Variation", " 7.14 Problem with TWFE in Multiple Group with Treatment Timing Variation To understand what TWFE is estimating, write the potential outcome as: \\[\\begin{equation} Y_{it}(0) = \\theta_{t} + \\eta_{i} + v_{it}..... (A) \\end{equation}\\] Then write the TWFE using the potential outcome As \\[\\begin{equation} Y_{it} = Y_{it}(0) + 1\\{t \\geq G_{i}\\}(Y_{it}(G_i)-Y_{it}(0))..... (B) \\end{equation}\\] Note that substituting \\((A)\\) into \\((B)\\) gives the TWFE. Let’s look at the terms in the above equation closely. $ 1{t G_{i}}$ represents \\(D_{it}\\) and \\((Y_{it}(G_i)-Y_{it}(0))\\) is the \\(\\alpha\\) parameter in equation TWFE. The equality \\((Y_{it}(G_i)-Y_{it}(0))=\\alpha\\) indicates that the effects of being treated is the same for each group in \\(\\zeta\\) and the effects do not vary over time. In other words, the effects of treatment are homogeneous across units and over time. "],["what-is-twfe-estimating-when-there-is-treatment-timing-variation.html", "7.15 What is TWFE Estimating when there is Treatment Timing Variation?", " 7.15 What is TWFE Estimating when there is Treatment Timing Variation? Using (goodman2021?)’s decomposition the TWFEDD estimate \\(\\hat{\\alpha}^{DD}\\) can be written as: \\[\\begin{equation} \\hat{\\alpha}^{DD} = \\sum_{k \\neq U} s_{ku} \\hat{\\alpha}^{2 \\times 2}_{k} + \\underbrace{\\sum_{k \\neq U} \\sum_{l&gt;k}[s^k_{kl}\\hat{\\alpha}_{kl}^{2 \\times 2,k}+ s^l_{kl}\\hat{\\alpha}_{kl}^{2 \\times 2,l}]}_\\text{timing only estimator}....TWFE(decomposition) \\end{equation}\\] Here, \\(\\hat{\\alpha}^{2 \\times 2}_{kU} = [\\bar{y}_k^{Post(k)}-\\bar{y}_k^{Pre(k)}] - [\\bar{y}_U^{Post(k)}-\\bar{y}_U^{Pre(k)}]\\); this is when group \\(k\\) is compared to untreated group \\(U\\). \\(\\hat{\\alpha}^{2 \\times 2, k}_{kl} = [\\bar{y}_k^{MID(k,l)}-\\bar{y}_k^{Pre(k)}] - [\\bar{y}_l^{MID(k,l)}-\\bar{y}_l^{Pre(k)}]\\); this is when early group \\((k)\\) is compared to late treated group \\((l)\\) during the period when group \\(l\\) is not yet treated. \\(\\hat{\\alpha}^{2 \\times 2, l}_{kl} = [\\bar{y}_l^{POST(l)}-\\bar{y}_k^{MID(k,l)}] - [\\bar{y}_k^{POST(l)}-\\bar{y}_k^{MID(k,l)}]\\); this is when late group \\((l)\\) is compared to early treated group \\((k)\\) using the window between \\(MID(k,l)\\) and \\(POST(l)\\) when the treatment status of group \\((k)\\) does not change. Here, early treated group is being used as the control group. Note that the second block in TWFE(decomposition) uses variation in timing of treatment to identify the effects. Each group serves as the control to the other during the window when the treatment status do not change. \\(s_{ku}\\), \\(s_{kl}^{k}\\), and $s_{kl}^{l} are the weights placed on the estimates that compares: \\(i)\\) treated to untreated units (giving rise to 2 \\(2 \\times 2\\) DD estimates); \\(ii)\\) early treated to late treated in between \\(PRE(k)\\) and \\(MID(k,l)\\) window; and \\(iii)\\) late treated to early treated between \\(MID(k,l)\\) to \\(POST(l)\\) window, respectively. (goodman2021?) presents the following interpretation for the weights: \\[\\begin{equation} s_{kU} = \\frac{(n_k + n_U)^2 \\overbrace{n_{kU}(1-n_{kU})\\bar{D}_k(1-\\bar{D}_k)}^{\\hat{V}^{D}_{kU}} }{\\hat{V}^{D}} \\end{equation}\\] \\[\\begin{equation} s_{kl}^{k} = \\frac{((n_k + n_l)(1-\\bar{D}_l))^2 \\overbrace{n_{kl}(1-n_{kl})\\frac{\\bar{D}_k-\\bar{D}_l}{1-\\bar{D}_l}\\frac{1-\\bar{D}_k}{1-\\bar{D}_l}}^{\\hat{V}^{D,k}_{kl}} }{\\hat{V}^{D}} \\end{equation}\\] and \\[\\begin{equation} s_{kl}^{l} = \\frac{((n_k + n_l)\\bar{D}_k)^2 \\overbrace{n_{kl}(1-n_{kl})\\frac{\\bar{D}_l}{\\bar{D}_k}\\frac{\\bar{D}_k-\\bar{D}_l}{\\bar{D}_k}}^{\\hat{V}^{D,l}_{kl}} }{\\hat{V}^{D}} \\end{equation}\\] where, \\[\\begin{equation} \\sum_{k \\neq U} s_{ku} + \\sum_{k \\neq l} \\sum_{l&gt;k}(s_{kl}^{k} + s_{kl}^{l}) = 1 \\end{equation}\\] The TWFE estimate depends on weight implied to each of the \\(2\\times 2\\) DD estimate. The weights depend on the sample size of the group that is treated as well as the untreated group. Note that the weight also depends on the variance of the subsample based on the treated vs untreated groups. For instance, \\({\\hat{V}^{D,k}_{kl}}\\) denotes the variance in \\(D_i\\) for the subsample defined by groups \\(k\\) and \\(l\\), for the period \\(Pre\\) and \\(Mid(k,\\;l)\\). Let us take a look at \\(\\bar{D}_k(1-\\bar{D}_k)\\), \\(\\frac{\\bar{D}_k-\\bar{D}_l}{1-\\bar{D}_l}\\frac{1-\\bar{D}_k}{1-\\bar{D}_l}\\), and \\(\\frac{\\bar{D}_l}{\\bar{D}_k}\\frac{\\bar{D}_k-\\bar{D}_l}{\\bar{D}_k}\\) more closely. It is seen that these values are maximized when treatment occurs at the middle of the time window the researcher uses. In other words, the TWFE estimate depends on when the treatment occurs in the panel; if there is heterogeneous effects between groups, these effects are going to be emphasized (or de-emphasized) depending on wherein the given time window the treatment falls. This can be explained using a simulation that uses treatment effects of 10 and 15 for the early and late treated groups, respectively. The timing window comprise of 20 periods; the treatment timing of the early treated group is fixed at period 9, whereas the treatment timing for the late treated group is allowed to vary backwards from period 16 to 10. The figure shows that the higher treatment effect for the late treated group is supressed when the treatment timing is towards the end of the panel; the treatment effect increases as the treatment period for the late treated group approaches to the middle of the panel. When the treatment for the late period occurs at the middle of the panel (period 10), the estimate is very close to 12.5 – the average effect of early and late treated groups. Note that \\(K\\) timing group yields \\(K^2 - K\\) \\(2\\times 2\\) “timing-only” DD estimates (\\(\\hat{\\beta}^{2\\times 2 k}_{kl}\\) or \\(\\hat{\\beta}^{2\\times 2 l}_{kl}\\)); one untreated unit (throught out the time window) yields \\(K^2\\) DD estimates. "],["assumptions-governing-twfedd-estimate.html", "7.16 Assumptions governing TWFEDD estimate", " 7.16 Assumptions governing TWFEDD estimate The TWFEDD estimate measures weighted average of all possible \\(2 \\times 2\\) DD average treatment effects on treated. In the case of groups being defined as treatment timing \\(k,\\;g,\\;U\\), the \\(2\\times 2\\) DD estimates can be written as: \\[\\begin{equation} \\hat{\\alpha}^{2\\times 2}_{kU} = [\\bar{y}_k^{POST(k)} - \\bar{y}_k^{PRE(k)}] - [\\bar{y}_U^{POST(k)} - \\bar{y}_U^{PRE(k)}] \\end{equation}\\] \\[\\begin{equation} \\hat{\\alpha}^{2\\times 2 k}_{kl} = [\\bar{y}_k^{MID(k,l)} - \\bar{y}_k^{PRE(k)}] - [\\bar{y}_l^{MID(k,l)} - \\bar{y}_l^{PRE(k)}] \\end{equation}\\] \\[\\begin{equation} \\hat{\\alpha}^{2\\times 2 l}_{kl} = [\\bar{y}_l^{POST(l)} - \\bar{y}_l^{MID(k,l)}] - [\\bar{y}_k^{POST(l)} - \\bar{y}_k^{MID(k,l)}] \\end{equation}\\] Now, let us express the estimates based on the counterfactuals. First, write \\[\\begin{equation} y_{it} = D_{it}Y_{it}(t_i) + (1-D_{it})Y_{0} \\end{equation}\\] where, \\(Y_{it}\\) is the outcome of unit \\(i\\) in time \\(t\\) and \\(Y_{0}\\) is the counterfactual outcome. Following (callaway2022?) define ATT for group \\(k\\) at time period \\(\\tau \\geq k\\) as \\(ATT_{k}(\\tau) = E[Y_{i\\tau}(t^{*}_{k}) - Y_{i\\tau}(0)|t_i = k]\\) Now, let us define \\(W\\) as the date range or windows with \\(T_W\\) periods. In practice, \\(W\\) represents the post treatment window in \\(2 \\times 2\\) DD. But note that there are \\(T_{W}\\) periods. In our case above, \\(W\\) for group \\(k\\) represents the \\(MID(k,l)\\) plus the \\(POST(l)\\) windows and \\(T_{W} = 2\\). Group \\(k\\) is treated in two windows – \\(MID(k,l)\\) and \\(POST(l)\\); hence, the \\(ATT_{k}(W)\\) is just the average of ATTs across the windows. \\[\\begin{equation} ATT_{k}(W) = \\frac{1}{T_{W}} \\sum_{t \\in W} E[Y_{it}(k)-Y_{it}(0)|t_{i}=k] \\end{equation}\\] Now, define the change in average untreated potential outcome between pre and the post period as: \\[\\begin{equation} \\Delta Y_{k}^{0}(W_1, W_0) = \\frac{1}{T_{W_1}} \\sum_{t \\in W_1} E[Y_{it}(0)|t_{i}=k] - \\frac{1}{T_{W_0}} \\sum_{t \\in W_1} E[Y_{it}(0)|t_{i}=k] \\end{equation}\\] Using this notation, the \\(2 \\times 2\\) \\(\\hat{\\beta}\\)s can be written as: \\[\\begin{equation} \\hat{\\alpha}_k^{2\\times 2} = ATT_{k}^{(POST(k))} + \\overbrace{[ \\Delta Y_{k}^{0}(POST(k),PRE(k)) - \\Delta Y_{U}^{0}(POST(k),PRE(k))}^{parallel\\;trend}] \\end{equation}\\] \\[\\begin{equation} \\hat{\\alpha}_{kl}^{2\\times 2 k} = ATT_{k}^{(MID(k,l))} + \\overbrace{[ \\Delta Y_{k}^{0}(MID(k,l),PRE(k)) - \\Delta Y_{U}^{0}(MID(k,l),PRE(k))}^{parallel\\;trend}] \\end{equation}\\] \\[\\begin{equation} \\hat{\\alpha}_{kl}^{2\\times 2 l} = ATT_{l}^{(MID(k,l))} + \\overbrace{[ \\Delta Y_{l}^{0}(POST(l),MID(k,l)) - \\Delta Y_{k}^{0}(POST(l),MID(k,l))}^{parallel\\;trend}] + [ATT_k(MID(k,l))-ATT_k(POST(l))] \\end{equation}\\] While \\(\\hat{\\alpha}_{k}^{2\\times 2}\\) and \\(\\hat{\\alpha}_{k}^{2 \\times 2}\\) depends on the parallel trend assumption, \\(\\hat{\\alpha}_{k}^{2 \\times 2}\\) also depends on the difference between group \\(k&#39;s\\) \\(ATT\\) in \\(MID(k,l)\\) and \\(POST(l)\\). This is because the late treatment group is compared also with the early treatment group, and if there is presence of treatment dynamic in early treated group, this will show up in \\(\\hat{\\alpha}_{kl}^{2\\times 2 l}\\). Substituting the expressions of \\(\\hat{\\alpha}_{k}^{2\\times 2}\\), \\(\\hat{\\alpha}_{k}^{2 \\times 2}\\), and \\(\\hat{\\alpha}_{kl}^{2\\times 2 l}\\) into the TWFE decomposition yields the following: \\[\\begin{equation} \\plim_{N \\to \\infty} \\hat{\\alpha} = \\alpha = VWATT + VWCT + \\Delta ATT \\end{equation}\\] where, VWATT is the variance weighted average treatment effect on the treated; VWCT is the variance weighted common trends; and \\(\\Delta ATT\\) is the change in average treatment effect on treated of group \\(k\\) between the \\(Mid(k,l)\\) and \\(Posk(l)\\) period (treatment effect dynamics or heterogeneity over time). An intuition is that parallel trend assumption justifies comparing treated vs. untreated (or not yet treated groups), and deviation in pathways of outcome can be attributed to treatement. As such, (callaway2022?) refers to these groups (untreated and not yet treated) as “good comparison” groups. Now, early treatment group, that serves as the comparison group for the late treated group, can be a “bad comparison” if the treatment effect (of early treated group) varies with time. "],["how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html", "7.17 How Does Treatment Effect Heterogeneity in Time Affect TWFE?", " 7.17 How Does Treatment Effect Heterogeneity in Time Affect TWFE? As we have seen earlier, the TWFE estimator is baised even in cases where the parallel trend assumption holds given that the treatment effect varies over time. As of VWATT, this is a weighted version of each \\(2 \\times 2\\) DD estimate and the weights are dependent on treatment timing as well as treatment heterogeneity across groups. For example, states with high anti-smoking sentiments have increases cigarette taxes earlier; increases in cigarette taxes can affect populace living in state with higher anti-smoking sentiments more compared to states with low anti smoking sentiments. Now given that states with relatively lower of anti-smoking sentiments increased cigarette taxes later (and if this falls in the middle of the panel), the VWATT will provide higher weights to the states with lower anti-smoking sentiments. In this case, the estimate of cigarette taxes will be underestimated. "],["causal-forest.html", "8 Causal Forest ", " 8 Causal Forest "],["introduction-1.html", "8.1 Introduction", " 8.1 Introduction The generalized random forest is a method that is quite flexible in estimating the quantity of interest. The theory of it is built using the moment criterion: \\(E[\\psi_{\\theta_i, \\; \\upsilon_i} (O_i) | X_i] = 0, \\; for \\; all \\; x \\; in \\; \\chi\\) Getting down to the nuts and bolts of the theory is beyond the scope of this write-up. Rather, we would want to take a closer look at causal forests – a component of GRF framework. "],["summary-of-grf.html", "8.2 Summary of GRF", " 8.2 Summary of GRF It seeks a generalized way to conduct causal inference under a non-parametric framework. GRF relies on random forest. Methods developed to aid causal inference such as: \\(i)\\) randomized controlled trial, \\(ii)\\) comparison between treatment and control units under unconfoundedness assumption, \\(iii)\\) difference-in-differences, and \\(iv)\\) panel data methods; can fit into GRF framework. To do so, one needs to feed in the method-specific encoding into the GRF framework (to guide the splitting process). "],["motivation-for-causal-forests.html", "8.3 Motivation for Causal Forests", " 8.3 Motivation for Causal Forests Let’s expand on estimating the average treatment effect of a treatment intervention \\(W\\). The specifics are listed as: \\(W_i \\in \\{0, \\; 1\\}\\): treatment intervention \\(X_i\\): covariates \\(Y_i\\): response/outcome In the parametric framework \\(\\tau\\), the treatment effect, is estimated using the following specification: \\(Y_i = \\tau W_i + \\beta_1 X_i + \\epsilon_i\\) The validity of \\(\\hat{\\tau}\\) as a causal estimand is justified under the following three assumptions. Unconfoundedness: \\(Y^{(0)}_i, \\; Y^{(1)}_i \\perp W_i | X_i\\). Treatment assignment is independent of the potential outcome once conditioned on the covariates. In other words, controling for covariates makes the treatment assignment as good as random. \\(X_i\\)s influence \\(Y_i\\)s in a linear way. The treatment effect is homogeneous. Assumption 1 is the identification assumption. In the traditional sense, one can control for \\(X_i\\)s in the regression framework and argue that this assumption is met. Even if all \\(X\\)s that influence the treatment assignment are observed (this is the assumption that we make throughout), we are unsure how \\(X\\)s affect the treatment. Often \\(X\\)s can affect treatment in a non-linear way. Assumptions 2 and 3 can be questioned and relaxed. One can let data determine the way \\(X\\) needs to be incorporated in the model specification (relaxing assumption 2). Moreover, treatment effects can vary across some covariates (relaxing assumption 3). First, lets relax assumption 2. This leads to the following partially linear model: \\(Y_i = \\tau W_i + f(X_i) + \\epsilon_i \\;............. equation 1\\) where, \\(f\\) is a function that maps out how \\(X\\) affects \\(Y\\). However, we don’t know \\(f\\) in practice. So, how do we go about estimating \\(\\tau\\)? The causal forest framework under GRF connects the old-school literature of causal inference with ML methods. Robinson (1988) shows that if two intermediate (nuiscance) objects, \\(e(X_i)\\) and \\(m(X_i)\\) are known, one can estimate \\(\\tau\\). The causal forest framework under GRF utilizes this result. Here: \\(e(X_i)\\) is the propensity score; the probability of being treated. \\(E[W_i| X_i = x]\\) \\(m(X_i)\\) is the conditional mean of \\(Y\\). \\(E[Y_i | X_i = x] = f(x) + \\tau e(x)\\) Demeaning equation 1 (substracting \\(m(x)\\)) gives the following residual-on-residual regression: \\(Y_i - m(x) = \\tau (W_i - e(x)) + \\epsilon \\; .............. equation 2\\) Intuition for equation (2) proceeds as follow. Note that \\(m(x)\\) is the conditional mean of Y given \\(X_i = x\\).10 This means that units with similar \\(X\\)s will have similar estimates for \\(m(x)\\) in \\(W=\\{0, \\; 1\\}\\), which would mean that estimates on \\(e(x)\\) would also be similar for these units across both treatment and control group. Now, consider that the treatment is positive; this will show up in \\(Y_i\\). \\(Y_i - m(x)\\) will be higher for \\(W=1\\) compared to \\(W=0\\) for similar estimates of \\(m(x)\\). On the other side, \\(W_i - e(x)\\) is positive for \\(W=1\\) and negative for \\(W=0\\) for similar estimates of \\(e(x)\\). Such variations in the left and right hand side quantities will allow to capture postive estimates on \\(\\tau\\). To gain ML methods are used to estimate \\(m(x)\\) and \\(e(x)\\) and residual-on-residual regression is used estimate \\(\\tau\\). It turns out that even noisy estimates of \\(e(x)\\) and \\(m(x)\\) can give ``ok” estimate of \\(\\tau\\). How to estimate \\(m(x)\\) and \\(e(x)\\)? Use ML methods (boosting; random forest) Use cross-fitting for prediction. prediction of observation \\(i&#39;s\\) outcome &amp; treatment assignment is obtained without using the observation ``\\(i\\)“. Lets take a look at residual-on-residual in the case of homogeneous treatment effect. # generate data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) # Generate W and Y W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) prob &lt;- 0.4 + 0.2 * (X[, 1] &gt; 0) Y &lt;- 2.5 * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n) # Train regression forests mx &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) ex &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) Wcen &lt;- W - ex$predictions Ycen &lt;- Y - mx$predictions reg &lt;- summary(lm(Ycen ~ Wcen)) reg ## ## Call: ## lm(formula = Ycen ~ Wcen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6656 -0.7147 -0.0139 0.7059 3.9119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.005379 0.023591 -0.228 0.82 ## Wcen 2.458187 0.047913 51.305 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.055 on 1998 degrees of freedom ## Multiple R-squared: 0.5685, Adjusted R-squared: 0.5683 ## F-statistic: 2632 on 1 and 1998 DF, p-value: &lt; 2.2e-16 print(paste0(&quot;The treatment effect estimate based on residual-on-residual regression is: &quot;, coefficients(reg)[2])) ## [1] &quot;The treatment effect estimate based on residual-on-residual regression is: 2.4581869361891&quot; print(paste0(&quot;The true treatment effect is: &quot;, 2.5)) ## [1] &quot;The true treatment effect is: 2.5&quot; We can also think of \\(m(x)\\) as the case when we ignore \\(W\\), although we know that treatment took place. This way, \\(m(x) = \\mu_{0}(x) + e(x)\\tau\\), where \\(\\mu_{0}(x)\\) is the baseline conditional expectation without the treatment. This makes it easy to see that units with similar features will have similar estimates of \\(m(x)\\).↩︎ "],["causal-forest-1.html", "8.4 Causal Forest", " 8.4 Causal Forest Both regression and causal forests consist of: 1) Building phase; and 2) estimation phase. The intuition regarding the regression/causal forest can be gleaned using the following figure. Figure 8.1: Figure 1. Adaptive weights In this simple case, the sample is partitioned into \\(N_1\\) and \\(N_2\\) neighborhoods accorinng to the splitting rule that the squared difference in sub-sample specific treatment effect is the maximum, i.e., \\(n_{N_1}n_{N_2}(\\tau_{N_1} - \\tau_{N_2})^2\\) is the maximum. This by construction leads to constant treatment effect in the neighborhood, while the effects may vary across the neighborhoods. This intuition allows us to relax assumption 3, and re-write the partially linear estimation framework as: \\(Y_i = \\tau(x) W_i + f(X_i) + \\epsilon_i\\). Here the estimate of the treatment effect \\(\\tau\\) is allowed to vary with the test point \\(x\\). In reference to Figure 1 above, \\(N_1\\) and \\(N_2\\) are neighborhoods where treatment effects are constant. To estimate the treatment effect of the test point \\(x\\), \\(\\tau(x)\\), we would run a weighted residual-on-residual regression of the form. \\(\\tau(x) := lm(Y_i - m(X_i)^{-i} \\sim \\tau(W_i - e(X_i)^{-i}), \\; weights = 1\\{X_i \\in N(x)\\}\\) where \\(m(X_i)^{-i}\\) and \\(e(X_i)^{-i}\\) are obtained from cross-fitting. The weights play a pivotal role here and takes a value 1 if \\(X_i\\) belongs to the same neighborhoods as \\(x\\). In the above figure, examples in \\(N_2\\) receive non-zero weight while those in \\(N_1\\) receive zero weight. However, this example only pertains to a tree. But we’d want to build a forest and apply the same analogy. Adaptive weights. The forest consists of \\(B\\) trees, so the weights for each \\(X_i\\) pertaining to the test point \\(x\\) is based off of all \\(B\\) trees. The causal forest utilizes adaptive weights using random forests. The tree specific weight for an example \\(i\\) at the \\(b^{th}\\) tree is given as: \\(\\alpha_{ib}(x) = \\frac{1(X_i \\in L_{b}(x))}{|L_{b}(x)|}\\), where \\(L(x)\\) is the leaf (neighborhood) that consist of the test sample \\(x\\). The forest specific weight for an example \\(i\\) is given as: \\(\\alpha_{i}(x) = \\frac{1}{B} \\sum_{b = 1}^{B} \\frac{1(X_i \\in L(x))}{|L(x)|}\\) It tracks the fraction of times an obsevation \\(i\\) falls on the same leaf as \\(x\\) in the course of the forest. Simply, it shows how similar \\(i\\) is to \\(x\\). Regression Forest. It utilizes the adaptive weights given to an example \\(i\\) (\\(i = \\{1, \\; 2, \\; ..., N\\}\\)) and constructs a weighted average to form the prediction of \\(x\\). The prediction for \\(x\\) based on the regression forest is: \\(\\hat{\\mu}(x) = \\frac{1}{B}\\sum_{i = 1}^{N} \\sum_{b=1}^{B} Y_{i} \\frac{1(X_i \\in L_{b}(x)}{|L_b(x)|}\\) \\(= \\sum_{i = 1}^{N} Y_{i} \\alpha_{i}\\) Note that this is different from the traditional prediction from the random forest that averages predictions from each tree. \\(\\hat{\\mu}(x.trad) = \\sum_{b = 1}^{B} \\frac{\\hat{Y}_b}{B}\\) Causal Forest. Causal forest is analogous to the regression forest in a sense that the target is \\(\\tau(x)\\) rather than \\(\\mu(x)\\). Conceptually the difference is encoded in the splitting criteria. While splitting, regression forest is based on the criterion: \\(\\max n_{N_1} n_{N_2}(\\mu_{N_1} - \\mu_{N_2})^2\\), whereas the causal forest is based on \\(\\max n_{N_1} n_{N_2}(\\tau_{N_1} - \\tau_{N_2})^2\\). In a world with infinite computing power, for each potential axis aligned split that extends from the parent node, one would estimate treatment effects at two of the child nodes (\\(\\tau_{L}\\) and \\(\\tau_{R}\\)) and go for the split that maximizes the squared difference between child specific treatment effects. However, in practice this is highly computationally demanding and infeasible. The application of causal forest estimates \\(\\tau_{P}\\) at the parent node and uses the gradient based function to guide the split. At each (parent) node the treatment effect is estimated only once. Once the vector of weights are determined for \\(i\\)s, the following residual-on-residual is ran: \\(\\tau(x) := lm(Y_i - m(X_i^{-i}) \\sim \\tau(x)(W_i - e(X_i)^{-i}), \\; weights = \\alpha_i(x)\\) This can be broken down as: Estimate \\(m^{-i}(X_i)\\) and \\(e^{-i}(X_i)\\) using random forest. Then estimate \\(\\alpha_i(x)\\). For each new sample point \\(x\\), a vector of weight will be determined based on adaptive weighting scheme of the random forest. Note that the weights will change for each new test point. Run a weighted residual-on-residual regression given by the equation above. "],["an-example-of-causal-forest.html", "8.5 An example of causal forest", " 8.5 An example of causal forest rm(list = ls()) library(devtools) ## Loading required package: usethis #devtools::install_github(&quot;grf-labs/grf&quot;, subdir = &quot;r-package/grf&quot;) library(grf) library(ggplot2) # generate data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) # Train a causal forest. W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n) # Train a causal forest c.forest &lt;- causal_forest(X, Y, W) # predict using the training data using out-of-bag prediction tau.hat.oob &lt;- predict(c.forest) hist(tau.hat.oob$predictions) # Estimate treatment effects for the test sample tau.hat &lt;- predict(c.forest, X.test) plot(X.test[, 1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2), xlab = &quot;x&quot;, ylab = &quot;tau&quot;, type = &quot;l&quot;) lines(X.test[, 1], pmax(0, X.test[, 1]), col = 2, lty = 2) # estimate conditional average treatment effect (CATE) on the full sample cate &lt;- average_treatment_effect(c.forest, target.sample = &quot;all&quot;) print(paste(&quot;Conditinal Average Treatment Effect (CATE) is: &quot;, cate[[1]])) ## [1] &quot;Conditinal Average Treatment Effect (CATE) is: 0.405834044743919&quot; # estimate conditional average treatment effect on treated catt &lt;- average_treatment_effect(c.forest, target.sample = &quot;treated&quot;) paste(&quot;Conditional Average Treatment Effect on the Treated (CATT)&quot;, catt[[1]]) ## [1] &quot;Conditional Average Treatment Effect on the Treated (CATT) 0.492310300523608&quot; # Add confidence intervals for heterogeneous treatment effects; growing more trees recommended tau.forest &lt;- causal_forest(X, Y, W, num.trees = 4000) tau.hat &lt;- predict(tau.forest, X.test, estimate.variance = TRUE) # for the test sample ul &lt;- tau.hat$predictions + 1.96 * sqrt(tau.hat$variance.estimates) ll &lt;- tau.hat$predictions - 1.96 * sqrt(tau.hat$variance.estimates) tau.hat$ul &lt;- ul tau.hat$ll &lt;- ll tau.hat$X.test &lt;- X.test[,1] ggplot(data = tau.hat, aes(x = X.test, y = predictions)) + geom_ribbon(aes(ymin = ll, ymax = ul), fill = &quot;grey70&quot;) + geom_line(aes(y = predictions)) + theme_bw() ###################################################### # # # In some cases prefitting Y and W separately may # be helpful. Say they use different covariates. # ###################################################### # Generate a new data n &lt;- 4000 p &lt;- 20 X &lt;- matrix(rnorm(n * p), n, p) TAU &lt;- 1 / (1 + exp(-X[, 3])) W &lt;- rbinom(n, 1, 1 / (1 + exp(-X[, 1] - X[, 2]))) # X[, 1] and X[, 2] influence W Y &lt;- pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n) # X[, 2], X[, 3], X[, 4:6] influence Y. So different set of Xs influence Y # Build a separate forest for Y and W forest.W &lt;- regression_forest(X, W, tune.parameters = &quot;all&quot;) W.hat &lt;- predict(forest.W)$predictions # this gives us the estimated propensity score (probability of treated) #plot(W.hat, X[, 1], col = as.factor(W)) #plot(W.hat, X[, 2], col = as.factor(W)) forest.Y &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) # note that W is not used here Y.hat &lt;- predict(forest.Y)$predictions # this gives the conditional mean of Y or m(x) #plot(Y, Y.hat) forest.Y.varimp &lt;- variable_importance(forest.Y) forest.Y.varimp ## [,1] ## [1,] 0.003306725 ## [2,] 0.467886532 ## [3,] 0.385392624 ## [4,] 0.040595176 ## [5,] 0.024537384 ## [6,] 0.051666432 ## [7,] 0.002604669 ## [8,] 0.001747140 ## [9,] 0.001460958 ## [10,] 0.001700408 ## [11,] 0.002464013 ## [12,] 0.001304409 ## [13,] 0.002807940 ## [14,] 0.002332690 ## [15,] 0.001483309 ## [16,] 0.001098320 ## [17,] 0.001448094 ## [18,] 0.002076831 ## [19,] 0.002872943 ## [20,] 0.001213402 # selects the important variables selected.vars &lt;- which(forest.Y.varimp / mean(forest.Y.varimp) &gt; 0.2) selected.vars ## [1] 2 3 4 5 6 # Trains a causal forest tau.forest &lt;- causal_forest(X[, selected.vars], Y, W, W.hat = W.hat, Y.hat = Y.hat, # specify e(x) and m(x) tune.parameters = &quot;all&quot;) # See if a causal forest succeeded in capturing heterogeneity by plotting # the TOC and calculating a 95% CI for the AUTOC. train &lt;- sample(1:n, n / 2) train.forest &lt;- causal_forest(X[train, ], Y[train], W[train]) eval.forest &lt;- causal_forest(X[-train, ], Y[-train], W[-train]) rate &lt;- rank_average_treatment_effect(eval.forest, predict(train.forest, X[-train, ])$predictions) rate ## estimate std.err target ## 0.0201784 0.04785508 priorities | AUTOC plot(rate) paste(&quot;AUTOC:&quot;, round(rate$estimate, 2), &quot;+/&quot;, round(1.96 * rate$std.err, 2)) ## [1] &quot;AUTOC: 0.02 +/ 0.09&quot; "],["heterogeneous-treatment-effects.html", "9 Heterogeneous Treatment Effects", " 9 Heterogeneous Treatment Effects This article summarizes heterogeneous treatment effects using ML. Simply put, its defined as the variation in response to treatment across several subgroups. For example, the impacts of Medicaid expansion on labor market outcomes can vary depending on uninsured rate prior to the expansion; the effects of discussion intervention program aimed to normalize disscussion regarding menstruation can increase demand for menstrual health products at a higher rate among those with high psychological cost in the baseline; in personalized medical treatment, we would want to identify the sub-group with higher response to a particular type of treatment. It is different from average treatment effect (ATE) such that the ATE focuses on the whole group, while heterogeneous treatment effect pertains to the specific sub-group characterized by features (\\(X\\)s). In this sense, one can think of ATE as the weighted average of subgroup specific ATEs. Using the potential outcome framework, ATE is given by: \\(E[Y_i^{1} - Y_i^{0}]\\). The heterogeneous treatment is: \\(E[Y_i^{1} - Y_i^{0} | X_i = x ]\\). Its the treatment conditional on \\(X_i\\), which is determined prior to observing the data. Hence, its also termed as the conditional average treatment effect (CATE). One simple example borrowed from Wager’s lecture notes to illustrate the concept is that of smoking in Geneva and Palo Alto. Say, two RCTs are conducted in Palo Alto and Geneva to evaluate whether cash incentives among teenagers can reduce the prevalence of smoking. # Palo-Alto smoke_mat &lt;- function(smoke_vec){ smoke &lt;- matrix(0, nrow =2, ncol = 3) smoke[, 1] &lt;- c(&quot;Treat&quot;, &quot;Control&quot;) smoke[ ,2] &lt;- c(smoke_vec[1], smoke_vec[2]) smoke[ ,3] &lt;- c(smoke_vec[3], smoke_vec[4]) return(smoke) } smoke &lt;- smoke_mat(c(152, 2362, 5, 122)) colnames(smoke) &lt;- c(&quot;Palo Alto&quot;, &quot;Non-S.&quot;, &quot;Smoker&quot;) data.frame(smoke) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) Palo.Alto Non.S. Smoker Treat 152 5 Control 2362 122 smoke &lt;- smoke_mat(c(581, 2278, 350, 1979)) colnames(smoke) &lt;- c(&quot;Geneva&quot;, &quot;Non-S.&quot;, &quot;Smoker&quot;) data.frame(smoke) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;left&quot;) Geneva Non.S. Smoker Treat 581 350 Control 2278 1979 \\(\\hat{\\tau}_{PA} = \\frac{5}{152+5} - \\frac{122}{2362 + 122} \\approx -1.7 pp\\) \\(\\hat{\\tau}_{GVA} = \\frac{350}{581+350} - \\frac{1979}{2278 + 1979} \\approx -8.9 pp\\) \\(\\hat{\\tau} = \\frac{2641}{2641 + 5188}\\tau_{PA} + \\frac{5188}{2641 + 5188}\\tau_{GVA}\\). Here, \\(\\hat{\\tau}_{PA}\\) is an estimate of \\(E[smoke \\;prevalence | \\; W = 1, \\; X = PA] \\; - \\; E[smoke \\;prevalence | \\; W = 0, \\; X = PA]\\), and its the treatment effect particular to Palo Alto. The average treatment effect \\(\\hat{\\tau}\\) is the weighted average of the two treatment effects. "],["some-ways-to-estimate-cate.html", "9.1 Some ways to estimate CATE", " 9.1 Some ways to estimate CATE Robinson’s partially linear model for homogeneous treatment effect is written as: \\(Y_i = \\tau W_i + f(X_i) + \\epsilon_i \\; ........(equation \\; 1)\\) Here, \\(\\tau\\) is assumed constant across sub-spaces of \\(X\\). We can expand to write Robinson’s partially linear model as: \\(Y_i = \\tau(X_i) W_i + f(X_i) + \\epsilon_i \\; ........(equation \\; 2)\\) where, \\(\\tau(.)\\) varies with \\(x\\). Equation 2 can be expressed as residual-on-residual regression format of: \\(Y_i - m(X_i) = \\tau(X_i) (W_i - e(X_i)) + \\epsilon_i \\; ........(equation \\; 3)\\) where, \\(m(x)\\) is the conditional expectation of \\(Y\\) given \\(X\\). \\(m(x) = E[Y_i | \\; X_i = x] = \\mu_{W = 0}(X_i) + \\tau(X_i) e(X_i)\\), where \\(\\mu_{0}(X_i)\\) is the baseline conditional response (in absense of treatment) and \\(e(x) = P(W_i = 1 | \\; X_i = x)\\).11 \\(\\tau(X)\\) is parameterized as \\(\\tau(x) = \\psi(x).\\beta\\), where \\(\\psi\\) is some pre-determined set of basis functions: \\(\\chi \\rightarrow R^k\\). A feasible loss function can be devised using equation 3 and using estimates of \\(m(x)\\) and \\(e(x)\\) from cross-fitting. \\(L = \\frac{1}{n} \\sum_{i = 1}^n((Y_i - \\hat{m}(X_i)^{-k(i)}) - (W_i - \\hat{e}(X_i)^{-k(i)}) \\; \\psi(X_i).\\beta)^2\\). Note that the parameter of interest is \\(\\beta\\). LASSO can be used to estimate \\(\\hat{\\beta}\\), where: \\(\\hat{\\beta} = argmin_{\\beta}\\{L + \\lambda \\; ||\\beta||_{1}\\}\\), where \\(\\lambda\\) is the regularizer on the complexity of \\(\\tau(.)\\).12 Note: The other approach is to use random forest to measure out weight of an observation \\(i\\) in relation to the test point \\(x\\). This approach is done using causal forest in the Generalized Random Forest framework. The distinction between \\(m(x)\\) and \\(m(X_i)\\) is such that the former is estimation performed at the new data point \\(x\\).↩︎ One can build a highly complex model and improve the in-sample fit. However, this model may perform badly while predicting out-of-sample cases. As such, the complexity of the model should be penalized while training the model.↩︎ "],["estimation-1.html", "9.2 Estimation", " 9.2 Estimation set.seed(194) # Generate Data n &lt;- 2000 p &lt;- 10 X &lt;- matrix(rnorm(n * p), n, p) X.test &lt;- matrix(0, 101, p) X.test[, 1] &lt;- seq(-2, 2, length.out = 101) W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0)) prob &lt;- 0.4 + 0.2 * (X[, 1] &gt; 0) Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmax(X[, 3], 0) + rnorm(n) ################################### ################################### # # # 1. estimate m(X) and e(X) # using cross-fitting # ################################### ################################### # cross-fitting index K &lt;- 10 # total folds ind &lt;- sample(1:length(W), replace = FALSE, size = length(W)) folds &lt;- cut(1:length(W), breaks = K, labels = FALSE) index &lt;- matrix(0, nrow = length(ind) / K, ncol = K) for(f in 1:K){ index[, f] &lt;- ind[which(folds == f)] } # Build a function to estimate conditional means (m(x) and e(x)) using random forest fun.rf.grf &lt;- function(X, Y, predictkfold){ rf_grf &lt;- regression_forest(X, Y, tune.parameters = &quot;all&quot;) p.grf &lt;- predict(rf_grf, predictkfold)$predictions return(p.grf) } # storing predict.mat &lt;- matrix(0, nrow = nrow(index), ncol = K) # to store e(x) predict.mat2 &lt;- predict.mat # to store m(x) # for each fold k use other folds for estimation for(k in seq(1:K)){ predict.mat[, k] &lt;- fun.rf.grf(X = X[c(index[, -k]), ], Y = W[index[, -k]], predictkfold = X[c(index[, k]), ]) predict.mat2[, k] &lt;- fun.rf.grf(X = X[c(index[, -k]), ], Y = Y[c(index[, -k])], predictkfold = X[c(index[, k]), ]) } W.hat &lt;- c(predict.mat) Y.hat &lt;- c(predict.mat2) ################################ ################################ # # 2. Use LASSO to minimize # the loss function ################################ ################################ # rearrange features and response according to index XX &lt;- X[c(index), ] YY &lt;- Y[c(index)] WW &lt;- W[c(index)] resid.Y &lt;- YY - Y.hat resid.W &lt;- WW - W.hat # Create basis expansion of features for(i in seq(1, ncol(XX))) { if(i == 1){ XX.basis &lt;- bs(XX[, i], knots = c(0.25, 0.5, 0.75), degree = 2) }else{ XX.basisnew &lt;- bs(XX[, i], knots = c(0.25, 0.5, 0.75), degree = 2) XX.basis &lt;- cbind(XX.basis, XX.basisnew) } } resid.W.X &lt;- resid.W * XX.basis resid.W.X &lt;- model.matrix(formula( ~ 0 + resid.W.X)) #plot(XX[ ,1], pmax(XX[ , 1], 0)) # cross validation for lasso to tune lambda lasso &lt;- cv.glmnet( x = resid.W.X, y = resid.Y, alpha = 1, intercept = FALSE ) #plot(lasso, main = &quot;Lasso penalty \\n \\n&quot;) # lambda with minimum MSE best.lambda &lt;- lasso$lambda.min lasso_tuned &lt;- glmnet( x = resid.W.X, y = resid.Y, lambda = best.lambda, intercept = FALSE ) #print(paste(&quot;The coefficients of lasso tuned are:&quot;, coef(lasso_tuned), sep = &quot; &quot;)) pred.lasso &lt;- predict(lasso, newx = XX.basis) ######################### # # Causal Forest # ######################### X.test &lt;- matrix(0, nrow = nrow(X), ncol = ncol(X)) X.test[, 1] &lt;- seq(-3, 3, length.out = nrow(X)) tau.forest &lt;- causal_forest(X, Y, W) tau.forest ## GRF forest object of type causal_forest ## Number of trees: 2000 ## Number of training samples: 2000 ## Variable importance: ## 1 2 3 4 5 6 7 8 9 10 ## 0.707 0.045 0.030 0.037 0.034 0.029 0.026 0.035 0.024 0.033 tau.hat &lt;- predict(tau.forest, X.test)$predictions par(oma=c(0,4,0,0)) plot(XX[order(XX[ , 1]), 1], pred.lasso[order(XX[, 1])], ylim = c(0, 3), t = &quot;l&quot;, xlab = &quot; &quot;, ylab = &quot; &quot;, xlim = c(-3, 3), lwd = 1.5) par(new = TRUE) plot(XX[order(XX[, 1]), 1], pmax(XX[order(XX[, 1]), 1], 0), col =&quot;red&quot;, ylim = c(0, 3), t = &quot;l&quot;, xlab = &quot;X1&quot;, ylab = &quot;tao(x)&quot;, xlim = c(-3, 3), lwd = 1.5) par(new = TRUE) plot(X.test[order(X.test[, 1]), 1], tau.hat[order(X.test[, 1])], t = &quot;l&quot;, col = &quot;blue&quot;, ylim = c(0, 3), xlab = &quot;X1&quot;, ylab = &quot;&quot;, xlim = c(-3, 3), lwd = 1.5) legend(&quot;topleft&quot;, c(&quot;Loss min Lasso&quot;, &quot;True Effect&quot;, &quot;Causal Forest&quot;), col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty = rep(1, 3)) "],["some-remarks-and-questions.html", "9.3 Some Remarks and Questions", " 9.3 Some Remarks and Questions For LASSO, we are using the basis of polynomial splines of degree 2 with interior knots at 25th, 75th, and 50th percentiles of each feature. We can see that although the effects are picked up, its slightly late and are lower compared to the true effect. A basis for linear splines performs well in this case. The causal forest framework on the other hand performs better. "]]
