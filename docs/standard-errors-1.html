<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Standard Errors | Causal Inference</title>
  <meta name="description" content="5 Standard Errors | Causal Inference" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Standard Errors | Causal Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Standard Errors | Causal Inference" />
  
  
  

<meta name="author" content="Vinish Shrestha" />


<meta name="date" content="2026-02-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-and-gradient-descent.html"/>
<link rel="next" href="logistic-regression.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="assets/kePrint-0.0.1/kePrint.js"></script>
<link href="assets/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="a-lab-experiment.html"><a href="a-lab-experiment.html"><i class="fa fa-check"></i><b>2.1</b> A lab experiment</a></li>
<li class="chapter" data-level="2.2" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.2</b> Challenges</a></li>
<li class="chapter" data-level="2.3" data-path="dag-directed-acyclic-graph.html"><a href="dag-directed-acyclic-graph.html"><i class="fa fa-check"></i><b>2.3</b> DAG (Directed Acyclic Graph)</a></li>
<li class="chapter" data-level="2.4" data-path="a-simulated-dgp.html"><a href="a-simulated-dgp.html"><i class="fa fa-check"></i><b>2.4</b> A simulated DGP</a></li>
<li class="chapter" data-level="2.5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="why-regression.html"><a href="why-regression.html"><i class="fa fa-check"></i><b>3</b> Why Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-best-fit-line.html"><a href="the-best-fit-line.html"><i class="fa fa-check"></i><b>3.1</b> The best-fit line</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression-specification.html"><a href="linear-regression-specification.html"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Specification</a></li>
<li class="chapter" data-level="3.3" data-path="law-of-iterated-expectation.html"><a href="law-of-iterated-expectation.html"><i class="fa fa-check"></i><b>3.3</b> Law of iterated expectation</a></li>
<li class="chapter" data-level="3.4" data-path="error-term.html"><a href="error-term.html"><i class="fa fa-check"></i><b>3.4</b> Error term</a></li>
<li class="chapter" data-level="3.5" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>3.5</b> Decomposition</a></li>
<li class="chapter" data-level="3.6" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>3.6</b> Estimation</a></li>
<li class="chapter" data-level="3.7" data-path="running-a-regression.html"><a href="running-a-regression.html"><i class="fa fa-check"></i><b>3.7</b> Running a regression</a></li>
<li class="chapter" data-level="3.8" data-path="standard-errors.html"><a href="standard-errors.html"><i class="fa fa-check"></i><b>3.8</b> Standard Errors</a></li>
<li class="chapter" data-level="3.9" data-path="an-exercise.html"><a href="an-exercise.html"><i class="fa fa-check"></i><b>3.9</b> An exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression-and-gradient-descent.html"><a href="regression-and-gradient-descent.html"><i class="fa fa-check"></i><b>4</b> Regression and Gradient Descent</a></li>
<li class="chapter" data-level="5" data-path="standard-errors-1.html"><a href="standard-errors-1.html"><i class="fa fa-check"></i><b>5</b> Standard Errors</a></li>
<li class="chapter" data-level="6" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>6</b> Logistic regression</a></li>
<li class="chapter" data-level="7" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>7</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="7.1" data-path="potential-outcome-framework-neyman-rubin-causal-model.html"><a href="potential-outcome-framework-neyman-rubin-causal-model.html"><i class="fa fa-check"></i><b>7.1</b> Potential Outcome Framework: Neyman-Rubin Causal Model</a></li>
<li class="chapter" data-level="7.2" data-path="average-treatment-effect-ate.html"><a href="average-treatment-effect-ate.html"><i class="fa fa-check"></i><b>7.2</b> Average treatment effect (ATE)</a></li>
<li class="chapter" data-level="7.3" data-path="rct.html"><a href="rct.html"><i class="fa fa-check"></i><b>7.3</b> RCT</a></li>
<li class="chapter" data-level="7.4" data-path="average-treatment-effect-on-the-treated-att.html"><a href="average-treatment-effect-on-the-treated-att.html"><i class="fa fa-check"></i><b>7.4</b> Average treatment effect on the treated (ATT)</a></li>
<li class="chapter" data-level="7.5" data-path="an-estimation-example.html"><a href="an-estimation-example.html"><i class="fa fa-check"></i><b>7.5</b> An estimation example</a></li>
<li class="chapter" data-level="7.6" data-path="unconfoundedness-assumption.html"><a href="unconfoundedness-assumption.html"><i class="fa fa-check"></i><b>7.6</b> Unconfoundedness assumption</a></li>
<li class="chapter" data-level="7.7" data-path="discussion-1.html"><a href="discussion-1.html"><i class="fa fa-check"></i><b>7.7</b> Discussion</a></li>
<li class="chapter" data-level="7.8" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>7.8</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ipw-and-aipw.html"><a href="ipw-and-aipw.html"><i class="fa fa-check"></i><b>8</b> IPW and AIPW</a>
<ul>
<li class="chapter" data-level="8.1" data-path="a-simple-example.html"><a href="a-simple-example.html"><i class="fa fa-check"></i><b>8.1</b> A simple example</a></li>
<li class="chapter" data-level="8.2" data-path="aggregated-estimator.html"><a href="aggregated-estimator.html"><i class="fa fa-check"></i><b>8.2</b> Aggregated Estimator</a></li>
<li class="chapter" data-level="8.3" data-path="propensity-score.html"><a href="propensity-score.html"><i class="fa fa-check"></i><b>8.3</b> Propensity score</a></li>
<li class="chapter" data-level="8.4" data-path="estimation-of-propensity-score.html"><a href="estimation-of-propensity-score.html"><i class="fa fa-check"></i><b>8.4</b> Estimation of propensity score</a></li>
<li class="chapter" data-level="8.5" data-path="using-cross-fitting-to-predict-propensity-score.html"><a href="using-cross-fitting-to-predict-propensity-score.html"><i class="fa fa-check"></i><b>8.5</b> Using cross-fitting to predict propensity score</a></li>
<li class="chapter" data-level="8.6" data-path="propensity-score-stratification.html"><a href="propensity-score-stratification.html"><i class="fa fa-check"></i><b>8.6</b> Propensity score stratification</a></li>
<li class="chapter" data-level="8.7" data-path="inverse-probability-weighting-ipw.html"><a href="inverse-probability-weighting-ipw.html"><i class="fa fa-check"></i><b>8.7</b> Inverse Probability Weighting (IPW)</a></li>
<li class="chapter" data-level="8.8" data-path="comparing-ipw-with-aggregated-estimate.html"><a href="comparing-ipw-with-aggregated-estimate.html"><i class="fa fa-check"></i><b>8.8</b> Comparing IPW with Aggregated Estimate</a></li>
<li class="chapter" data-level="8.9" data-path="aipw-and-estimation.html"><a href="aipw-and-estimation.html"><i class="fa fa-check"></i><b>8.9</b> AIPW and Estimation</a></li>
<li class="chapter" data-level="8.10" data-path="assessing-balance.html"><a href="assessing-balance.html"><i class="fa fa-check"></i><b>8.10</b> Assessing Balance</a></li>
<li class="chapter" data-level="8.11" data-path="cross-fitting.html"><a href="cross-fitting.html"><i class="fa fa-check"></i><b>8.11</b> Cross-fitting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="difference-in-differences.html"><a href="difference-in-differences.html"><i class="fa fa-check"></i><b>9</b> Difference in Differences</a>
<ul>
<li class="chapter" data-level="9.1" data-path="a-quick-introduction.html"><a href="a-quick-introduction.html"><i class="fa fa-check"></i><b>9.1</b> A Quick Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="set-up.html"><a href="set-up.html"><i class="fa fa-check"></i><b>9.2</b> Set up</a></li>
<li class="chapter" data-level="9.3" data-path="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><a href="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><i class="fa fa-check"></i><b>9.3</b> An example: Evaluating the impact of Medicaid expansion on uninsured rate</a></li>
<li class="chapter" data-level="9.4" data-path="naive-estimator.html"><a href="naive-estimator.html"><i class="fa fa-check"></i><b>9.4</b> Naive estimator</a></li>
<li class="chapter" data-level="9.5" data-path="canonical-difference-in-differences-framework.html"><a href="canonical-difference-in-differences-framework.html"><i class="fa fa-check"></i><b>9.5</b> Canonical Difference in Differences Framework</a></li>
<li class="chapter" data-level="9.6" data-path="did-in-multi-period-set-up.html"><a href="did-in-multi-period-set-up.html"><i class="fa fa-check"></i><b>9.6</b> DiD in multi-period set up</a></li>
<li class="chapter" data-level="9.7" data-path="conditional-parallel-trend-assumption.html"><a href="conditional-parallel-trend-assumption.html"><i class="fa fa-check"></i><b>9.7</b> Conditional Parallel Trend Assumption</a></li>
<li class="chapter" data-level="9.8" data-path="some-concerns-with-controls.html"><a href="some-concerns-with-controls.html"><i class="fa fa-check"></i><b>9.8</b> Some concerns with controls</a></li>
<li class="chapter" data-level="9.9" data-path="the-2-times-2-difference-in-differences-estimate.html"><a href="the-2-times-2-difference-in-differences-estimate.html"><i class="fa fa-check"></i><b>9.9</b> The <span class="math inline">\(2 \times 2\)</span> Difference-in-Differences Estimate</a></li>
<li class="chapter" data-level="9.10" data-path="event-study-model.html"><a href="event-study-model.html"><i class="fa fa-check"></i><b>9.10</b> Event study model</a></li>
<li class="chapter" data-level="9.11" data-path="two-way-fixed-effect-twfe-revisited.html"><a href="two-way-fixed-effect-twfe-revisited.html"><i class="fa fa-check"></i><b>9.11</b> Two way fixed effect (TWFE) Revisited</a></li>
<li class="chapter" data-level="9.12" data-path="various-ways-of-estimation.html"><a href="various-ways-of-estimation.html"><i class="fa fa-check"></i><b>9.12</b> Various ways of estimation</a></li>
<li class="chapter" data-level="9.13" data-path="multi-period-multi-group-and-variation-in-treatment-timing.html"><a href="multi-period-multi-group-and-variation-in-treatment-timing.html"><i class="fa fa-check"></i><b>9.13</b> Multi Period, Multi Group and Variation in Treatment Timing</a></li>
<li class="chapter" data-level="9.14" data-path="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><a href="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><i class="fa fa-check"></i><b>9.14</b> Problem with TWFE in Multiple Group with Treatment Timing Variation</a></li>
<li class="chapter" data-level="9.15" data-path="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><a href="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><i class="fa fa-check"></i><b>9.15</b> What is TWFE Estimating when there is Treatment Timing Variation?</a></li>
<li class="chapter" data-level="9.16" data-path="assumptions-governing-twfedd-estimate.html"><a href="assumptions-governing-twfedd-estimate.html"><i class="fa fa-check"></i><b>9.16</b> Assumptions governing TWFEDD estimate</a></li>
<li class="chapter" data-level="9.17" data-path="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><a href="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><i class="fa fa-check"></i><b>9.17</b> How Does Treatment Effect Heterogeneity in Time Affect TWFE?</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="causal-forest.html"><a href="causal-forest.html"><i class="fa fa-check"></i><b>10</b> Causal Forest</a>
<ul>
<li class="chapter" data-level="10.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="summary-of-grf.html"><a href="summary-of-grf.html"><i class="fa fa-check"></i><b>10.2</b> Summary of GRF</a></li>
<li class="chapter" data-level="10.3" data-path="motivation-for-causal-forests.html"><a href="motivation-for-causal-forests.html"><i class="fa fa-check"></i><b>10.3</b> Motivation for Causal Forests</a></li>
<li class="chapter" data-level="10.4" data-path="causal-forest-1.html"><a href="causal-forest-1.html"><i class="fa fa-check"></i><b>10.4</b> Causal Forest</a></li>
<li class="chapter" data-level="10.5" data-path="an-example-of-causal-forest.html"><a href="an-example-of-causal-forest.html"><i class="fa fa-check"></i><b>10.5</b> An example of causal forest</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>11</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="11.1" data-path="some-ways-to-estimate-cate.html"><a href="some-ways-to-estimate-cate.html"><i class="fa fa-check"></i><b>11.1</b> Some ways to estimate CATE</a></li>
<li class="chapter" data-level="11.2" data-path="estimation-1.html"><a href="estimation-1.html"><i class="fa fa-check"></i><b>11.2</b> Estimation</a></li>
<li class="chapter" data-level="11.3" data-path="some-remarks-and-questions.html"><a href="some-remarks-and-questions.html"><i class="fa fa-check"></i><b>11.3</b> Some Remarks and Questions</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="standard-errors-1" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Standard Errors<a href="standard-errors-1.html#standard-errors-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Course:</strong> Causal Inference</p>
<p><strong>Topic:</strong> Estimation of linear regression model and standard errors</p>
<style>
.jp-Notebook,
.jp-NotebookPanel-notebook {
    max-width: 900px;
    margin: auto;
}

.jp-Cell {
    padding-left: 40px;
    padding-right: 40px;
}
</style>
<style>
@media print {
 body {
   margin: 1in;
 }
}
</style>
<p><strong>Standard Errors</strong></p>
<p>As previously mentioned, we start with a specification oriented towards the population and use a sample to estimate
the population parameters. The <span class="math inline">\(\hat{\beta}\)</span> are the sample estimates that inform us about the population parameters <span class="math inline">\(\beta\)</span>.
In this sense, sampling variability – if you were to take say 1,000 samples from the population and re-estimate the parameter –
will give you different estimates of <span class="math inline">\(\beta\)</span>. Just as the sample mean is a random variable with its own distribution,
so is <span class="math inline">\(\hat{\beta}\)</span>. The standard error of <span class="math inline">\(\hat{\beta}\)</span> plays the same role as the standard error of the mean,
which we motivate below.</p>
<p>Consider the following example of mean height. Say, the population distribution is normal with mean 175 cm and standard deviation
of 7.6 cm. You first take a sample of 1,000 individuals and estimate the mean height; then re-take the next sample,
re-estimate the mean, and so on for 1,000 different samples. This will give you a distribution of mean height estimates,
which will itself be normal with a variance driven entirely by sampling variability.</p>
<p>It is important to note that in practice you only ever have <strong>one sample</strong> – the simulation below is a thought experiment
to build intuition for what would happen under repeated sampling. This is the frequentist conception of uncertainty.</p>
<p>By the <strong>Central Limit Theorem (CLT)</strong>, the sampling distribution of the mean is:
<span class="math display">\[\hat{X} \sim \mathcal{N}\left(\mu,\ \frac{\sigma}{\sqrt{n}}\right)\]</span>
where <span class="math inline">\(\sigma\)</span> is the population standard deviation (fixed but unknown in practice) and <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span> is the
<strong>standard error</strong> – the standard deviation of the sampling distribution of the mean, not of the population itself.
Note the distinction: <span class="math inline">\(\sigma\)</span> describes variability in individual heights; <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span> describes variability
in the <em>estimate of the mean</em> across repeated samples.</p>
<p>Let’s simulate height coming from a normal distribution with mean 175 cm and standard deviation 7.6 cm.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="standard-errors-1.html#cb77-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb77-2"><a href="standard-errors-1.html#cb77-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb77-3"><a href="standard-errors-1.html#cb77-3" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> add_dummy_feature</span>
<span id="cb77-4"><a href="standard-errors-1.html#cb77-4" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb77-5"><a href="standard-errors-1.html#cb77-5" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb77-6"><a href="standard-errors-1.html#cb77-6" tabindex="-1"></a></span>
<span id="cb77-7"><a href="standard-errors-1.html#cb77-7" tabindex="-1"></a>mean_store <span class="op">=</span> []</span>
<span id="cb77-8"><a href="standard-errors-1.html#cb77-8" tabindex="-1"></a><span class="bu">iter</span> <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb77-9"><a href="standard-errors-1.html#cb77-9" tabindex="-1"></a>mean_height <span class="op">=</span> <span class="dv">175</span></span>
<span id="cb77-10"><a href="standard-errors-1.html#cb77-10" tabindex="-1"></a>std_height <span class="op">=</span> <span class="fl">7.6</span></span>
<span id="cb77-11"><a href="standard-errors-1.html#cb77-11" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb77-12"><a href="standard-errors-1.html#cb77-12" tabindex="-1"></a></span>
<span id="cb77-13"><a href="standard-errors-1.html#cb77-13" tabindex="-1"></a></span>
<span id="cb77-14"><a href="standard-errors-1.html#cb77-14" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">iter</span>):</span>
<span id="cb77-15"><a href="standard-errors-1.html#cb77-15" tabindex="-1"></a>    height <span class="op">=</span> np.random.normal(mean_height, std_height, n) <span class="co"># sample height from a normal distribution</span></span>
<span id="cb77-16"><a href="standard-errors-1.html#cb77-16" tabindex="-1"></a>    mean_store.append(height.mean())</span>
<span id="cb77-17"><a href="standard-errors-1.html#cb77-17" tabindex="-1"></a></span>
<span id="cb77-18"><a href="standard-errors-1.html#cb77-18" tabindex="-1"></a><span class="co"># plot the histogram of mean height</span></span>
<span id="cb77-19"><a href="standard-errors-1.html#cb77-19" tabindex="-1"></a>mean_store <span class="op">=</span> np.array(mean_store).ravel()    </span>
<span id="cb77-20"><a href="standard-errors-1.html#cb77-20" tabindex="-1"></a></span>
<span id="cb77-21"><a href="standard-errors-1.html#cb77-21" tabindex="-1"></a><span class="co"># overlay theoretical normal dist</span></span>
<span id="cb77-22"><a href="standard-errors-1.html#cb77-22" tabindex="-1"></a>x_space <span class="op">=</span> np.linspace(mean_store.<span class="bu">min</span>(), mean_store.<span class="bu">max</span>(), <span class="dv">1000</span>)</span>
<span id="cb77-23"><a href="standard-errors-1.html#cb77-23" tabindex="-1"></a>theo_nd <span class="op">=</span> stats.norm.pdf(x_space, mean_height, std_height<span class="op">/</span>np.sqrt(n))</span>
<span id="cb77-24"><a href="standard-errors-1.html#cb77-24" tabindex="-1"></a></span>
<span id="cb77-25"><a href="standard-errors-1.html#cb77-25" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb77-26"><a href="standard-errors-1.html#cb77-26" tabindex="-1"></a>plt.hist(mean_store, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&#39;steelblue&#39;</span>, edgecolor<span class="op">=</span><span class="st">&#39;black&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, density<span class="op">=</span><span class="st">&#39;True&#39;</span>)</span>
<span id="cb77-27"><a href="standard-errors-1.html#cb77-27" tabindex="-1"></a>plt.plot(x_space, theo_nd, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>, linewidth <span class="op">=</span> <span class="dv">2</span>, label <span class="op">=</span> <span class="st">&#39;Theoretical Dist.&#39;</span>)</span>
<span id="cb77-28"><a href="standard-errors-1.html#cb77-28" tabindex="-1"></a>plt.title(<span class="ss">f&#39;Distribution of mean height from </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss"> different samples&#39;</span>)</span>
<span id="cb77-29"><a href="standard-errors-1.html#cb77-29" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;mean height&#39;</span>)</span>
<span id="cb77-30"><a href="standard-errors-1.html#cb77-30" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;density&#39;</span>)</span>
<span id="cb77-31"><a href="standard-errors-1.html#cb77-31" tabindex="-1"></a>plt.legend()</span>
<span id="cb77-32"><a href="standard-errors-1.html#cb77-32" tabindex="-1"></a>plt.grid(<span class="va">False</span>)</span>
<span id="cb77-33"><a href="standard-errors-1.html#cb77-33" tabindex="-1"></a></span>
<span id="cb77-34"><a href="standard-errors-1.html#cb77-34" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;average of mean height is </span><span class="sc">{</span>mean_store<span class="sc">.</span>mean()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> and std is </span><span class="sc">{</span>mean_store<span class="sc">.</span>std()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>average of mean height is 174.997 and std is 0.2402</code></pre>
<div class="float">
<img src="standard_errors_files/standard_errors_4_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>It is clear that the standard deviation of the mean height depends on: i) the standard deviation of the population height; and ii)
n – the number of observations. The standard deviation of the mean height will be lower if the population standard deviation is lower
(meaning that height is relatively more homogeneous). Next, one can lower it by increasing the sample size.</p>
<p>Just as the sample mean has a measure for deviation due to sampling variability (standard deviation), <span class="math inline">\(\hat{\beta}}\)</span> too has a measure that we know as standard errors. Simply put, the standard errors measure how fluctuating the estimates of <span class="math inline">\(\beta\)</span> can be given different samples.</p>
<p>Right off the start, it should be mentioned that reported standard errors are mostly incorrect. This could be due to several unknown reasons including the functional form of the specified model. But rather than dwelling on why the reported standard errors are incorrect, I want to discuss some known ways to fix the standard errors.</p>
<p>First, (recall) we start with the assumption on the error term. We discussed the i.i.d. assumtion of the error term. Again, the i.i.d. assumption states that error term are <em>independent</em> and <em>identically</em> distributed. The former means that errors are not correlated, whereas the latter means that errors are extracted from the same distribution with the same mean and variance.</p>
<p>Under the i.i.d. assumption, errors are homoskedastic. Estimation of standard errors take the following steps.</p>
<ol style="list-style-type: decimal">
<li><p>First estimate the regression standard error as: <span class="math inline">\(\sigma_{reg}^2 = \frac{1}{n}\epsilon^{T} \epsilon\)</span>.</p></li>
<li><p>The standard error of estimates then is: <span class="math inline">\(\sqrt{Var(\hat{\beta})} = (X^{T}X)^{-1} \times \sigma_{reg}\)</span>, where <span class="math inline">\(X^{T}X\)</span> is the variance-covariance matrix with variance terms in the diagonal.</p></li>
</ol>
<p>However, both of these assumptions (identical and independently distributed) will mostly likely fail in practice. This means that we’d need to adjust the standard errors appropriately.</p>
<p>Let’s take a classic example between education and earnings. We’ll also consider ‘ability’ as a control variable in the DGP. Of course, this is only a simulation exercise to clarify the context of standard errors.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="standard-errors-1.html#cb79-1" tabindex="-1"></a></span>
<span id="cb79-2"><a href="standard-errors-1.html#cb79-2" tabindex="-1"></a><span class="co"># 1. generate ability score with a mean of 50 and sd of 20.</span></span>
<span id="cb79-3"><a href="standard-errors-1.html#cb79-3" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb79-4"><a href="standard-errors-1.html#cb79-4" tabindex="-1"></a>ability <span class="op">=</span> np.random.normal(<span class="dv">50</span>, <span class="dv">20</span>, m)</span>
<span id="cb79-5"><a href="standard-errors-1.html#cb79-5" tabindex="-1"></a>ability_scaled <span class="op">=</span> (ability <span class="op">-</span> ability.mean()) <span class="op">/</span> ability.std() <span class="co"># ability scaled</span></span>
<span id="cb79-6"><a href="standard-errors-1.html#cb79-6" tabindex="-1"></a></span>
<span id="cb79-7"><a href="standard-errors-1.html#cb79-7" tabindex="-1"></a><span class="co"># plot histogram of ability score</span></span>
<span id="cb79-8"><a href="standard-errors-1.html#cb79-8" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb79-9"><a href="standard-errors-1.html#cb79-9" tabindex="-1"></a>plt.hist(ability, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&#39;steelblue&#39;</span>, edgecolor<span class="op">=</span><span class="st">&#39;black&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb79-10"><a href="standard-errors-1.html#cb79-10" tabindex="-1"></a>plt.title(<span class="st">&#39;Distribution of ability&#39;</span>)</span>
<span id="cb79-11"><a href="standard-errors-1.html#cb79-11" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;ability score&#39;</span>)</span>
<span id="cb79-12"><a href="standard-errors-1.html#cb79-12" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;frequency&#39;</span>)</span>
<span id="cb79-13"><a href="standard-errors-1.html#cb79-13" tabindex="-1"></a></span>
<span id="cb79-14"><a href="standard-errors-1.html#cb79-14" tabindex="-1"></a><span class="co"># 2. education is positively dependent on ability</span></span>
<span id="cb79-15"><a href="standard-errors-1.html#cb79-15" tabindex="-1"></a>educa <span class="op">=</span> <span class="dv">7</span> <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> ability <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">4</span>, m)</span>
<span id="cb79-16"><a href="standard-errors-1.html#cb79-16" tabindex="-1"></a>educa[educa<span class="op">&lt;</span><span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb79-17"><a href="standard-errors-1.html#cb79-17" tabindex="-1"></a>educa_scaled <span class="op">=</span> (educa <span class="op">-</span> educa.mean()) <span class="op">/</span> educa.std() <span class="co"># education scaled</span></span>
<span id="cb79-18"><a href="standard-errors-1.html#cb79-18" tabindex="-1"></a><span class="co">#plt.hist(educa)</span></span>
<span id="cb79-19"><a href="standard-errors-1.html#cb79-19" tabindex="-1"></a></span>
<span id="cb79-20"><a href="standard-errors-1.html#cb79-20" tabindex="-1"></a><span class="co"># 3. generate income</span></span>
<span id="cb79-21"><a href="standard-errors-1.html#cb79-21" tabindex="-1"></a><span class="co"># Income comes from distribution with varying standard deviation</span></span>
<span id="cb79-22"><a href="standard-errors-1.html#cb79-22" tabindex="-1"></a><span class="co"># this marks heteroskedasticity</span></span>
<span id="cb79-23"><a href="standard-errors-1.html#cb79-23" tabindex="-1"></a>error_term <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">500</span> <span class="op">+</span> (<span class="dv">200</span><span class="op">*</span>educa))</span>
<span id="cb79-24"><a href="standard-errors-1.html#cb79-24" tabindex="-1"></a></span>
<span id="cb79-25"><a href="standard-errors-1.html#cb79-25" tabindex="-1"></a><span class="co"># generate income using the following DGP </span></span>
<span id="cb79-26"><a href="standard-errors-1.html#cb79-26" tabindex="-1"></a>income <span class="op">=</span> <span class="dv">40000</span> <span class="op">+</span> <span class="dv">10000</span> <span class="op">*</span> educa_scaled <span class="op">+</span> <span class="dv">700</span> <span class="op">*</span> ability_scaled <span class="op">+</span> np.array(error_term).ravel()</span>
<span id="cb79-27"><a href="standard-errors-1.html#cb79-27" tabindex="-1"></a>true_coefficients <span class="op">=</span> np.array([<span class="dv">40000</span>, <span class="dv">10000</span>, <span class="dv">700</span>])</span>
<span id="cb79-28"><a href="standard-errors-1.html#cb79-28" tabindex="-1"></a></span>
<span id="cb79-29"><a href="standard-errors-1.html#cb79-29" tabindex="-1"></a><span class="co"># scatter plot between schooling and education </span></span>
<span id="cb79-30"><a href="standard-errors-1.html#cb79-30" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb79-31"><a href="standard-errors-1.html#cb79-31" tabindex="-1"></a>plt.scatter(educa, income)</span>
<span id="cb79-32"><a href="standard-errors-1.html#cb79-32" tabindex="-1"></a>plt.title(<span class="st">&#39;Scatter plot of education and income&#39;</span>)</span>
<span id="cb79-33"><a href="standard-errors-1.html#cb79-33" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle <span class="op">=</span><span class="st">&#39;dashed&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>)</span>
<span id="cb79-34"><a href="standard-errors-1.html#cb79-34" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Schooling&#39;</span>)</span>
<span id="cb79-35"><a href="standard-errors-1.html#cb79-35" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Earnings&#39;</span>)</span>
<span id="cb79-36"><a href="standard-errors-1.html#cb79-36" tabindex="-1"></a></span>
<span id="cb79-37"><a href="standard-errors-1.html#cb79-37" tabindex="-1"></a><span class="co"># scatter plot between schooling and error</span></span>
<span id="cb79-38"><a href="standard-errors-1.html#cb79-38" tabindex="-1"></a>error_true <span class="op">=</span> income <span class="op">-</span> (<span class="dv">40000</span> <span class="op">+</span> <span class="dv">10000</span> <span class="op">*</span> educa_scaled <span class="op">+</span> <span class="dv">700</span> <span class="op">*</span> ability_scaled)</span>
<span id="cb79-39"><a href="standard-errors-1.html#cb79-39" tabindex="-1"></a></span>
<span id="cb79-40"><a href="standard-errors-1.html#cb79-40" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb79-41"><a href="standard-errors-1.html#cb79-41" tabindex="-1"></a>plt.scatter(educa, error_term, alpha <span class="op">=</span> <span class="fl">0.3</span>, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>)</span>
<span id="cb79-42"><a href="standard-errors-1.html#cb79-42" tabindex="-1"></a>plt.title(<span class="st">&#39;Relationship between schooling and error term </span><span class="ch">\n</span><span class="st"> using the true parameters&#39;</span>)</span>
<span id="cb79-43"><a href="standard-errors-1.html#cb79-43" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Schooling&#39;</span>)</span>
<span id="cb79-44"><a href="standard-errors-1.html#cb79-44" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Error&#39;</span>)</span>
<span id="cb79-45"><a href="standard-errors-1.html#cb79-45" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">&#39;dashed&#39;</span>)</span></code></pre></div>
<div class="float">
<img src="standard_errors_files/standard_errors_6_0.png" alt="png" />
<div class="figcaption">png</div>
</div>
<div class="float">
<img src="standard_errors_files/standard_errors_6_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<div class="float">
<img src="standard_errors_files/standard_errors_6_2.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>The plot above shows that although there exist a positive correlation between schooling and earnings, variation in earnings is higher for greater values of education. Using the true coefficients (we have these since this is a simulated DGP) we extract error and plot the relationship between schooling and errors. Here, we see a funnel shaped scatter plot with the mean of error aligned at 0.
This simple example breaks the homoskedasticity assumption. The main point is that the estimation of standard errors need to reflect the fact that error term comes from distribution with different variances. We now have what is known as heteroskedasticity.</p>
<p>Let’s start with regression estimates using the gradient descent.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="standard-errors-1.html#cb80-1" tabindex="-1"></a><span class="co"># features</span></span>
<span id="cb80-2"><a href="standard-errors-1.html#cb80-2" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((educa_scaled.reshape(m,<span class="dv">1</span>), ability_scaled.reshape(m,<span class="dv">1</span>)), axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb80-3"><a href="standard-errors-1.html#cb80-3" tabindex="-1"></a><span class="co"># Note: no need to scale the features as they already are scaled </span></span>
<span id="cb80-4"><a href="standard-errors-1.html#cb80-4" tabindex="-1"></a>X <span class="op">=</span> add_dummy_feature(X) <span class="co"># intercept term</span></span>
<span id="cb80-5"><a href="standard-errors-1.html#cb80-5" tabindex="-1"></a>y <span class="op">=</span> income.reshape((m, <span class="dv">1</span>))</span>
<span id="cb80-6"><a href="standard-errors-1.html#cb80-6" tabindex="-1"></a>p <span class="op">=</span> X.shape[<span class="dv">1</span>]           <span class="co"># number of features</span></span>
<span id="cb80-7"><a href="standard-errors-1.html#cb80-7" tabindex="-1"></a></span>
<span id="cb80-8"><a href="standard-errors-1.html#cb80-8" tabindex="-1"></a><span class="co"># initialize theta</span></span>
<span id="cb80-9"><a href="standard-errors-1.html#cb80-9" tabindex="-1"></a>theta <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, p).reshape((p, <span class="dv">1</span>))</span>
<span id="cb80-10"><a href="standard-errors-1.html#cb80-10" tabindex="-1"></a><span class="co"># learning rate</span></span>
<span id="cb80-11"><a href="standard-errors-1.html#cb80-11" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb80-12"><a href="standard-errors-1.html#cb80-12" tabindex="-1"></a><span class="co"># number of iterations</span></span>
<span id="cb80-13"><a href="standard-errors-1.html#cb80-13" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb80-14"><a href="standard-errors-1.html#cb80-14" tabindex="-1"></a></span>
<span id="cb80-15"><a href="standard-errors-1.html#cb80-15" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb80-16"><a href="standard-errors-1.html#cb80-16" tabindex="-1"></a></span>
<span id="cb80-17"><a href="standard-errors-1.html#cb80-17" tabindex="-1"></a>    <span class="co"># get the gradient and adjust theta against the gradient using the learning rate</span></span>
<span id="cb80-18"><a href="standard-errors-1.html#cb80-18" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X.T<span class="op">@</span>(y <span class="op">-</span> X<span class="op">@</span>theta )</span>
<span id="cb80-19"><a href="standard-errors-1.html#cb80-19" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">-</span> eta<span class="op">*</span>gradient </span>
<span id="cb80-20"><a href="standard-errors-1.html#cb80-20" tabindex="-1"></a> </span>
<span id="cb80-21"><a href="standard-errors-1.html#cb80-21" tabindex="-1"></a></span>
<span id="cb80-22"><a href="standard-errors-1.html#cb80-22" tabindex="-1"></a><span class="co"># Use sklearn module to check</span></span>
<span id="cb80-23"><a href="standard-errors-1.html#cb80-23" tabindex="-1"></a>reg <span class="op">=</span> linear_model.LinearRegression(fit_intercept<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb80-24"><a href="standard-errors-1.html#cb80-24" tabindex="-1"></a>reg.fit(X, y)</span>
<span id="cb80-25"><a href="standard-errors-1.html#cb80-25" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The estimates from sklearn is: </span><span class="sc">{</span>reg<span class="sc">.</span>coef_<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb80-26"><a href="standard-errors-1.html#cb80-26" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The estimates from manual GD is: </span><span class="sc">{</span>theta<span class="sc">.</span>reshape((<span class="dv">1</span>, <span class="dv">3</span>))<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>) </span></code></pre></div>
<pre><code>The estimates from sklearn is: [[39987.2777  9991.4859   695.5676]]
The estimates from manual GD is: [[39987.2777  9991.4859   695.5676]]</code></pre>
<p>We are simply running gradient descent in the code above. This should be familiar to you from previous lectures. Now, we’d want to move on to the standard errors to determine precision of these estimates. First, let’s explore the graphical relationship between education and residuals. Note that I’ve distinguised error vs. residual – the former is what you get from using true coefficients (you never see this in practice), while the latter uses the estimates.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="standard-errors-1.html#cb82-1" tabindex="-1"></a><span class="co"># Get residuals (note that we are using the estimated thetas)</span></span>
<span id="cb82-2"><a href="standard-errors-1.html#cb82-2" tabindex="-1"></a>residual <span class="op">=</span> y <span class="op">-</span> X <span class="op">@</span> theta</span>
<span id="cb82-3"><a href="standard-errors-1.html#cb82-3" tabindex="-1"></a></span>
<span id="cb82-4"><a href="standard-errors-1.html#cb82-4" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb82-5"><a href="standard-errors-1.html#cb82-5" tabindex="-1"></a>plt.scatter(educa, residual, alpha <span class="op">=</span> <span class="fl">0.3</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb82-6"><a href="standard-errors-1.html#cb82-6" tabindex="-1"></a>plt.title(<span class="st">&#39;Relationship between education and residuals&#39;</span>)</span>
<span id="cb82-7"><a href="standard-errors-1.html#cb82-7" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;education&#39;</span>)</span>
<span id="cb82-8"><a href="standard-errors-1.html#cb82-8" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;residuals&#39;</span>)</span>
<span id="cb82-9"><a href="standard-errors-1.html#cb82-9" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">&#39;dashed&#39;</span>)</span></code></pre></div>
<div class="float">
<img src="standard_errors_files/standard_errors_10_0.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>As you can see we’ve got a funnel shaped relationship between education and residual, again signaling heteroskedasticity. This is precisely coming from differing variance of error term based on education. This won’t affect the estimates but will affect the standard errors. Note that there are many tests for homoskedasticity vs heteroskedasticity. But in practice, the case of homoskedasticity almost always fails. The simple way to see it is to plot the residuals with the variable of concern. If you have a funnel looking shape, then you’ve got the case of heteroskedasticity.</p>
<p>First, let’s do some benchmarking by estimating the standard errors under the homoskedasticity assumption – error terms have the same variance (which is incorrect in this case and in practice).</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="standard-errors-1.html#cb83-1" tabindex="-1"></a><span class="co"># 1. Get the standard error of the regression</span></span>
<span id="cb83-2"><a href="standard-errors-1.html#cb83-2" tabindex="-1"></a>error <span class="op">=</span> (X <span class="op">@</span> theta <span class="op">-</span> y)</span>
<span id="cb83-3"><a href="standard-errors-1.html#cb83-3" tabindex="-1"></a>error_sq <span class="op">=</span> error.T <span class="op">@</span> error</span>
<span id="cb83-4"><a href="standard-errors-1.html#cb83-4" tabindex="-1"></a>sigma_sq <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (X.shape[<span class="dv">0</span>] <span class="op">-</span><span class="dv">3</span>) <span class="op">*</span> error_sq</span>
<span id="cb83-5"><a href="standard-errors-1.html#cb83-5" tabindex="-1"></a>se_reg <span class="op">=</span> np.sqrt(sigma_sq)</span>
<span id="cb83-6"><a href="standard-errors-1.html#cb83-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard error of the regression is: </span><span class="sc">{</span>se_reg<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb83-7"><a href="standard-errors-1.html#cb83-7" tabindex="-1"></a></span>
<span id="cb83-8"><a href="standard-errors-1.html#cb83-8" tabindex="-1"></a><span class="co"># 2. get standard errors of the respective coefficients </span></span>
<span id="cb83-9"><a href="standard-errors-1.html#cb83-9" tabindex="-1"></a>var_cov <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">*</span> sigma_sq</span>
<span id="cb83-10"><a href="standard-errors-1.html#cb83-10" tabindex="-1"></a>manual_se <span class="op">=</span> np.sqrt(np.diag(var_cov))</span>
<span id="cb83-11"><a href="standard-errors-1.html#cb83-11" tabindex="-1"></a></span>
<span id="cb83-12"><a href="standard-errors-1.html#cb83-12" tabindex="-1"></a></span>
<span id="cb83-13"><a href="standard-errors-1.html#cb83-13" tabindex="-1"></a><span class="co"># compare it with standard errors from statsmodels</span></span>
<span id="cb83-14"><a href="standard-errors-1.html#cb83-14" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb83-15"><a href="standard-errors-1.html#cb83-15" tabindex="-1"></a><span class="co"># se from stats model</span></span>
<span id="cb83-16"><a href="standard-errors-1.html#cb83-16" tabindex="-1"></a>X_sm <span class="op">=</span> sm.add_constant(X)   <span class="co"># add intercept</span></span>
<span id="cb83-17"><a href="standard-errors-1.html#cb83-17" tabindex="-1"></a>model <span class="op">=</span> sm.OLS(y, X_sm).fit()</span>
<span id="cb83-18"><a href="standard-errors-1.html#cb83-18" tabindex="-1"></a></span>
<span id="cb83-19"><a href="standard-errors-1.html#cb83-19" tabindex="-1"></a></span>
<span id="cb83-20"><a href="standard-errors-1.html#cb83-20" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The estimates from statmodels: </span><span class="sc">{</span>model<span class="sc">.</span>params<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)   <span class="co"># coefficients</span></span>
<span id="cb83-21"><a href="standard-errors-1.html#cb83-21" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The estimates from sklearn is: </span><span class="sc">{</span>reg<span class="sc">.</span>coef_<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb83-22"><a href="standard-errors-1.html#cb83-22" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The estimates from manual GD is: </span><span class="sc">{</span>theta<span class="sc">.</span>reshape((<span class="dv">1</span>, <span class="dv">3</span>))<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&#39;</span>) </span>
<span id="cb83-23"><a href="standard-errors-1.html#cb83-23" tabindex="-1"></a></span>
<span id="cb83-24"><a href="standard-errors-1.html#cb83-24" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard errors from statmodel under homoskedasticity: </span><span class="sc">{</span>model<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)      <span class="co"># standard errors</span></span>
<span id="cb83-25"><a href="standard-errors-1.html#cb83-25" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard errors (manual estimation) under homoskedasticity: </span><span class="sc">{</span>manual_se<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>standard error of the regression is: [[3048.97689642]] 
 

The estimates from statmodels: [39987.2777  9991.4859   695.5676]
The estimates from sklearn is: [[39987.2777  9991.4859   695.5676]]
The estimates from manual GD is: [[39987.2777  9991.4859   695.5676]] 
 

standard errors from statmodel under homoskedasticity: [30.4898 33.9277 33.9277]
standard errors (manual estimation) under homoskedasticity: [30.4898 33.9277 33.9277] 
 </code></pre>
<p>Now that we have estimated standard errors under the homoskedasticity assumption let’s see how we can fix this. Before we move on, I want to reiterate that the origin of heteroskedasicity is due to error terms coming from the distribution of different variance. In that regard, we want to account for this in our estimation of standard errors.</p>
<p>To do so, we’ll use a sandwich method, which you should’ve heard from you previous classes. So what does it entail? Basically, we’d want to form weights using the size of the error term.</p>
<p>Let’s just derive the sandwich estimator:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} = (X^{T}X)^{-1}X^{T}Y \\
\hat{\beta} = (X^{T}X)^{-1}X^{T}(X\beta + \epsilon) \\
\hat{\beta} = \beta + (X^{T}X)^{-1}X^{T}\epsilon
\end{align}\]</span></p>
<p>The end line is the starting point. Note that <span class="math inline">\(Var(a)=0\)</span> and <span class="math inline">\(Var(ax) = a^{2}Var(x)\)</span>, where <span class="math inline">\(a\)</span> is a constant and <span class="math inline">\(x\)</span> is a random variable. Let’s then take the variance of <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p><span class="math display">\[\begin{align}
Var(\hat{\beta}) = Var((X^{T}X)^{-1}X^{T}\epsilon) \\
= (X^{T}X)^{-1}Var(X^{T}\epsilon)(X^{T}X)^{-1} \\
= (X^{T}X)^{-1}X^{T}Var(\epsilon)X(X^{T}X)^{-1} \\
= (X^{T}X)^{-1}X^{T} \Omega X(X^{T}X)^{-1}
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(\Omega\)</span> is the scaling term and it is <span class="math inline">\(\epsilon\epsilon^{T}.I\)</span>, which is a <span class="math inline">\(n \times n\)</span> diagonal matrix. We don’t observe this, so replace this with the sample counterpart <span class="math inline">\(\hat{\Omega} = \hat{\epsilon}\hat{\epsilon}^{T}I\)</span>. Notice that under the case of homoskedasticity <span class="math inline">\(\Omega = \sigma^2 I\)</span>, which collapses <span class="math inline">\(Var{\hat{\beta}}\)</span> to <span class="math inline">\((X^{T}X)^{-1} \sigma^2\)</span>. This is what we used to estimate standard errors under homoskedasticity.</p>
<p>Let’s estimate the standard errors accounting for heteroskedasicity. I’m going to divide this into bun and stuffing part as I write the code.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="standard-errors-1.html#cb85-1" tabindex="-1"></a>bun <span class="op">=</span> np.linalg.inv(X.T<span class="op">@</span>X)</span>
<span id="cb85-2"><a href="standard-errors-1.html#cb85-2" tabindex="-1"></a>omega <span class="op">=</span> np.diag((residual<span class="op">**</span><span class="dv">2</span>).ravel()) <span class="co"># n * n diagonal matrix</span></span>
<span id="cb85-3"><a href="standard-errors-1.html#cb85-3" tabindex="-1"></a>stuff <span class="op">=</span> X.T <span class="op">@</span> omega <span class="op">@</span> X</span>
<span id="cb85-4"><a href="standard-errors-1.html#cb85-4" tabindex="-1"></a></span>
<span id="cb85-5"><a href="standard-errors-1.html#cb85-5" tabindex="-1"></a><span class="co"># get the variance covariance matrix</span></span>
<span id="cb85-6"><a href="standard-errors-1.html#cb85-6" tabindex="-1"></a>var_cov <span class="op">=</span> bun <span class="op">@</span> stuff <span class="op">@</span> bun</span>
<span id="cb85-7"><a href="standard-errors-1.html#cb85-7" tabindex="-1"></a>robust_se <span class="op">=</span> np.sqrt(np.diag(var_cov))</span>
<span id="cb85-8"><a href="standard-errors-1.html#cb85-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;heteroskedasticity robust standard error: </span><span class="sc">{</span>robust_se<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb85-9"><a href="standard-errors-1.html#cb85-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;se under homoskedastic assumption: </span><span class="sc">{</span>manual_se<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb85-10"><a href="standard-errors-1.html#cb85-10" tabindex="-1"></a></span>
<span id="cb85-11"><a href="standard-errors-1.html#cb85-11" tabindex="-1"></a><span class="co"># robust se using sm</span></span>
<span id="cb85-12"><a href="standard-errors-1.html#cb85-12" tabindex="-1"></a>model2 <span class="op">=</span> sm.OLS(y, X).fit(cov_type<span class="op">=</span><span class="st">&#39;HC0&#39;</span>)</span>
<span id="cb85-13"><a href="standard-errors-1.html#cb85-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;standard errors from stats model under heteroskedasticity: </span><span class="sc">{</span>model2<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>heteroskedasticity robust standard error: [30.4852 36.4511 34.2139]
se under homoskedastic assumption: [30.4898 33.9277 33.9277]
standard errors from stats model under heteroskedasticity: [30.4852 36.4511 34.2139]</code></pre>
<p>If you compare the standard errors under homoskedasticity versus heteroskedasticity, you will see that standard errors under heteroskedasticity are larger, particularly for the education estimate.</p>
<p>Note that there are various forms of heteroskedasicity robust standard error including “HC1”, “HC2”, and “HC3”. Here, we’ve used the “H0” type – you can dig deeper according to your need.</p>
<p>Let’s now discuss the case where error terms are not independent and are correlated. One can think of this in a geospatial form – the unexplained portion of income for people living in a particular area can be correlated. For example, if you have people living in Des Moines, Iowa and NYC in your sample, you’ll probably think that income is spatially correlated due to local market conditions, taste, cost of living and other unobserved factors attributing to spatial clustering.</p>
<p>In the following simulation exercise, we’ll incorporate this cluter-type correlation in the error terms. Specifically, we’ll build error as:</p>
<p><span class="math display">\[\begin{equation}
\epsilon_{ic} = u_{c} + v_{i}
\end{equation}\]</span></p>
<p>Essentially, the error term consists of: i) <span class="math inline">\(u_c\)</span> – the cluster specific shock (all units within a specific cluster gets the same value); and ii) individual specific term. Both <span class="math inline">\(u_c\)</span> and <span class="math inline">\(v_i\)</span> will come from a normal distribution with mean 0 and standard deviation 3,000 and 1,000, respectively.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="standard-errors-1.html#cb87-1" tabindex="-1"></a><span class="co"># number of clusters</span></span>
<span id="cb87-2"><a href="standard-errors-1.html#cb87-2" tabindex="-1"></a>nc <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb87-3"><a href="standard-errors-1.html#cb87-3" tabindex="-1"></a></span>
<span id="cb87-4"><a href="standard-errors-1.html#cb87-4" tabindex="-1"></a><span class="co"># number of units within cluster </span></span>
<span id="cb87-5"><a href="standard-errors-1.html#cb87-5" tabindex="-1"></a>ni <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb87-6"><a href="standard-errors-1.html#cb87-6" tabindex="-1"></a></span>
<span id="cb87-7"><a href="standard-errors-1.html#cb87-7" tabindex="-1"></a><span class="co"># total number of units</span></span>
<span id="cb87-8"><a href="standard-errors-1.html#cb87-8" tabindex="-1"></a>m <span class="op">=</span> nc <span class="op">*</span> ni</span>
<span id="cb87-9"><a href="standard-errors-1.html#cb87-9" tabindex="-1"></a></span>
<span id="cb87-10"><a href="standard-errors-1.html#cb87-10" tabindex="-1"></a><span class="co"># get the cluster shock</span></span>
<span id="cb87-11"><a href="standard-errors-1.html#cb87-11" tabindex="-1"></a>cluster_shock <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">3000</span>, nc)</span>
<span id="cb87-12"><a href="standard-errors-1.html#cb87-12" tabindex="-1"></a>cluster_index <span class="op">=</span> np.repeat(np.linspace(<span class="dv">1</span>, nc), ni).astype(<span class="st">&#39;int&#39;</span>)</span>
<span id="cb87-13"><a href="standard-errors-1.html#cb87-13" tabindex="-1"></a>u_shock <span class="op">=</span> cluster_shock[cluster_index<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb87-14"><a href="standard-errors-1.html#cb87-14" tabindex="-1"></a></span>
<span id="cb87-15"><a href="standard-errors-1.html#cb87-15" tabindex="-1"></a><span class="co"># get the individuals shock</span></span>
<span id="cb87-16"><a href="standard-errors-1.html#cb87-16" tabindex="-1"></a><span class="co"># in this case cluster shock dominates individual shock</span></span>
<span id="cb87-17"><a href="standard-errors-1.html#cb87-17" tabindex="-1"></a>v_shock <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1000</span>, m)</span>
<span id="cb87-18"><a href="standard-errors-1.html#cb87-18" tabindex="-1"></a></span>
<span id="cb87-19"><a href="standard-errors-1.html#cb87-19" tabindex="-1"></a>error <span class="op">=</span> u_shock <span class="op">+</span> v_shock</span></code></pre></div>
<p>Let’s estimate the model and plot the relationship between education and residuals by some clusters.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="standard-errors-1.html#cb88-1" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((educa_scaled.reshape((m, <span class="dv">1</span>)), ability_scaled.reshape((m,<span class="dv">1</span>))), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb88-2"><a href="standard-errors-1.html#cb88-2" tabindex="-1"></a>X <span class="op">=</span> add_dummy_feature(X)</span>
<span id="cb88-3"><a href="standard-errors-1.html#cb88-3" tabindex="-1"></a>y_c <span class="op">=</span> <span class="dv">40000</span> <span class="op">+</span> <span class="dv">10000</span> <span class="op">*</span> educa_scaled <span class="op">+</span> <span class="dv">700</span> <span class="op">*</span> ability_scaled <span class="op">+</span> error.ravel()</span>
<span id="cb88-4"><a href="standard-errors-1.html#cb88-4" tabindex="-1"></a>y_c <span class="op">=</span> y_c.reshape((m, <span class="dv">1</span>))</span>
<span id="cb88-5"><a href="standard-errors-1.html#cb88-5" tabindex="-1"></a></span>
<span id="cb88-6"><a href="standard-errors-1.html#cb88-6" tabindex="-1"></a><span class="co"># initialize theta</span></span>
<span id="cb88-7"><a href="standard-errors-1.html#cb88-7" tabindex="-1"></a>theta <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, p).reshape((p, <span class="dv">1</span>))</span>
<span id="cb88-8"><a href="standard-errors-1.html#cb88-8" tabindex="-1"></a><span class="co"># learning rate</span></span>
<span id="cb88-9"><a href="standard-errors-1.html#cb88-9" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb88-10"><a href="standard-errors-1.html#cb88-10" tabindex="-1"></a><span class="co"># number of iterations</span></span>
<span id="cb88-11"><a href="standard-errors-1.html#cb88-11" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb88-12"><a href="standard-errors-1.html#cb88-12" tabindex="-1"></a></span>
<span id="cb88-13"><a href="standard-errors-1.html#cb88-13" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb88-14"><a href="standard-errors-1.html#cb88-14" tabindex="-1"></a>    <span class="co"># get the gradient and adjust theta against the gradient using the learning rate</span></span>
<span id="cb88-15"><a href="standard-errors-1.html#cb88-15" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X.T<span class="op">@</span>(y_c <span class="op">-</span> X<span class="op">@</span>theta)</span>
<span id="cb88-16"><a href="standard-errors-1.html#cb88-16" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">-</span> eta<span class="op">*</span>gradient </span>
<span id="cb88-17"><a href="standard-errors-1.html#cb88-17" tabindex="-1"></a></span>
<span id="cb88-18"><a href="standard-errors-1.html#cb88-18" tabindex="-1"></a><span class="bu">print</span>(theta)</span>
<span id="cb88-19"><a href="standard-errors-1.html#cb88-19" tabindex="-1"></a></span>
<span id="cb88-20"><a href="standard-errors-1.html#cb88-20" tabindex="-1"></a>resid_clus <span class="op">=</span> y_c <span class="op">-</span> X<span class="op">@</span>theta </span>
<span id="cb88-21"><a href="standard-errors-1.html#cb88-21" tabindex="-1"></a></span>
<span id="cb88-22"><a href="standard-errors-1.html#cb88-22" tabindex="-1"></a></span>
<span id="cb88-23"><a href="standard-errors-1.html#cb88-23" tabindex="-1"></a><span class="co"># then re-generate income </span></span>
<span id="cb88-24"><a href="standard-errors-1.html#cb88-24" tabindex="-1"></a></span>
<span id="cb88-25"><a href="standard-errors-1.html#cb88-25" tabindex="-1"></a>error_clus <span class="op">=</span> income <span class="op">-</span> (<span class="dv">40000</span> <span class="op">+</span> <span class="dv">10000</span> <span class="op">*</span> educa_scaled <span class="op">+</span> <span class="dv">700</span> <span class="op">*</span> ability_scaled)</span>
<span id="cb88-26"><a href="standard-errors-1.html#cb88-26" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb88-27"><a href="standard-errors-1.html#cb88-27" tabindex="-1"></a>plt.scatter(educa[cluster_index<span class="op">==</span><span class="dv">1</span>], resid_clus[cluster_index<span class="op">==</span><span class="dv">1</span>], color <span class="op">=</span> <span class="st">&#39;red&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, label<span class="op">=</span><span class="st">&#39;Cluster 1&#39;</span>)</span>
<span id="cb88-28"><a href="standard-errors-1.html#cb88-28" tabindex="-1"></a>plt.scatter(educa[cluster_index<span class="op">==</span><span class="dv">10</span>], resid_clus[cluster_index<span class="op">==</span><span class="dv">10</span>], color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, label<span class="op">=</span><span class="st">&#39;Cluster 10&#39;</span>)</span>
<span id="cb88-29"><a href="standard-errors-1.html#cb88-29" tabindex="-1"></a>plt.scatter(educa[cluster_index<span class="op">==</span><span class="dv">20</span>], resid_clus[cluster_index<span class="op">==</span><span class="dv">20</span>], color <span class="op">=</span> <span class="st">&#39;green&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, label<span class="op">=</span><span class="st">&#39;Cluster 20&#39;</span>)</span>
<span id="cb88-30"><a href="standard-errors-1.html#cb88-30" tabindex="-1"></a>plt.legend()</span>
<span id="cb88-31"><a href="standard-errors-1.html#cb88-31" tabindex="-1"></a>plt.title(<span class="st">&#39;Relationship between schooling and residuals </span><span class="ch">\n</span><span class="st"> by some chosen cluster&#39;</span>)</span>
<span id="cb88-32"><a href="standard-errors-1.html#cb88-32" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;schooling&#39;</span>)</span>
<span id="cb88-33"><a href="standard-errors-1.html#cb88-33" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;residuals&#39;</span>)</span></code></pre></div>
<pre><code>[[40216.91715969]
 [ 9940.69952697]
 [  754.42493803]]





Text(0, 0.5, &#39;residuals&#39;)</code></pre>
<div class="float">
<img src="standard_errors_files/standard_errors_18_2.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>In the figure above, we see that residuals across different clusters are grouped at certain residual values. This depicts the clustering problem. Now, we need to adjust our the standard errors to account for this kind of clustering. The variance now takes the form:</p>
<p><span class="math display">\[\begin{equation}
Var(\hat \beta) = (X^{T}X)^{-1} \bigg(\sum_{c=1}^{G} X^{T}_c \epsilon_c \epsilon^{T}_c X_c \bigg) (X^{T}X)^{-1}
\end{equation}\]</span></p>
<p>Let’s estimate this!!</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="standard-errors-1.html#cb90-1" tabindex="-1"></a>bun <span class="op">=</span> np.linalg.inv(X.T<span class="op">@</span>X)</span>
<span id="cb90-2"><a href="standard-errors-1.html#cb90-2" tabindex="-1"></a>stuff <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>]<span class="op">*</span>X.shape[<span class="dv">1</span>]).reshape((X.shape[<span class="dv">1</span>], X.shape[<span class="dv">1</span>]))</span>
<span id="cb90-3"><a href="standard-errors-1.html#cb90-3" tabindex="-1"></a></span>
<span id="cb90-4"><a href="standard-errors-1.html#cb90-4" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, nc<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb90-5"><a href="standard-errors-1.html#cb90-5" tabindex="-1"></a></span>
<span id="cb90-6"><a href="standard-errors-1.html#cb90-6" tabindex="-1"></a>    X_c <span class="op">=</span> X[cluster_index<span class="op">==</span>i]</span>
<span id="cb90-7"><a href="standard-errors-1.html#cb90-7" tabindex="-1"></a>    res_c <span class="op">=</span> resid_clus[cluster_index<span class="op">==</span>i]</span>
<span id="cb90-8"><a href="standard-errors-1.html#cb90-8" tabindex="-1"></a></span>
<span id="cb90-9"><a href="standard-errors-1.html#cb90-9" tabindex="-1"></a>    mid <span class="op">=</span> X_c.T<span class="op">@</span>res_c<span class="op">@</span>res_c.T<span class="op">@</span>X_c</span>
<span id="cb90-10"><a href="standard-errors-1.html#cb90-10" tabindex="-1"></a>    stuff <span class="op">+=</span> mid</span>
<span id="cb90-11"><a href="standard-errors-1.html#cb90-11" tabindex="-1"></a></span>
<span id="cb90-12"><a href="standard-errors-1.html#cb90-12" tabindex="-1"></a>var_cov_clus <span class="op">=</span> bun<span class="op">@</span>stuff<span class="op">@</span>bun</span>
<span id="cb90-13"><a href="standard-errors-1.html#cb90-13" tabindex="-1"></a>se_clus <span class="op">=</span> np.sqrt(np.diag(var_cov_clus))</span>
<span id="cb90-14"><a href="standard-errors-1.html#cb90-14" tabindex="-1"></a></span>
<span id="cb90-15"><a href="standard-errors-1.html#cb90-15" tabindex="-1"></a></span>
<span id="cb90-16"><a href="standard-errors-1.html#cb90-16" tabindex="-1"></a></span>
<span id="cb90-17"><a href="standard-errors-1.html#cb90-17" tabindex="-1"></a><span class="co"># small sample correction term</span></span>
<span id="cb90-18"><a href="standard-errors-1.html#cb90-18" tabindex="-1"></a>correction <span class="op">=</span> ((nc<span class="op">/</span>(nc<span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> (m<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>(m<span class="op">-</span>p))</span>
<span id="cb90-19"><a href="standard-errors-1.html#cb90-19" tabindex="-1"></a>var_cov_clust_corr <span class="op">=</span> var_cov_clus <span class="op">*</span> correction</span>
<span id="cb90-20"><a href="standard-errors-1.html#cb90-20" tabindex="-1"></a>se_clus_correct <span class="op">=</span> np.sqrt(np.diag(var_cov_clust_corr))</span>
<span id="cb90-21"><a href="standard-errors-1.html#cb90-21" tabindex="-1"></a></span>
<span id="cb90-22"><a href="standard-errors-1.html#cb90-22" tabindex="-1"></a>model_clus <span class="op">=</span> sm.OLS(y_c, X).fit(cov_type<span class="op">=</span><span class="st">&#39;cluster&#39;</span>, cov_kwds<span class="op">=</span>{<span class="st">&#39;groups&#39;</span>: cluster_index})</span>
<span id="cb90-23"><a href="standard-errors-1.html#cb90-23" tabindex="-1"></a>model_noclus <span class="op">=</span> sm.OLS(y_c, X).fit()</span>
<span id="cb90-24"><a href="standard-errors-1.html#cb90-24" tabindex="-1"></a></span>
<span id="cb90-25"><a href="standard-errors-1.html#cb90-25" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Clustered standard errors (not small sample corrected): </span><span class="sc">{</span>se_clus<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb90-26"><a href="standard-errors-1.html#cb90-26" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Clustered se corrected for small sample: </span><span class="sc">{</span>se_clus_correct<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb90-27"><a href="standard-errors-1.html#cb90-27" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Clustered standard error from statsmodel: </span><span class="sc">{</span>model_clus<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb90-28"><a href="standard-errors-1.html#cb90-28" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;standard error without clustering from statsmodel: </span><span class="sc">{</span>model_noclus<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>Clustered standard errors (not small sample corrected): [452.4901  38.7462  35.9821]
Clustered se corrected for small sample: [457.1298  39.1435  36.3511]
Clustered standard error from statsmodel: [457.1298  39.1435  36.3511]
standard error without clustering from statsmodel: [33.5377 37.3193 37.3193]</code></pre>
<p>We’ve now computed the clustered standard errors and compared it with those from statsmodel. They are exactly the same. However, clustered standard errors are larger compared to non-clustered standard errors.</p>
<p><strong>Bootstrapped standard errors</strong></p>
<p>Bootstapping can lead to a convenient way of obtaining standard errors. The bootstrapping process assumes
the sample as the population and performs resampling <span class="math inline">\(k\)</span> number of times from the population (with replacement). For each re-sampling we estimates the coefficients. With this process, you’ll then have <span class="math inline">\(k\)</span> number of estimates. Then the standard error
is simply the standard deviation of the estimates.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="standard-errors-1.html#cb92-1" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb92-2"><a href="standard-errors-1.html#cb92-2" tabindex="-1"></a></span>
<span id="cb92-3"><a href="standard-errors-1.html#cb92-3" tabindex="-1"></a>rep <span class="op">=</span> <span class="dv">199</span></span>
<span id="cb92-4"><a href="standard-errors-1.html#cb92-4" tabindex="-1"></a><span class="co"># learning rate</span></span>
<span id="cb92-5"><a href="standard-errors-1.html#cb92-5" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb92-6"><a href="standard-errors-1.html#cb92-6" tabindex="-1"></a><span class="co"># number of iterations</span></span>
<span id="cb92-7"><a href="standard-errors-1.html#cb92-7" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb92-8"><a href="standard-errors-1.html#cb92-8" tabindex="-1"></a></span>
<span id="cb92-9"><a href="standard-errors-1.html#cb92-9" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, rep):</span>
<span id="cb92-10"><a href="standard-errors-1.html#cb92-10" tabindex="-1"></a></span>
<span id="cb92-11"><a href="standard-errors-1.html#cb92-11" tabindex="-1"></a>    boot_index <span class="op">=</span> np.random.choice(X.shape[<span class="dv">0</span>], size<span class="op">=</span>X.shape[<span class="dv">0</span>], replace<span class="op">=</span><span class="va">True</span>).astype(<span class="st">&#39;int&#39;</span>)</span>
<span id="cb92-12"><a href="standard-errors-1.html#cb92-12" tabindex="-1"></a>    boot_index <span class="op">=</span> np.array(boot_index) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb92-13"><a href="standard-errors-1.html#cb92-13" tabindex="-1"></a>    X_boot <span class="op">=</span> X[boot_index] </span>
<span id="cb92-14"><a href="standard-errors-1.html#cb92-14" tabindex="-1"></a>    y_boot <span class="op">=</span> y_c[boot_index]</span>
<span id="cb92-15"><a href="standard-errors-1.html#cb92-15" tabindex="-1"></a></span>
<span id="cb92-16"><a href="standard-errors-1.html#cb92-16" tabindex="-1"></a>    <span class="co"># initialize theta</span></span>
<span id="cb92-17"><a href="standard-errors-1.html#cb92-17" tabindex="-1"></a>    theta <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, p).reshape((p, <span class="dv">1</span>))</span>
<span id="cb92-18"><a href="standard-errors-1.html#cb92-18" tabindex="-1"></a></span>
<span id="cb92-19"><a href="standard-errors-1.html#cb92-19" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb92-20"><a href="standard-errors-1.html#cb92-20" tabindex="-1"></a>    <span class="co"># get the gradient and adjust theta against the gradient using the learning rate</span></span>
<span id="cb92-21"><a href="standard-errors-1.html#cb92-21" tabindex="-1"></a>        gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>X_boot.shape[<span class="dv">0</span>] <span class="op">*</span> X_boot.T<span class="op">@</span>(y_boot <span class="op">-</span> X_boot<span class="op">@</span>theta)</span>
<span id="cb92-22"><a href="standard-errors-1.html#cb92-22" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> eta<span class="op">*</span>gradient </span>
<span id="cb92-23"><a href="standard-errors-1.html#cb92-23" tabindex="-1"></a></span>
<span id="cb92-24"><a href="standard-errors-1.html#cb92-24" tabindex="-1"></a>    <span class="cf">if</span> k <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb92-25"><a href="standard-errors-1.html#cb92-25" tabindex="-1"></a>        theta_store <span class="op">=</span> theta.ravel()</span>
<span id="cb92-26"><a href="standard-errors-1.html#cb92-26" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb92-27"><a href="standard-errors-1.html#cb92-27" tabindex="-1"></a>        theta_new <span class="op">=</span> theta.ravel()</span>
<span id="cb92-28"><a href="standard-errors-1.html#cb92-28" tabindex="-1"></a>        theta_store <span class="op">=</span> np.vstack((theta_store, theta_new))    </span>
<span id="cb92-29"><a href="standard-errors-1.html#cb92-29" tabindex="-1"></a></span>
<span id="cb92-30"><a href="standard-errors-1.html#cb92-30" tabindex="-1"></a>    <span class="co">#print(f&#39;Rep {k} estimate: {theta}&#39;)</span></span>
<span id="cb92-31"><a href="standard-errors-1.html#cb92-31" tabindex="-1"></a></span>
<span id="cb92-32"><a href="standard-errors-1.html#cb92-32" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb92-33"><a href="standard-errors-1.html#cb92-33" tabindex="-1"></a>plt.hist(theta_store[:,<span class="dv">1</span>], bins <span class="op">=</span> <span class="dv">30</span>, color<span class="op">=</span><span class="st">&#39;steelblue&#39;</span>, edgecolor<span class="op">=</span><span class="st">&#39;black&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb92-34"><a href="standard-errors-1.html#cb92-34" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Schooling estimates&#39;</span>)</span>
<span id="cb92-35"><a href="standard-errors-1.html#cb92-35" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Frequency&#39;</span>)</span>
<span id="cb92-36"><a href="standard-errors-1.html#cb92-36" tabindex="-1"></a>plt.title(<span class="ss">f&#39;Histogram of bootstrapped estimate </span><span class="ch">\n</span><span class="ss"> from </span><span class="sc">{</span>rep<span class="sc">}</span><span class="ss"> replications.&#39;</span>)</span>
<span id="cb92-37"><a href="standard-errors-1.html#cb92-37" tabindex="-1"></a></span>
<span id="cb92-38"><a href="standard-errors-1.html#cb92-38" tabindex="-1"></a></span>
<span id="cb92-39"><a href="standard-errors-1.html#cb92-39" tabindex="-1"></a>boot_se <span class="op">=</span> theta_store.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb92-40"><a href="standard-errors-1.html#cb92-40" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;bootstrapped non-clustered version of se: </span><span class="sc">{</span>boot_se<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb92-41"><a href="standard-errors-1.html#cb92-41" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;standard error without clustering from statsmodel: </span><span class="sc">{</span>model_noclus<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>bootstrapped non-clustered version of se: [30.6567 37.8813 38.6351]
standard error without clustering from statsmodel: [33.5377 37.3193 37.3193]</code></pre>
<div class="float">
<img src="standard_errors_files/standard_errors_22_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>There you have it – the bootstrapped version of the standard errors. Note that we’ve not accounted for the clusters and one can do that by sampling clusters (with replacement) rather than units.</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-and-gradient-descent.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  },
  "toolbar": {
    "position": "static"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
