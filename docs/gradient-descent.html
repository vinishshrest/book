<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Gradient Descent | Causal Inference</title>
  <meta name="description" content="4.2 Gradient Descent | Causal Inference" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Gradient Descent | Causal Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Gradient Descent | Causal Inference" />
  
  
  

<meta name="author" content="Vinish Shrestha" />


<meta name="date" content="2026-01-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="normal-equation.html"/>
<link rel="next" href="multivariate-model.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="a-lab-experiment.html"><a href="a-lab-experiment.html"><i class="fa fa-check"></i><b>2.1</b> A lab experiment</a></li>
<li class="chapter" data-level="2.2" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.2</b> Challenges</a></li>
<li class="chapter" data-level="2.3" data-path="dag-directed-acyclic-graph.html"><a href="dag-directed-acyclic-graph.html"><i class="fa fa-check"></i><b>2.3</b> DAG (Directed Acyclic Graph)</a></li>
<li class="chapter" data-level="2.4" data-path="a-simulated-dgp.html"><a href="a-simulated-dgp.html"><i class="fa fa-check"></i><b>2.4</b> A simulated DGP</a></li>
<li class="chapter" data-level="2.5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="why-regression.html"><a href="why-regression.html"><i class="fa fa-check"></i><b>3</b> Why Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-best-fit-line.html"><a href="the-best-fit-line.html"><i class="fa fa-check"></i><b>3.1</b> The best-fit line</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression-specification.html"><a href="linear-regression-specification.html"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Specification</a></li>
<li class="chapter" data-level="3.3" data-path="law-of-iterated-expectation.html"><a href="law-of-iterated-expectation.html"><i class="fa fa-check"></i><b>3.3</b> Law of iterated expectation</a></li>
<li class="chapter" data-level="3.4" data-path="error-term.html"><a href="error-term.html"><i class="fa fa-check"></i><b>3.4</b> Error term</a></li>
<li class="chapter" data-level="3.5" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>3.5</b> Decomposition</a></li>
<li class="chapter" data-level="3.6" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>3.6</b> Estimation</a></li>
<li class="chapter" data-level="3.7" data-path="running-a-regression.html"><a href="running-a-regression.html"><i class="fa fa-check"></i><b>3.7</b> Running a regression</a></li>
<li class="chapter" data-level="3.8" data-path="standard-errors.html"><a href="standard-errors.html"><i class="fa fa-check"></i><b>3.8</b> Standard Errors</a></li>
<li class="chapter" data-level="3.9" data-path="an-exercise.html"><a href="an-exercise.html"><i class="fa fa-check"></i><b>3.9</b> An exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression-and-gradient-descent.html"><a href="regression-and-gradient-descent.html"><i class="fa fa-check"></i><b>4</b> Regression and Gradient Descent</a>
<ul>
<li class="chapter" data-level="4.1" data-path="normal-equation.html"><a href="normal-equation.html"><i class="fa fa-check"></i><b>4.1</b> Normal Equation</a></li>
<li class="chapter" data-level="4.2" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>4.2</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.3" data-path="multivariate-model.html"><a href="multivariate-model.html"><i class="fa fa-check"></i><b>4.3</b> Multivariate model</a></li>
<li class="chapter" data-level="4.4" data-path="standard-error.html"><a href="standard-error.html"><i class="fa fa-check"></i><b>4.4</b> Standard Error</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>5</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="potential-outcome-framework-neyman-rubin-causal-model.html"><a href="potential-outcome-framework-neyman-rubin-causal-model.html"><i class="fa fa-check"></i><b>5.1</b> Potential Outcome Framework: Neyman-Rubin Causal Model</a></li>
<li class="chapter" data-level="5.2" data-path="average-treatment-effect-ate.html"><a href="average-treatment-effect-ate.html"><i class="fa fa-check"></i><b>5.2</b> Average treatment effect (ATE)</a></li>
<li class="chapter" data-level="5.3" data-path="rct.html"><a href="rct.html"><i class="fa fa-check"></i><b>5.3</b> RCT</a></li>
<li class="chapter" data-level="5.4" data-path="average-treatment-effect-on-the-treated-att.html"><a href="average-treatment-effect-on-the-treated-att.html"><i class="fa fa-check"></i><b>5.4</b> Average treatment effect on the treated (ATT)</a></li>
<li class="chapter" data-level="5.5" data-path="an-estimation-example.html"><a href="an-estimation-example.html"><i class="fa fa-check"></i><b>5.5</b> An estimation example</a></li>
<li class="chapter" data-level="5.6" data-path="unconfoundedness-assumption.html"><a href="unconfoundedness-assumption.html"><i class="fa fa-check"></i><b>5.6</b> Unconfoundedness assumption</a></li>
<li class="chapter" data-level="5.7" data-path="discussion-1.html"><a href="discussion-1.html"><i class="fa fa-check"></i><b>5.7</b> Discussion</a></li>
<li class="chapter" data-level="5.8" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>5.8</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ipw-and-aipw.html"><a href="ipw-and-aipw.html"><i class="fa fa-check"></i><b>6</b> IPW and AIPW</a>
<ul>
<li class="chapter" data-level="6.1" data-path="a-simple-example.html"><a href="a-simple-example.html"><i class="fa fa-check"></i><b>6.1</b> A simple example</a></li>
<li class="chapter" data-level="6.2" data-path="aggregated-estimator.html"><a href="aggregated-estimator.html"><i class="fa fa-check"></i><b>6.2</b> Aggregated Estimator</a></li>
<li class="chapter" data-level="6.3" data-path="propensity-score.html"><a href="propensity-score.html"><i class="fa fa-check"></i><b>6.3</b> Propensity score</a></li>
<li class="chapter" data-level="6.4" data-path="estimation-of-propensity-score.html"><a href="estimation-of-propensity-score.html"><i class="fa fa-check"></i><b>6.4</b> Estimation of propensity score</a></li>
<li class="chapter" data-level="6.5" data-path="using-cross-fitting-to-predict-propensity-score.html"><a href="using-cross-fitting-to-predict-propensity-score.html"><i class="fa fa-check"></i><b>6.5</b> Using cross-fitting to predict propensity score</a></li>
<li class="chapter" data-level="6.6" data-path="propensity-score-stratification.html"><a href="propensity-score-stratification.html"><i class="fa fa-check"></i><b>6.6</b> Propensity score stratification</a></li>
<li class="chapter" data-level="6.7" data-path="inverse-probability-weighting-ipw.html"><a href="inverse-probability-weighting-ipw.html"><i class="fa fa-check"></i><b>6.7</b> Inverse Probability Weighting (IPW)</a></li>
<li class="chapter" data-level="6.8" data-path="comparing-ipw-with-aggregated-estimate.html"><a href="comparing-ipw-with-aggregated-estimate.html"><i class="fa fa-check"></i><b>6.8</b> Comparing IPW with Aggregated Estimate</a></li>
<li class="chapter" data-level="6.9" data-path="aipw-and-estimation.html"><a href="aipw-and-estimation.html"><i class="fa fa-check"></i><b>6.9</b> AIPW and Estimation</a></li>
<li class="chapter" data-level="6.10" data-path="assessing-balance.html"><a href="assessing-balance.html"><i class="fa fa-check"></i><b>6.10</b> Assessing Balance</a></li>
<li class="chapter" data-level="6.11" data-path="cross-fitting.html"><a href="cross-fitting.html"><i class="fa fa-check"></i><b>6.11</b> Cross-fitting</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="difference-in-differences.html"><a href="difference-in-differences.html"><i class="fa fa-check"></i><b>7</b> Difference in Differences</a>
<ul>
<li class="chapter" data-level="7.1" data-path="a-quick-introduction.html"><a href="a-quick-introduction.html"><i class="fa fa-check"></i><b>7.1</b> A Quick Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="set-up.html"><a href="set-up.html"><i class="fa fa-check"></i><b>7.2</b> Set up</a></li>
<li class="chapter" data-level="7.3" data-path="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><a href="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><i class="fa fa-check"></i><b>7.3</b> An example: Evaluating the impact of Medicaid expansion on uninsured rate</a></li>
<li class="chapter" data-level="7.4" data-path="naive-estimator.html"><a href="naive-estimator.html"><i class="fa fa-check"></i><b>7.4</b> Naive estimator</a></li>
<li class="chapter" data-level="7.5" data-path="canonical-difference-in-differences-framework.html"><a href="canonical-difference-in-differences-framework.html"><i class="fa fa-check"></i><b>7.5</b> Canonical Difference in Differences Framework</a></li>
<li class="chapter" data-level="7.6" data-path="did-in-multi-period-set-up.html"><a href="did-in-multi-period-set-up.html"><i class="fa fa-check"></i><b>7.6</b> DiD in multi-period set up</a></li>
<li class="chapter" data-level="7.7" data-path="conditional-parallel-trend-assumption.html"><a href="conditional-parallel-trend-assumption.html"><i class="fa fa-check"></i><b>7.7</b> Conditional Parallel Trend Assumption</a></li>
<li class="chapter" data-level="7.8" data-path="some-concerns-with-controls.html"><a href="some-concerns-with-controls.html"><i class="fa fa-check"></i><b>7.8</b> Some concerns with controls</a></li>
<li class="chapter" data-level="7.9" data-path="the-2-times-2-difference-in-differences-estimate.html"><a href="the-2-times-2-difference-in-differences-estimate.html"><i class="fa fa-check"></i><b>7.9</b> The <span class="math inline">\(2 \times 2\)</span> Difference-in-Differences Estimate</a></li>
<li class="chapter" data-level="7.10" data-path="event-study-model.html"><a href="event-study-model.html"><i class="fa fa-check"></i><b>7.10</b> Event study model</a></li>
<li class="chapter" data-level="7.11" data-path="two-way-fixed-effect-twfe-revisited.html"><a href="two-way-fixed-effect-twfe-revisited.html"><i class="fa fa-check"></i><b>7.11</b> Two way fixed effect (TWFE) Revisited</a></li>
<li class="chapter" data-level="7.12" data-path="various-ways-of-estimation.html"><a href="various-ways-of-estimation.html"><i class="fa fa-check"></i><b>7.12</b> Various ways of estimation</a></li>
<li class="chapter" data-level="7.13" data-path="multi-period-multi-group-and-variation-in-treatment-timing.html"><a href="multi-period-multi-group-and-variation-in-treatment-timing.html"><i class="fa fa-check"></i><b>7.13</b> Multi Period, Multi Group and Variation in Treatment Timing</a></li>
<li class="chapter" data-level="7.14" data-path="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><a href="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><i class="fa fa-check"></i><b>7.14</b> Problem with TWFE in Multiple Group with Treatment Timing Variation</a></li>
<li class="chapter" data-level="7.15" data-path="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><a href="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><i class="fa fa-check"></i><b>7.15</b> What is TWFE Estimating when there is Treatment Timing Variation?</a></li>
<li class="chapter" data-level="7.16" data-path="assumptions-governing-twfedd-estimate.html"><a href="assumptions-governing-twfedd-estimate.html"><i class="fa fa-check"></i><b>7.16</b> Assumptions governing TWFEDD estimate</a></li>
<li class="chapter" data-level="7.17" data-path="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><a href="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><i class="fa fa-check"></i><b>7.17</b> How Does Treatment Effect Heterogeneity in Time Affect TWFE?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="causal-forest.html"><a href="causal-forest.html"><i class="fa fa-check"></i><b>8</b> Causal Forest</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="summary-of-grf.html"><a href="summary-of-grf.html"><i class="fa fa-check"></i><b>8.2</b> Summary of GRF</a></li>
<li class="chapter" data-level="8.3" data-path="motivation-for-causal-forests.html"><a href="motivation-for-causal-forests.html"><i class="fa fa-check"></i><b>8.3</b> Motivation for Causal Forests</a></li>
<li class="chapter" data-level="8.4" data-path="causal-forest-1.html"><a href="causal-forest-1.html"><i class="fa fa-check"></i><b>8.4</b> Causal Forest</a></li>
<li class="chapter" data-level="8.5" data-path="an-example-of-causal-forest.html"><a href="an-example-of-causal-forest.html"><i class="fa fa-check"></i><b>8.5</b> An example of causal forest</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>9</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="9.1" data-path="some-ways-to-estimate-cate.html"><a href="some-ways-to-estimate-cate.html"><i class="fa fa-check"></i><b>9.1</b> Some ways to estimate CATE</a></li>
<li class="chapter" data-level="9.2" data-path="estimation-1.html"><a href="estimation-1.html"><i class="fa fa-check"></i><b>9.2</b> Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="some-remarks-and-questions.html"><a href="some-remarks-and-questions.html"><i class="fa fa-check"></i><b>9.3</b> Some Remarks and Questions</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient-descent" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Gradient Descent<a href="gradient-descent.html#gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Imagine that you are standing at the top of a mountain and want to descend the mountain as quickly as possible. One simple way is to consider a few directions – north, south, east, and west – and evaluate the steepness (gradient). Then you’d want to take a small step towards the steepest direction, pause, and re-evaluate the steepness. Doing this repeatedly gets you to the bottom of the mountain as fast as possible.</p>
<p>The idea of gradient descent is similar in context to the aforementioned analogy. We’ve already been exposed to the idea of MSE and the objective of minimizing MSE. Instead of using the closed form normal equation to solve for the minimum of MSE, gradient descent uses <em>gradient</em> of MSE to adjust the estimates and move closer to the minimum.</p>
<p>The gradient is a vector of partial derivatives of MSE that points to the direction of steepest increase increase in MSE. Hence, to minimize the loss, we’d want to move in opposite direction of the gradient. By repeatedly updating our parameter in this way, we move closer and closer to the minimum of the loss function. To simply the concept, we’ll start with the univariate case without the intercept.</p>
<p><span class="math display">\[
\begin{aligned}
Y_i = \beta X_i + \epsilon_i
\end{aligned}
\]</span></p>
<p>The MSE and the derivative is:</p>
<p><span class="math display">\[
\begin{aligned}
MSE(\beta) = \frac{1}{m} (Y - \beta X)^{T}(Y - \beta X)  \\
\frac{\partial{MSE}}{\partial{\beta}} = -\frac{2}{m} X^{T}(Y - \beta X)
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(\frac{2}{m}X^{T}(Y - \beta X)\)</span> is the gradient of the univariate specification, which informs the direction of the steepest increase in MSE. To reduce MSE, we therefore move in the opposite direction of the gradient.</p>
<p>Now that we have the gradient, the gradient descent algorithm can be set up as follows:
1. Start with an initial guesses of the parameter <span class="math inline">\((\beta_o)\)</span>.
2. Update estimates of parameters by moving to the opposite direction of the gradient.
<span class="math display">\[     
      \beta_{new} = \beta_o - \eta \times gradient_o.
      \]</span>
where, <span class="math inline">\(\eta\)</span> is the <em>learning rate</em>.
3. Re-evaluate the gradient using <span class="math inline">\(\beta_{new}\)</span>.
4. Repeat steps 2 and 3 for a given number of times or until convergence is reached.</p>
<p>Let’s simulate data for univariate model specification to visually see what this looks like.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="gradient-descent.html#cb10-1" tabindex="-1"></a>dat_uni <span class="op">=</span> make_regression(</span>
<span id="cb10-2"><a href="gradient-descent.html#cb10-2" tabindex="-1"></a>                n_samples<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb10-3"><a href="gradient-descent.html#cb10-3" tabindex="-1"></a>                n_features<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb10-4"><a href="gradient-descent.html#cb10-4" tabindex="-1"></a>                n_informative<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb10-5"><a href="gradient-descent.html#cb10-5" tabindex="-1"></a>                n_targets<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb10-6"><a href="gradient-descent.html#cb10-6" tabindex="-1"></a>                bias<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb10-7"><a href="gradient-descent.html#cb10-7" tabindex="-1"></a>                noise<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb10-8"><a href="gradient-descent.html#cb10-8" tabindex="-1"></a>                random_state<span class="op">=</span><span class="dv">42</span>, </span>
<span id="cb10-9"><a href="gradient-descent.html#cb10-9" tabindex="-1"></a>                coef<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-10"><a href="gradient-descent.html#cb10-10" tabindex="-1"></a>)</span>
<span id="cb10-11"><a href="gradient-descent.html#cb10-11" tabindex="-1"></a></span>
<span id="cb10-12"><a href="gradient-descent.html#cb10-12" tabindex="-1"></a>X_uni, y_uni, coef <span class="op">=</span> dat_uni</span>
<span id="cb10-13"><a href="gradient-descent.html#cb10-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The first five rows of X: </span><span class="sc">{</span>X_uni[<span class="dv">0</span>:<span class="dv">5</span>,:]<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb10-14"><a href="gradient-descent.html#cb10-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The first five y values: </span><span class="sc">{</span>y_uni[<span class="dv">0</span>:<span class="dv">5</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb10-15"><a href="gradient-descent.html#cb10-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The coefficient of univariate model is: </span><span class="sc">{</span>coef<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb10-16"><a href="gradient-descent.html#cb10-16" tabindex="-1"></a></span>
<span id="cb10-17"><a href="gradient-descent.html#cb10-17" tabindex="-1"></a><span class="co"># set up gradient descent </span></span>
<span id="cb10-18"><a href="gradient-descent.html#cb10-18" tabindex="-1"></a>beta <span class="op">=</span> <span class="op">-</span><span class="dv">5</span> <span class="co"># initialize beta</span></span>
<span id="cb10-19"><a href="gradient-descent.html#cb10-19" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># learning rate</span></span>
<span id="cb10-20"><a href="gradient-descent.html#cb10-20" tabindex="-1"></a>m <span class="op">=</span> X_uni.shape[<span class="dv">0</span>] <span class="co"># number of observations</span></span>
<span id="cb10-21"><a href="gradient-descent.html#cb10-21" tabindex="-1"></a>iter_val <span class="op">=</span> <span class="dv">100</span> <span class="co"># number of iteration steps</span></span>
<span id="cb10-22"><a href="gradient-descent.html#cb10-22" tabindex="-1"></a>y_uni <span class="op">=</span> y_uni.reshape((m,<span class="dv">1</span>)) <span class="co"># reshape into m*1 vector</span></span>
<span id="cb10-23"><a href="gradient-descent.html#cb10-23" tabindex="-1"></a>beta_store <span class="op">=</span> np.ones(m)</span>
<span id="cb10-24"><a href="gradient-descent.html#cb10-24" tabindex="-1"></a>loss_store <span class="op">=</span> np.ones(m)</span>
<span id="cb10-25"><a href="gradient-descent.html#cb10-25" tabindex="-1"></a></span>
<span id="cb10-26"><a href="gradient-descent.html#cb10-26" tabindex="-1"></a><span class="co"># loop </span></span>
<span id="cb10-27"><a href="gradient-descent.html#cb10-27" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iter_val):</span>
<span id="cb10-28"><a href="gradient-descent.html#cb10-28" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X_uni.T <span class="op">@</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni)</span>
<span id="cb10-29"><a href="gradient-descent.html#cb10-29" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni).T <span class="op">@</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni)</span>
<span id="cb10-30"><a href="gradient-descent.html#cb10-30" tabindex="-1"></a>    beta <span class="op">=</span> beta <span class="op">-</span> eta<span class="op">*</span>gradient</span>
<span id="cb10-31"><a href="gradient-descent.html#cb10-31" tabindex="-1"></a>    beta_store[i] <span class="op">=</span> beta.item() <span class="co"># use .item to extract scalar</span></span>
<span id="cb10-32"><a href="gradient-descent.html#cb10-32" tabindex="-1"></a>    loss_store[i] <span class="op">=</span> loss.item() <span class="co"># use .item to extract scalar</span></span>
<span id="cb10-33"><a href="gradient-descent.html#cb10-33" tabindex="-1"></a></span>
<span id="cb10-34"><a href="gradient-descent.html#cb10-34" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;gradient descent at work: </span><span class="sc">{</span>beta_store<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb10-35"><a href="gradient-descent.html#cb10-35" tabindex="-1"></a></span>
<span id="cb10-36"><a href="gradient-descent.html#cb10-36" tabindex="-1"></a><span class="co"># figure </span></span>
<span id="cb10-37"><a href="gradient-descent.html#cb10-37" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb10-38"><a href="gradient-descent.html#cb10-38" tabindex="-1"></a>plt.scatter(beta_store, loss_store)</span>
<span id="cb10-39"><a href="gradient-descent.html#cb10-39" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;beta value&quot;</span>)</span>
<span id="cb10-40"><a href="gradient-descent.html#cb10-40" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;loss&quot;</span>)</span>
<span id="cb10-41"><a href="gradient-descent.html#cb10-41" tabindex="-1"></a>plt.title(<span class="st">&quot;Gradient Descent at Work&quot;</span>)</span>
<span id="cb10-42"><a href="gradient-descent.html#cb10-42" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>The first five rows of X: [[ 0.19686124]
 [ 0.35711257]
 [-1.91328024]
 [-0.03582604]
 [ 0.76743473]] 
 

The first five y values: [  8.21720459  14.90627167 -79.86242262  -1.49541829  32.03357001] 
 

The coefficient of univariate model is: 41.7411003148779 
 

gradient descent at work: [ 2.73384129  9.18803147 14.57430322 19.06935566 22.82065107 25.95125242
 28.56386053 30.74418321 32.56374696 34.0822434  35.3494875  36.4070518
 37.28963019 38.02617604 38.64085208 39.15382306 39.58191721 39.93917836
 40.23732664 40.48614292 40.69378975 40.86707908 41.01169573 41.13238393
 41.23310291 41.3171568  41.38730303 41.44584277 41.49469646 41.53546675
 41.56949114 41.59788581 41.62158226 41.64135787 41.65786138 41.6716342
 41.68312815 41.6927203  41.70072532 41.70740581 41.71298095 41.71763361
 41.72151644 41.72475682 41.72746103 41.7297178  41.73160117 41.73317291
 41.73448459 41.73557923 41.73649276 41.73725513 41.73789136 41.73842232
 41.73886542 41.73923521 41.73954381 41.73980135 41.74001628 41.74019565
 41.74034533 41.74047025 41.74057451 41.74066151 41.74073411 41.7407947
 41.74084527 41.74088747 41.74092269 41.74095208 41.74097661 41.74099708
 41.74101416 41.74102841 41.74104031 41.74105024 41.74105852 41.74106544
 41.74107121 41.74107603 41.74108004 41.7410834  41.7410862  41.74108853
 41.74109048 41.74109211 41.74109347 41.7410946  41.74109555 41.74109633
 41.74109699 41.74109754 41.741098   41.74109838 41.7410987  41.74109897
 41.74109919 41.74109938 41.74109953 41.74109966] 
 </code></pre>
<div class="float">
<img src="01_Regression_Gradient_Descent_files/01_Regression_Gradient_Descent_16_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Here, we see that the algorithm converges at the estimated <span class="math inline">\(\beta\)</span> little over 40. Let’s print out the best beta from gradient descent and the true parameter for comparison.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="gradient-descent.html#cb12-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;True parameter of the univariate model: </span><span class="sc">{</span>coef<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> &quot;</span>)</span>
<span id="cb12-2"><a href="gradient-descent.html#cb12-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Best estimate from gradient descent: </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>) </span></code></pre></div>
<pre><code>True parameter of the univariate model: 41.7411003148779 
 
 
Best estimate from gradient descent: [[41.74109966]] 
 </code></pre>
<p>See that the estimate obtained from gradient descent is close to the true parameter.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="normal-equation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multivariate-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  },
  "toolbar": {
    "position": "static"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
