<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Regression and Gradient Descent | Causal Inference</title>
  <meta name="description" content="4 Regression and Gradient Descent | Causal Inference" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Regression and Gradient Descent | Causal Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Regression and Gradient Descent | Causal Inference" />
  
  
  

<meta name="author" content="Vinish Shrestha" />


<meta name="date" content="2026-02-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="an-exercise.html"/>
<link rel="next" href="causal-inference.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="assets/kePrint-0.0.1/kePrint.js"></script>
<link href="assets/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="a-lab-experiment.html"><a href="a-lab-experiment.html"><i class="fa fa-check"></i><b>2.1</b> A lab experiment</a></li>
<li class="chapter" data-level="2.2" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.2</b> Challenges</a></li>
<li class="chapter" data-level="2.3" data-path="dag-directed-acyclic-graph.html"><a href="dag-directed-acyclic-graph.html"><i class="fa fa-check"></i><b>2.3</b> DAG (Directed Acyclic Graph)</a></li>
<li class="chapter" data-level="2.4" data-path="a-simulated-dgp.html"><a href="a-simulated-dgp.html"><i class="fa fa-check"></i><b>2.4</b> A simulated DGP</a></li>
<li class="chapter" data-level="2.5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="why-regression.html"><a href="why-regression.html"><i class="fa fa-check"></i><b>3</b> Why Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-best-fit-line.html"><a href="the-best-fit-line.html"><i class="fa fa-check"></i><b>3.1</b> The best-fit line</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression-specification.html"><a href="linear-regression-specification.html"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Specification</a></li>
<li class="chapter" data-level="3.3" data-path="law-of-iterated-expectation.html"><a href="law-of-iterated-expectation.html"><i class="fa fa-check"></i><b>3.3</b> Law of iterated expectation</a></li>
<li class="chapter" data-level="3.4" data-path="error-term.html"><a href="error-term.html"><i class="fa fa-check"></i><b>3.4</b> Error term</a></li>
<li class="chapter" data-level="3.5" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>3.5</b> Decomposition</a></li>
<li class="chapter" data-level="3.6" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>3.6</b> Estimation</a></li>
<li class="chapter" data-level="3.7" data-path="running-a-regression.html"><a href="running-a-regression.html"><i class="fa fa-check"></i><b>3.7</b> Running a regression</a></li>
<li class="chapter" data-level="3.8" data-path="standard-errors.html"><a href="standard-errors.html"><i class="fa fa-check"></i><b>3.8</b> Standard Errors</a></li>
<li class="chapter" data-level="3.9" data-path="an-exercise.html"><a href="an-exercise.html"><i class="fa fa-check"></i><b>3.9</b> An exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression-and-gradient-descent.html"><a href="regression-and-gradient-descent.html"><i class="fa fa-check"></i><b>4</b> Regression and Gradient Descent</a></li>
<li class="chapter" data-level="5" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>5</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="potential-outcome-framework-neyman-rubin-causal-model.html"><a href="potential-outcome-framework-neyman-rubin-causal-model.html"><i class="fa fa-check"></i><b>5.1</b> Potential Outcome Framework: Neyman-Rubin Causal Model</a></li>
<li class="chapter" data-level="5.2" data-path="average-treatment-effect-ate.html"><a href="average-treatment-effect-ate.html"><i class="fa fa-check"></i><b>5.2</b> Average treatment effect (ATE)</a></li>
<li class="chapter" data-level="5.3" data-path="rct.html"><a href="rct.html"><i class="fa fa-check"></i><b>5.3</b> RCT</a></li>
<li class="chapter" data-level="5.4" data-path="average-treatment-effect-on-the-treated-att.html"><a href="average-treatment-effect-on-the-treated-att.html"><i class="fa fa-check"></i><b>5.4</b> Average treatment effect on the treated (ATT)</a></li>
<li class="chapter" data-level="5.5" data-path="an-estimation-example.html"><a href="an-estimation-example.html"><i class="fa fa-check"></i><b>5.5</b> An estimation example</a></li>
<li class="chapter" data-level="5.6" data-path="unconfoundedness-assumption.html"><a href="unconfoundedness-assumption.html"><i class="fa fa-check"></i><b>5.6</b> Unconfoundedness assumption</a></li>
<li class="chapter" data-level="5.7" data-path="discussion-1.html"><a href="discussion-1.html"><i class="fa fa-check"></i><b>5.7</b> Discussion</a></li>
<li class="chapter" data-level="5.8" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>5.8</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ipw-and-aipw.html"><a href="ipw-and-aipw.html"><i class="fa fa-check"></i><b>6</b> IPW and AIPW</a>
<ul>
<li class="chapter" data-level="6.1" data-path="a-simple-example.html"><a href="a-simple-example.html"><i class="fa fa-check"></i><b>6.1</b> A simple example</a></li>
<li class="chapter" data-level="6.2" data-path="aggregated-estimator.html"><a href="aggregated-estimator.html"><i class="fa fa-check"></i><b>6.2</b> Aggregated Estimator</a></li>
<li class="chapter" data-level="6.3" data-path="propensity-score.html"><a href="propensity-score.html"><i class="fa fa-check"></i><b>6.3</b> Propensity score</a></li>
<li class="chapter" data-level="6.4" data-path="estimation-of-propensity-score.html"><a href="estimation-of-propensity-score.html"><i class="fa fa-check"></i><b>6.4</b> Estimation of propensity score</a></li>
<li class="chapter" data-level="6.5" data-path="using-cross-fitting-to-predict-propensity-score.html"><a href="using-cross-fitting-to-predict-propensity-score.html"><i class="fa fa-check"></i><b>6.5</b> Using cross-fitting to predict propensity score</a></li>
<li class="chapter" data-level="6.6" data-path="propensity-score-stratification.html"><a href="propensity-score-stratification.html"><i class="fa fa-check"></i><b>6.6</b> Propensity score stratification</a></li>
<li class="chapter" data-level="6.7" data-path="inverse-probability-weighting-ipw.html"><a href="inverse-probability-weighting-ipw.html"><i class="fa fa-check"></i><b>6.7</b> Inverse Probability Weighting (IPW)</a></li>
<li class="chapter" data-level="6.8" data-path="comparing-ipw-with-aggregated-estimate.html"><a href="comparing-ipw-with-aggregated-estimate.html"><i class="fa fa-check"></i><b>6.8</b> Comparing IPW with Aggregated Estimate</a></li>
<li class="chapter" data-level="6.9" data-path="aipw-and-estimation.html"><a href="aipw-and-estimation.html"><i class="fa fa-check"></i><b>6.9</b> AIPW and Estimation</a></li>
<li class="chapter" data-level="6.10" data-path="assessing-balance.html"><a href="assessing-balance.html"><i class="fa fa-check"></i><b>6.10</b> Assessing Balance</a></li>
<li class="chapter" data-level="6.11" data-path="cross-fitting.html"><a href="cross-fitting.html"><i class="fa fa-check"></i><b>6.11</b> Cross-fitting</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="difference-in-differences.html"><a href="difference-in-differences.html"><i class="fa fa-check"></i><b>7</b> Difference in Differences</a>
<ul>
<li class="chapter" data-level="7.1" data-path="a-quick-introduction.html"><a href="a-quick-introduction.html"><i class="fa fa-check"></i><b>7.1</b> A Quick Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="set-up.html"><a href="set-up.html"><i class="fa fa-check"></i><b>7.2</b> Set up</a></li>
<li class="chapter" data-level="7.3" data-path="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><a href="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><i class="fa fa-check"></i><b>7.3</b> An example: Evaluating the impact of Medicaid expansion on uninsured rate</a></li>
<li class="chapter" data-level="7.4" data-path="naive-estimator.html"><a href="naive-estimator.html"><i class="fa fa-check"></i><b>7.4</b> Naive estimator</a></li>
<li class="chapter" data-level="7.5" data-path="canonical-difference-in-differences-framework.html"><a href="canonical-difference-in-differences-framework.html"><i class="fa fa-check"></i><b>7.5</b> Canonical Difference in Differences Framework</a></li>
<li class="chapter" data-level="7.6" data-path="did-in-multi-period-set-up.html"><a href="did-in-multi-period-set-up.html"><i class="fa fa-check"></i><b>7.6</b> DiD in multi-period set up</a></li>
<li class="chapter" data-level="7.7" data-path="conditional-parallel-trend-assumption.html"><a href="conditional-parallel-trend-assumption.html"><i class="fa fa-check"></i><b>7.7</b> Conditional Parallel Trend Assumption</a></li>
<li class="chapter" data-level="7.8" data-path="some-concerns-with-controls.html"><a href="some-concerns-with-controls.html"><i class="fa fa-check"></i><b>7.8</b> Some concerns with controls</a></li>
<li class="chapter" data-level="7.9" data-path="the-2-times-2-difference-in-differences-estimate.html"><a href="the-2-times-2-difference-in-differences-estimate.html"><i class="fa fa-check"></i><b>7.9</b> The <span class="math inline">\(2 \times 2\)</span> Difference-in-Differences Estimate</a></li>
<li class="chapter" data-level="7.10" data-path="event-study-model.html"><a href="event-study-model.html"><i class="fa fa-check"></i><b>7.10</b> Event study model</a></li>
<li class="chapter" data-level="7.11" data-path="two-way-fixed-effect-twfe-revisited.html"><a href="two-way-fixed-effect-twfe-revisited.html"><i class="fa fa-check"></i><b>7.11</b> Two way fixed effect (TWFE) Revisited</a></li>
<li class="chapter" data-level="7.12" data-path="various-ways-of-estimation.html"><a href="various-ways-of-estimation.html"><i class="fa fa-check"></i><b>7.12</b> Various ways of estimation</a></li>
<li class="chapter" data-level="7.13" data-path="multi-period-multi-group-and-variation-in-treatment-timing.html"><a href="multi-period-multi-group-and-variation-in-treatment-timing.html"><i class="fa fa-check"></i><b>7.13</b> Multi Period, Multi Group and Variation in Treatment Timing</a></li>
<li class="chapter" data-level="7.14" data-path="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><a href="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><i class="fa fa-check"></i><b>7.14</b> Problem with TWFE in Multiple Group with Treatment Timing Variation</a></li>
<li class="chapter" data-level="7.15" data-path="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><a href="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><i class="fa fa-check"></i><b>7.15</b> What is TWFE Estimating when there is Treatment Timing Variation?</a></li>
<li class="chapter" data-level="7.16" data-path="assumptions-governing-twfedd-estimate.html"><a href="assumptions-governing-twfedd-estimate.html"><i class="fa fa-check"></i><b>7.16</b> Assumptions governing TWFEDD estimate</a></li>
<li class="chapter" data-level="7.17" data-path="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><a href="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><i class="fa fa-check"></i><b>7.17</b> How Does Treatment Effect Heterogeneity in Time Affect TWFE?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="causal-forest.html"><a href="causal-forest.html"><i class="fa fa-check"></i><b>8</b> Causal Forest</a>
<ul>
<li class="chapter" data-level="8.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="summary-of-grf.html"><a href="summary-of-grf.html"><i class="fa fa-check"></i><b>8.2</b> Summary of GRF</a></li>
<li class="chapter" data-level="8.3" data-path="motivation-for-causal-forests.html"><a href="motivation-for-causal-forests.html"><i class="fa fa-check"></i><b>8.3</b> Motivation for Causal Forests</a></li>
<li class="chapter" data-level="8.4" data-path="causal-forest-1.html"><a href="causal-forest-1.html"><i class="fa fa-check"></i><b>8.4</b> Causal Forest</a></li>
<li class="chapter" data-level="8.5" data-path="an-example-of-causal-forest.html"><a href="an-example-of-causal-forest.html"><i class="fa fa-check"></i><b>8.5</b> An example of causal forest</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>9</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="9.1" data-path="some-ways-to-estimate-cate.html"><a href="some-ways-to-estimate-cate.html"><i class="fa fa-check"></i><b>9.1</b> Some ways to estimate CATE</a></li>
<li class="chapter" data-level="9.2" data-path="estimation-1.html"><a href="estimation-1.html"><i class="fa fa-check"></i><b>9.2</b> Estimation</a></li>
<li class="chapter" data-level="9.3" data-path="some-remarks-and-questions.html"><a href="some-remarks-and-questions.html"><i class="fa fa-check"></i><b>9.3</b> Some Remarks and Questions</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-and-gradient-descent" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Regression and Gradient Descent<a href="regression-and-gradient-descent.html#regression-and-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Course:</strong> Causal Inference</p>
<p><strong>Topic:</strong> Estimation of linear regression model with and without closed-form solutions</p>
<style>
.jp-Notebook,
.jp-NotebookPanel-notebook {
    max-width: 900px;
    margin: auto;
}

.jp-Cell {
    padding-left: 40px;
    padding-right: 40px;
}
</style>
<style>
@media print {
 body {
   margin: 1in;
 }
}
</style>
<p>Last time we took a look at the method of minimizing the sum of the square of residuals.
Today let’s take a look at two other ways of estimating a linear regression specification:
i) Normal equation method, and ii) Gradient descent. We’ll do these manually and compare our
results using python libraries to see whether we’ve done it correctly.</p>
<p>Lets first load the necessary libraries.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="regression-and-gradient-descent.html#cb59-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb59-2"><a href="regression-and-gradient-descent.html#cb59-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb59-3"><a href="regression-and-gradient-descent.html#cb59-3" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_regression</span>
<span id="cb59-4"><a href="regression-and-gradient-descent.html#cb59-4" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> add_dummy_feature</span>
<span id="cb59-5"><a href="regression-and-gradient-descent.html#cb59-5" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb59-6"><a href="regression-and-gradient-descent.html#cb59-6" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb59-7"><a href="regression-and-gradient-descent.html#cb59-7" tabindex="-1"></a>root_dir <span class="op">=</span> <span class="st">&quot;/home/vinish/Dropbox/Machine Learning&quot;</span></span></code></pre></div>
<p>sklearn is an open sourced library in python that is mainly built for predictive data analysis, which is
built on top of NumPy, SciPy, and matplotlib. You can get more information about this package on
<a href="https://scikit-learn.org/stable/index.html">sklearn</a>.</p>
<p>We’ll be using simulated data from sklearn’s module called “datasets” by using the make_regression() function. The true model has the following
attributes:<br />
i) 2 informative features (X)<br />
ii) 1 target (Y)<br />
iii) intercept with the coefficient of 10</p>
<p>Let’s use make_regression() to simulate our data.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="regression-and-gradient-descent.html#cb60-1" tabindex="-1"></a>X, y, coefficient <span class="op">=</span> make_regression(n_samples <span class="op">=</span> <span class="dv">1000</span>,</span>
<span id="cb60-2"><a href="regression-and-gradient-descent.html#cb60-2" tabindex="-1"></a>                       n_features <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb60-3"><a href="regression-and-gradient-descent.html#cb60-3" tabindex="-1"></a>                       n_informative <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb60-4"><a href="regression-and-gradient-descent.html#cb60-4" tabindex="-1"></a>                       n_targets <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb60-5"><a href="regression-and-gradient-descent.html#cb60-5" tabindex="-1"></a>                       noise <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb60-6"><a href="regression-and-gradient-descent.html#cb60-6" tabindex="-1"></a>                       bias <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb60-7"><a href="regression-and-gradient-descent.html#cb60-7" tabindex="-1"></a>                       random_state <span class="op">=</span> <span class="dv">42</span>, </span>
<span id="cb60-8"><a href="regression-and-gradient-descent.html#cb60-8" tabindex="-1"></a>                       coef <span class="op">=</span> <span class="va">True</span></span>
<span id="cb60-9"><a href="regression-and-gradient-descent.html#cb60-9" tabindex="-1"></a></span>
<span id="cb60-10"><a href="regression-and-gradient-descent.html#cb60-10" tabindex="-1"></a>)</span></code></pre></div>
<p>Note that I’ve set coef = True. This will return the model parameters and the intercept is set at 10.</p>
<p>We take a look at the first five rows.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="regression-and-gradient-descent.html#cb61-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;print features: </span><span class="sc">{</span>X[<span class="dv">0</span>:<span class="dv">5</span>, :]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb61-2"><a href="regression-and-gradient-descent.html#cb61-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;target: </span><span class="sc">{</span>y[<span class="dv">0</span>:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>print features: [[-0.16711808  0.14671369]
 [-0.02090159  0.11732738]
 [ 0.15041891  0.364961  ]
 [ 0.55560447  0.08958068]
 [ 0.05820872 -1.1429703 ]]
target: [ 3.24877735  8.66339401 19.45702327 31.55545159  3.92293402]</code></pre>
<p>And let’s print out the model parameters. Note these are the true coefficients that are used to generate data.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="regression-and-gradient-descent.html#cb63-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;coefficients: </span><span class="sc">{</span>coefficient<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>coefficients: [40.71064891  6.60098441]</code></pre>
<p>Plot the relationship between <span class="math inline">\(X1\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="regression-and-gradient-descent.html#cb65-1" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb65-2"><a href="regression-and-gradient-descent.html#cb65-2" tabindex="-1"></a>plt.scatter(X[:,<span class="dv">1</span>], y)</span>
<span id="cb65-3"><a href="regression-and-gradient-descent.html#cb65-3" tabindex="-1"></a>plt.show()</span>
<span id="cb65-4"><a href="regression-and-gradient-descent.html#cb65-4" tabindex="-1"></a><span class="co">#plt.savefig(root_dir + &quot;/Codes/Output/make_data_scatter.pdf&quot;)</span></span></code></pre></div>
<div class="float">
<img src="01_Regression_Gradient_Descent_files/01_Regression_Gradient_Descent_12_0.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Now, we are ready to discuss two methods. Let’s start with the Normal equation method.</p>
<p><strong>i. Normal Equation</strong></p>
<p>Consider the following regression model specification:</p>
<p><span class="math display">\[
Y_i = \alpha + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i
\]</span></p>
<p>The job is to estimate model parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span>.</p>
<p>Note that <span class="math inline">\(\epsilon_i\)</span> is the error term and we’d want to minimize some version of this. Let’s write out the
error as:</p>
<p><span class="math display">\[
\epsilon_i = Y_i - \alpha - \beta_1 X_{1i} -\beta_2 X_{2i}
\]</span></p>
<p>We know that the error term <span class="math inline">\(\epsilon\)</span> is a <span class="math inline">\(n \times 1\)</span> vector. We can obtain residuals by using estimates of model parameters. Of course, we don’t want to pick any parameters – the estimates should follow some objective.</p>
<p>One idea is to estimate the model parameters with an objective of minimizing the mean of the error. However, this is 0 by construction. So what we’d want to do instead is minimize the mean squared error.</p>
<p><span class="math display">\[
MSE(X, h_{\theta}) = \frac{1}{m} \sum_{i=1}^{m} (\theta^{T} x_i - y_i)^2
\]</span></p>
<p>From the equation above, we know that the MSE is just the mean of the sum of the squared errors. We can write the sum of the squared of errors using the matrix version as:</p>
<p><span class="math display">\[
SSE(X, \theta) = (y - X\theta)^T(y-X\theta)
\]</span></p>
<p>Expanding this and setting the derivatives w.r.t. <span class="math inline">\(\theta\)</span> equal to zero gives:
<span class="math display">\[
\begin{aligned}
SSE(X, \theta) = y^Ty - 2\theta^{T} X^Ty + \theta^{T}X^{T}X\theta \\
\frac{\partial(SSE)}{\partial{\theta}} = -2X^{T}y + 2X^{T}X\theta = 0
\end{aligned}
\]</span></p>
<p>Now solving for <span class="math inline">\(\theta\)</span> gives the normal equation:
<span class="math display">\[
\hat{\theta} = (X^TX)^{-1}X^{T}y
\]</span></p>
<p>Let’s code the normal equation and print out the estimates. Before jumping into estimating the
normal equation, we’ve got to be careful and add the intercept term (all ones) on X as the simulated data from make_regression comes without it. We’ll do this using add_dummy_feature() in sklearn.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="regression-and-gradient-descent.html#cb66-1" tabindex="-1"></a><span class="co"># The normal equation</span></span>
<span id="cb66-2"><a href="regression-and-gradient-descent.html#cb66-2" tabindex="-1"></a>X <span class="op">=</span> add_dummy_feature(X)</span>
<span id="cb66-3"><a href="regression-and-gradient-descent.html#cb66-3" tabindex="-1"></a>theta_best <span class="op">=</span> np.linalg.inv((X.T <span class="op">@</span> X)) <span class="op">@</span> (X.T <span class="op">@</span> y)</span>
<span id="cb66-4"><a href="regression-and-gradient-descent.html#cb66-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;coefficients from normal equation: </span><span class="sc">{</span>theta_best<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>coefficients from normal equation: [10.00156877 40.74650082  6.62076534]</code></pre>
<p><strong>ii. Gradient Descent</strong></p>
<p>Imagine that you are standing at the top of a mountain and want to descend the mountain as quickly as possible. One simple way is to consider a few directions – north, south, east, and west – and evaluate the steepness (gradient). Then you’d want to take a small step towards the steepest direction, pause, and re-evaluate the steepness. Doing this repeatedly gets you to the bottom of the mountain as fast as possible.</p>
<p>The idea of gradient descent is similar in context to the aforementioned analogy. We’ve already been exposed to the idea of MSE and the objective of minimizing MSE. Instead of using the closed form normal equation to solve for the minimum of MSE, gradient descent uses <em>gradient</em> of MSE to adjust the estimates and move closer to the minimum.</p>
<p>The gradient is a vector of partial derivatives of MSE that points to the direction of steepest increase increase in MSE. Hence, to minimize the loss, we’d want to move in opposite direction of the gradient. By repeatedly updating our parameter in this way, we move closer and closer to the minimum of the loss function. To simply the concept, we’ll start with the univariate case without the intercept.</p>
<p><span class="math display">\[
\begin{aligned}
Y_i = \beta X_i + \epsilon_i
\end{aligned}
\]</span></p>
<p>The MSE and the derivative is:</p>
<p><span class="math display">\[
\begin{aligned}
MSE(\beta) = \frac{1}{m} (Y - \beta X)^{T}(Y - \beta X)  \\
\frac{\partial{MSE}}{\partial{\beta}} = -\frac{2}{m} X^{T}(Y - \beta X)
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(\frac{2}{m}X^{T}(Y - \beta X)\)</span> is the gradient of the univariate specification, which informs the direction of the steepest increase in MSE. To reduce MSE, we therefore move in the opposite direction of the gradient.</p>
<p>Now that we have the gradient, the gradient descent algorithm can be set up as follows:
1. Start with an initial guesses of the parameter <span class="math inline">\((\beta_o)\)</span>.
2. Update estimates of parameters by moving to the opposite direction of the gradient.
<span class="math display">\[     
      \beta_{new} = \beta_o - \eta \times gradient_o.
      \]</span>
where, <span class="math inline">\(\eta\)</span> is the <em>learning rate</em>.
3. Re-evaluate the gradient using <span class="math inline">\(\beta_{new}\)</span>.
4. Repeat steps 2 and 3 for a given number of times or until convergence is reached.</p>
<p>Let’s simulate data for univariate model specification to visually see what this looks like.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="regression-and-gradient-descent.html#cb68-1" tabindex="-1"></a>dat_uni <span class="op">=</span> make_regression(</span>
<span id="cb68-2"><a href="regression-and-gradient-descent.html#cb68-2" tabindex="-1"></a>                n_samples<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb68-3"><a href="regression-and-gradient-descent.html#cb68-3" tabindex="-1"></a>                n_features<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb68-4"><a href="regression-and-gradient-descent.html#cb68-4" tabindex="-1"></a>                n_informative<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb68-5"><a href="regression-and-gradient-descent.html#cb68-5" tabindex="-1"></a>                n_targets<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb68-6"><a href="regression-and-gradient-descent.html#cb68-6" tabindex="-1"></a>                bias<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb68-7"><a href="regression-and-gradient-descent.html#cb68-7" tabindex="-1"></a>                noise<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb68-8"><a href="regression-and-gradient-descent.html#cb68-8" tabindex="-1"></a>                random_state<span class="op">=</span><span class="dv">42</span>, </span>
<span id="cb68-9"><a href="regression-and-gradient-descent.html#cb68-9" tabindex="-1"></a>                coef<span class="op">=</span><span class="va">True</span></span>
<span id="cb68-10"><a href="regression-and-gradient-descent.html#cb68-10" tabindex="-1"></a>)</span>
<span id="cb68-11"><a href="regression-and-gradient-descent.html#cb68-11" tabindex="-1"></a></span>
<span id="cb68-12"><a href="regression-and-gradient-descent.html#cb68-12" tabindex="-1"></a>X_uni, y_uni, coef <span class="op">=</span> dat_uni</span>
<span id="cb68-13"><a href="regression-and-gradient-descent.html#cb68-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The first five rows of X: </span><span class="sc">{</span>X_uni[<span class="dv">0</span>:<span class="dv">5</span>,:]<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb68-14"><a href="regression-and-gradient-descent.html#cb68-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The first five y values: </span><span class="sc">{</span>y_uni[<span class="dv">0</span>:<span class="dv">5</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb68-15"><a href="regression-and-gradient-descent.html#cb68-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The coefficient of univariate model is: </span><span class="sc">{</span>coef<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb68-16"><a href="regression-and-gradient-descent.html#cb68-16" tabindex="-1"></a></span>
<span id="cb68-17"><a href="regression-and-gradient-descent.html#cb68-17" tabindex="-1"></a><span class="co"># set up gradient descent </span></span>
<span id="cb68-18"><a href="regression-and-gradient-descent.html#cb68-18" tabindex="-1"></a>beta <span class="op">=</span> <span class="op">-</span><span class="dv">5</span> <span class="co"># initialize beta</span></span>
<span id="cb68-19"><a href="regression-and-gradient-descent.html#cb68-19" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># learning rate</span></span>
<span id="cb68-20"><a href="regression-and-gradient-descent.html#cb68-20" tabindex="-1"></a>m <span class="op">=</span> X_uni.shape[<span class="dv">0</span>] <span class="co"># number of observations</span></span>
<span id="cb68-21"><a href="regression-and-gradient-descent.html#cb68-21" tabindex="-1"></a>iter_val <span class="op">=</span> <span class="dv">100</span> <span class="co"># number of iteration steps</span></span>
<span id="cb68-22"><a href="regression-and-gradient-descent.html#cb68-22" tabindex="-1"></a>y_uni <span class="op">=</span> y_uni.reshape((m,<span class="dv">1</span>)) <span class="co"># reshape into m*1 vector</span></span>
<span id="cb68-23"><a href="regression-and-gradient-descent.html#cb68-23" tabindex="-1"></a>beta_store <span class="op">=</span> np.ones(m)</span>
<span id="cb68-24"><a href="regression-and-gradient-descent.html#cb68-24" tabindex="-1"></a>loss_store <span class="op">=</span> np.ones(m)</span>
<span id="cb68-25"><a href="regression-and-gradient-descent.html#cb68-25" tabindex="-1"></a></span>
<span id="cb68-26"><a href="regression-and-gradient-descent.html#cb68-26" tabindex="-1"></a><span class="co"># loop </span></span>
<span id="cb68-27"><a href="regression-and-gradient-descent.html#cb68-27" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iter_val):</span>
<span id="cb68-28"><a href="regression-and-gradient-descent.html#cb68-28" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X_uni.T <span class="op">@</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni)</span>
<span id="cb68-29"><a href="regression-and-gradient-descent.html#cb68-29" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni).T <span class="op">@</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni)</span>
<span id="cb68-30"><a href="regression-and-gradient-descent.html#cb68-30" tabindex="-1"></a>    beta <span class="op">=</span> beta <span class="op">-</span> eta<span class="op">*</span>gradient</span>
<span id="cb68-31"><a href="regression-and-gradient-descent.html#cb68-31" tabindex="-1"></a>    beta_store[i] <span class="op">=</span> beta.item() <span class="co"># use .item to extract scalar</span></span>
<span id="cb68-32"><a href="regression-and-gradient-descent.html#cb68-32" tabindex="-1"></a>    loss_store[i] <span class="op">=</span> loss.item() <span class="co"># use .item to extract scalar</span></span>
<span id="cb68-33"><a href="regression-and-gradient-descent.html#cb68-33" tabindex="-1"></a></span>
<span id="cb68-34"><a href="regression-and-gradient-descent.html#cb68-34" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;gradient descent at work: </span><span class="sc">{</span>beta_store<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb68-35"><a href="regression-and-gradient-descent.html#cb68-35" tabindex="-1"></a></span>
<span id="cb68-36"><a href="regression-and-gradient-descent.html#cb68-36" tabindex="-1"></a><span class="co"># figure </span></span>
<span id="cb68-37"><a href="regression-and-gradient-descent.html#cb68-37" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb68-38"><a href="regression-and-gradient-descent.html#cb68-38" tabindex="-1"></a>plt.scatter(beta_store, loss_store)</span>
<span id="cb68-39"><a href="regression-and-gradient-descent.html#cb68-39" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;beta value&quot;</span>)</span>
<span id="cb68-40"><a href="regression-and-gradient-descent.html#cb68-40" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;loss&quot;</span>)</span>
<span id="cb68-41"><a href="regression-and-gradient-descent.html#cb68-41" tabindex="-1"></a>plt.title(<span class="st">&quot;Gradient Descent at Work&quot;</span>)</span>
<span id="cb68-42"><a href="regression-and-gradient-descent.html#cb68-42" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>The first five rows of X: [[ 0.19686124]
 [ 0.35711257]
 [-1.91328024]
 [-0.03582604]
 [ 0.76743473]] 
 

The first five y values: [  8.21720459  14.90627167 -79.86242262  -1.49541829  32.03357001] 
 

The coefficient of univariate model is: 41.7411003148779 
 

gradient descent at work: [ 2.73384129  9.18803147 14.57430322 19.06935566 22.82065107 25.95125242
 28.56386053 30.74418321 32.56374696 34.0822434  35.3494875  36.4070518
 37.28963019 38.02617604 38.64085208 39.15382306 39.58191721 39.93917836
 40.23732664 40.48614292 40.69378975 40.86707908 41.01169573 41.13238393
 41.23310291 41.3171568  41.38730303 41.44584277 41.49469646 41.53546675
 41.56949114 41.59788581 41.62158226 41.64135787 41.65786138 41.6716342
 41.68312815 41.6927203  41.70072532 41.70740581 41.71298095 41.71763361
 41.72151644 41.72475682 41.72746103 41.7297178  41.73160117 41.73317291
 41.73448459 41.73557923 41.73649276 41.73725513 41.73789136 41.73842232
 41.73886542 41.73923521 41.73954381 41.73980135 41.74001628 41.74019565
 41.74034533 41.74047025 41.74057451 41.74066151 41.74073411 41.7407947
 41.74084527 41.74088747 41.74092269 41.74095208 41.74097661 41.74099708
 41.74101416 41.74102841 41.74104031 41.74105024 41.74105852 41.74106544
 41.74107121 41.74107603 41.74108004 41.7410834  41.7410862  41.74108853
 41.74109048 41.74109211 41.74109347 41.7410946  41.74109555 41.74109633
 41.74109699 41.74109754 41.741098   41.74109838 41.7410987  41.74109897
 41.74109919 41.74109938 41.74109953 41.74109966] 
 </code></pre>
<div class="float">
<img src="01_Regression_Gradient_Descent_files/01_Regression_Gradient_Descent_16_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Here, we see that the algorithm converges at the estimated <span class="math inline">\(\beta\)</span> little over 40. Let’s print out the best beta from gradient descent and the true parameter for comparison.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="regression-and-gradient-descent.html#cb70-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;True parameter of the univariate model: </span><span class="sc">{</span>coef<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> &quot;</span>)</span>
<span id="cb70-2"><a href="regression-and-gradient-descent.html#cb70-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Best estimate from gradient descent: </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>) </span></code></pre></div>
<pre><code>True parameter of the univariate model: 41.7411003148779 
 
 
Best estimate from gradient descent: [[41.74109966]] 
 </code></pre>
<p>See that the estimate obtained from gradient descent is close to the true parameter.</p>
<p><strong>Multivariate model</strong></p>
<p>The gradient descent works similarly in case of multivariate model specification except that we’ll have a vector of partial derivatives. The multivariate model specified at the very begining is:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i = \alpha + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i \\
MSE(\theta) = \frac{1}{m} (Y - X \theta)^{T}(Y - X \theta)
\end{aligned}
\]</span></p>
<p>I’ve expressed MSE in matrix form, where <span class="math inline">\(\theta\)</span> incorportates the vector of parameters: <span class="math inline">\(\theta = [\alpha, \; \beta_1, \; \beta_2]^{T}\)</span>.</p>
<p>The gradient vector is given as:
<span class="math display">\[
\frac{\partial MSE}{\partial \theta} = \frac{1}{m} 2 X^{T}(Y - X \theta)
\]</span></p>
<p>The gradient vector is of dimension <span class="math inline">\(3 \times 1\)</span> and stacks all partials of MSE with respect to <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>.</p>
<p>Let’s code the gradient descent algorithm and print out both the true parameters and their estimates.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="regression-and-gradient-descent.html#cb72-1" tabindex="-1"></a><span class="co"># Use the gradient descent algorithm</span></span>
<span id="cb72-2"><a href="regression-and-gradient-descent.html#cb72-2" tabindex="-1"></a>m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb72-3"><a href="regression-and-gradient-descent.html#cb72-3" tabindex="-1"></a>y <span class="op">=</span> y.reshape((m, <span class="dv">1</span>))</span>
<span id="cb72-4"><a href="regression-and-gradient-descent.html#cb72-4" tabindex="-1"></a>theta <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb72-5"><a href="regression-and-gradient-descent.html#cb72-5" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb72-6"><a href="regression-and-gradient-descent.html#cb72-6" tabindex="-1"></a></span>
<span id="cb72-7"><a href="regression-and-gradient-descent.html#cb72-7" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb72-8"><a href="regression-and-gradient-descent.html#cb72-8" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="op">/</span> m <span class="op">*</span> X.T <span class="op">@</span> (y <span class="op">-</span> X <span class="op">@</span> theta)</span>
<span id="cb72-9"><a href="regression-and-gradient-descent.html#cb72-9" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> gradient </span>
<span id="cb72-10"><a href="regression-and-gradient-descent.html#cb72-10" tabindex="-1"></a></span>
<span id="cb72-11"><a href="regression-and-gradient-descent.html#cb72-11" tabindex="-1"></a><span class="co"># True coefficients </span></span>
<span id="cb72-12"><a href="regression-and-gradient-descent.html#cb72-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;True coefficients: </span><span class="sc">{</span>np<span class="sc">.</span>hstack([<span class="dv">10</span>, coefficient])<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb72-13"><a href="regression-and-gradient-descent.html#cb72-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Coefficients from gradient descent: </span><span class="sc">{</span>theta<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>True coefficients: [10.         40.71064891  6.60098441]
Coefficients from gradient descent: [[10.00156879]
 [40.74650076]
 [ 6.62076533]]</code></pre>
<p>Note that these coefficients are exactly similar to those obtained from the normal equation method. We can also make sure that we’ve done the estimation correctly by comparing the estimates with those obtained from built in module in sklearn used for purposes of estimating linear regression models.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="regression-and-gradient-descent.html#cb74-1" tabindex="-1"></a><span class="co"># sklearn</span></span>
<span id="cb74-2"><a href="regression-and-gradient-descent.html#cb74-2" tabindex="-1"></a>reg <span class="op">=</span> linear_model.LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb74-3"><a href="regression-and-gradient-descent.html#cb74-3" tabindex="-1"></a>reg.fit(X, y)</span>
<span id="cb74-4"><a href="regression-and-gradient-descent.html#cb74-4" tabindex="-1"></a>best_theta_coef <span class="op">=</span> reg.coef_</span>
<span id="cb74-5"><a href="regression-and-gradient-descent.html#cb74-5" tabindex="-1"></a>best_int_coef <span class="op">=</span> reg.intercept_</span>
<span id="cb74-6"><a href="regression-and-gradient-descent.html#cb74-6" tabindex="-1"></a>best_theta_coef <span class="op">=</span> np.concatenate([best_int_coef, best_theta_coef[:, <span class="dv">1</span>:<span class="dv">3</span>].ravel()], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb74-7"><a href="regression-and-gradient-descent.html#cb74-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;coefficients from sklearn: </span><span class="sc">{</span>best_theta_coef<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>coefficients from sklearn: [10.00156877 40.74650082  6.62076534]</code></pre>
<p>Not too bad!!</p>
<p><strong>Standard Error</strong></p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="regression-and-gradient-descent.html#cb76-1" tabindex="-1"></a><span class="co"># ----------------------------------------------</span></span>
<span id="cb76-2"><a href="regression-and-gradient-descent.html#cb76-2" tabindex="-1"></a></span>
<span id="cb76-3"><a href="regression-and-gradient-descent.html#cb76-3" tabindex="-1"></a></span>
<span id="cb76-4"><a href="regression-and-gradient-descent.html#cb76-4" tabindex="-1"></a><span class="co"># Standard errors </span></span>
<span id="cb76-5"><a href="regression-and-gradient-descent.html#cb76-5" tabindex="-1"></a></span>
<span id="cb76-6"><a href="regression-and-gradient-descent.html#cb76-6" tabindex="-1"></a></span>
<span id="cb76-7"><a href="regression-and-gradient-descent.html#cb76-7" tabindex="-1"></a><span class="co"># ----------------------------------------------</span></span>
<span id="cb76-8"><a href="regression-and-gradient-descent.html#cb76-8" tabindex="-1"></a></span>
<span id="cb76-9"><a href="regression-and-gradient-descent.html#cb76-9" tabindex="-1"></a><span class="co"># 1. Get the standard error of the regression</span></span>
<span id="cb76-10"><a href="regression-and-gradient-descent.html#cb76-10" tabindex="-1"></a></span>
<span id="cb76-11"><a href="regression-and-gradient-descent.html#cb76-11" tabindex="-1"></a>error <span class="op">=</span> (X <span class="op">@</span> theta <span class="op">-</span> y)</span>
<span id="cb76-12"><a href="regression-and-gradient-descent.html#cb76-12" tabindex="-1"></a>error_sq <span class="op">=</span> error.T <span class="op">@</span> error</span>
<span id="cb76-13"><a href="regression-and-gradient-descent.html#cb76-13" tabindex="-1"></a>sigma_sq <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (m <span class="op">-</span><span class="dv">3</span>) <span class="op">*</span> error_sq</span>
<span id="cb76-14"><a href="regression-and-gradient-descent.html#cb76-14" tabindex="-1"></a>se_reg <span class="op">=</span> np.sqrt(sigma_sq)</span>
<span id="cb76-15"><a href="regression-and-gradient-descent.html#cb76-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard error of the regression is: </span><span class="sc">{</span>se_reg<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb76-16"><a href="regression-and-gradient-descent.html#cb76-16" tabindex="-1"></a></span>
<span id="cb76-17"><a href="regression-and-gradient-descent.html#cb76-17" tabindex="-1"></a><span class="co"># 2. get standard errors of the respective coefficients </span></span>
<span id="cb76-18"><a href="regression-and-gradient-descent.html#cb76-18" tabindex="-1"></a>var_cov <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">*</span> sigma_sq</span>
<span id="cb76-19"><a href="regression-and-gradient-descent.html#cb76-19" tabindex="-1"></a>manual_se <span class="op">=</span> np.sqrt(np.diag(var_cov))</span>
<span id="cb76-20"><a href="regression-and-gradient-descent.html#cb76-20" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard errors of coefficients (manual estimation): </span><span class="sc">{</span>manual_se<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb76-21"><a href="regression-and-gradient-descent.html#cb76-21" tabindex="-1"></a></span>
<span id="cb76-22"><a href="regression-and-gradient-descent.html#cb76-22" tabindex="-1"></a><span class="co"># se from stats model</span></span>
<span id="cb76-23"><a href="regression-and-gradient-descent.html#cb76-23" tabindex="-1"></a>X_sm <span class="op">=</span> sm.add_constant(X)   <span class="co"># add intercept</span></span>
<span id="cb76-24"><a href="regression-and-gradient-descent.html#cb76-24" tabindex="-1"></a>model <span class="op">=</span> sm.OLS(y, X_sm).fit()</span>
<span id="cb76-25"><a href="regression-and-gradient-descent.html#cb76-25" tabindex="-1"></a></span>
<span id="cb76-26"><a href="regression-and-gradient-descent.html#cb76-26" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;coefficients from statmodels: </span><span class="sc">{</span>model<span class="sc">.</span>params<span class="sc">}</span><span class="ss">&quot;</span>)   <span class="co"># coefficients</span></span>
<span id="cb76-27"><a href="regression-and-gradient-descent.html#cb76-27" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard errors from statmodels: </span><span class="sc">{</span>model<span class="sc">.</span>bse<span class="sc">}</span><span class="ss">&quot;</span>)      <span class="co"># standard errors</span></span></code></pre></div>
<pre><code>standard error of the regression is: [[0.98562573]]
standard errors of coefficients (manual estimation): [0.03123574 0.03242909 0.03072431]
coefficients from statmodels: [10.00156877 40.74650082  6.62076534]
standard errors from statmodels: [0.03123574 0.03242909 0.03072431]</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"></code></pre></div>

<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"></code></pre></div>
<p><strong>Course:</strong> Causal Inference</p>
<p><strong>Topic:</strong> Standard Errors</p>
<style>
.jp-Notebook,
.jp-NotebookPanel-notebook {
    max-width: 900px;
    margin: auto;
}

.jp-Cell {
    padding-left: 40px;
    padding-right: 40px;
}
</style>
<style>
@media print {
 body {
   margin: 1in;
 }
}
</style>
<p><strong>Standard Error</strong></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="regression-and-gradient-descent.html#cb80-1" tabindex="-1"></a><span class="co"># ----------------------------------------------</span></span>
<span id="cb80-2"><a href="regression-and-gradient-descent.html#cb80-2" tabindex="-1"></a></span>
<span id="cb80-3"><a href="regression-and-gradient-descent.html#cb80-3" tabindex="-1"></a></span>
<span id="cb80-4"><a href="regression-and-gradient-descent.html#cb80-4" tabindex="-1"></a><span class="co"># Standard errors </span></span>
<span id="cb80-5"><a href="regression-and-gradient-descent.html#cb80-5" tabindex="-1"></a></span>
<span id="cb80-6"><a href="regression-and-gradient-descent.html#cb80-6" tabindex="-1"></a></span>
<span id="cb80-7"><a href="regression-and-gradient-descent.html#cb80-7" tabindex="-1"></a><span class="co"># ----------------------------------------------</span></span></code></pre></div>
<p><strong>Standard Errors</strong></p>
<p>As previously mentioned, we start with a specification oriented towards the population and use a sample to estimate
the population parameters. The <span class="math inline">\(\hat{\beta}\)</span> are the sample estimates that inform us about the population parameters <span class="math inline">\(\beta\)</span>.
In this sense, sampling variability – if you were to take say 1,000 samples from the population and re-estimate the parameter –
will give you different estimates of <span class="math inline">\(\beta\)</span>. Just as the sample mean is a random variable with its own distribution,
so is <span class="math inline">\(\hat{\beta}\)</span>. The standard error of <span class="math inline">\(\hat{\beta}\)</span> plays the exact same role as the standard error of the mean,
which we motivate below.</p>
<p>Consider the example of mean height. Say the population distribution is normal with mean 175 cm and standard deviation
of 7.6 cm. You first take a sample of 1,000 individuals and estimate the mean height; then re-take the next sample,
re-estimate the mean, and so on for 1,000 different samples. This will give you a distribution of mean height estimates,
which will itself be normal with a variance driven entirely by sampling variability.</p>
<p>It is important to note that in practice you only ever have <strong>one sample</strong> – the simulation below is a thought experiment
to build intuition for what would happen under repeated sampling. This is the frequentist conception of uncertainty.</p>
<p>By the <strong>Central Limit Theorem (CLT)</strong>, the sampling distribution of the mean is:
<span class="math display">\[\hat{X} \sim \mathcal{N}\left(\mu,\ \frac{\sigma}{\sqrt{n}}\right)\]</span>
where <span class="math inline">\(\sigma\)</span> is the population standard deviation (fixed but unknown in practice) and <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span> is the
<strong>standard error</strong> – the standard deviation of the sampling distribution of the mean, not of the population itself.
Note the distinction: <span class="math inline">\(\sigma\)</span> describes variability in individual heights; <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span> describes variability
in the <em>estimate of the mean</em> across repeated samples.</p>
<p>Let’s simulate height coming from a normal distribution with mean 175 cm and standard deviation 7.6 cm.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="regression-and-gradient-descent.html#cb81-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb81-2"><a href="regression-and-gradient-descent.html#cb81-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb81-3"><a href="regression-and-gradient-descent.html#cb81-3" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> add_dummy_feature</span>
<span id="cb81-4"><a href="regression-and-gradient-descent.html#cb81-4" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb81-5"><a href="regression-and-gradient-descent.html#cb81-5" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb81-6"><a href="regression-and-gradient-descent.html#cb81-6" tabindex="-1"></a></span>
<span id="cb81-7"><a href="regression-and-gradient-descent.html#cb81-7" tabindex="-1"></a>mean_store <span class="op">=</span> []</span>
<span id="cb81-8"><a href="regression-and-gradient-descent.html#cb81-8" tabindex="-1"></a><span class="bu">iter</span> <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb81-9"><a href="regression-and-gradient-descent.html#cb81-9" tabindex="-1"></a>mean_height <span class="op">=</span> <span class="dv">175</span></span>
<span id="cb81-10"><a href="regression-and-gradient-descent.html#cb81-10" tabindex="-1"></a>std_height <span class="op">=</span> <span class="fl">7.6</span></span>
<span id="cb81-11"><a href="regression-and-gradient-descent.html#cb81-11" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb81-12"><a href="regression-and-gradient-descent.html#cb81-12" tabindex="-1"></a></span>
<span id="cb81-13"><a href="regression-and-gradient-descent.html#cb81-13" tabindex="-1"></a></span>
<span id="cb81-14"><a href="regression-and-gradient-descent.html#cb81-14" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">iter</span>):</span>
<span id="cb81-15"><a href="regression-and-gradient-descent.html#cb81-15" tabindex="-1"></a>    height <span class="op">=</span> np.random.normal(mean_height, std_height, n) <span class="co"># sample height from a normal distribution</span></span>
<span id="cb81-16"><a href="regression-and-gradient-descent.html#cb81-16" tabindex="-1"></a>    mean_store.append(height.mean())</span>
<span id="cb81-17"><a href="regression-and-gradient-descent.html#cb81-17" tabindex="-1"></a></span>
<span id="cb81-18"><a href="regression-and-gradient-descent.html#cb81-18" tabindex="-1"></a><span class="co"># plot the histogram of mean height</span></span>
<span id="cb81-19"><a href="regression-and-gradient-descent.html#cb81-19" tabindex="-1"></a>mean_store <span class="op">=</span> np.array(mean_store).ravel()    </span>
<span id="cb81-20"><a href="regression-and-gradient-descent.html#cb81-20" tabindex="-1"></a></span>
<span id="cb81-21"><a href="regression-and-gradient-descent.html#cb81-21" tabindex="-1"></a><span class="co"># overlay theoretical normal dist</span></span>
<span id="cb81-22"><a href="regression-and-gradient-descent.html#cb81-22" tabindex="-1"></a>x_space <span class="op">=</span> np.linspace(mean_store.<span class="bu">min</span>(), mean_store.<span class="bu">max</span>(), <span class="dv">1000</span>)</span>
<span id="cb81-23"><a href="regression-and-gradient-descent.html#cb81-23" tabindex="-1"></a>theo_nd <span class="op">=</span> stats.norm.pdf(x_space, mean_height, std_height<span class="op">/</span>np.sqrt(n))</span>
<span id="cb81-24"><a href="regression-and-gradient-descent.html#cb81-24" tabindex="-1"></a></span>
<span id="cb81-25"><a href="regression-and-gradient-descent.html#cb81-25" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb81-26"><a href="regression-and-gradient-descent.html#cb81-26" tabindex="-1"></a>plt.hist(mean_store, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&#39;steelblue&#39;</span>, edgecolor<span class="op">=</span><span class="st">&#39;black&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, density<span class="op">=</span><span class="st">&#39;True&#39;</span>)</span>
<span id="cb81-27"><a href="regression-and-gradient-descent.html#cb81-27" tabindex="-1"></a>plt.plot(x_space, theo_nd, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>, linewidth <span class="op">=</span> <span class="dv">2</span>, label <span class="op">=</span> <span class="st">&#39;Theoretical Dist.&#39;</span>)</span>
<span id="cb81-28"><a href="regression-and-gradient-descent.html#cb81-28" tabindex="-1"></a>plt.title(<span class="ss">f&#39;Distribution of mean height from </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss"> different samples&#39;</span>)</span>
<span id="cb81-29"><a href="regression-and-gradient-descent.html#cb81-29" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;mean height&#39;</span>)</span>
<span id="cb81-30"><a href="regression-and-gradient-descent.html#cb81-30" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;density&#39;</span>)</span>
<span id="cb81-31"><a href="regression-and-gradient-descent.html#cb81-31" tabindex="-1"></a>plt.legend()</span>
<span id="cb81-32"><a href="regression-and-gradient-descent.html#cb81-32" tabindex="-1"></a>plt.grid(<span class="va">False</span>)</span>
<span id="cb81-33"><a href="regression-and-gradient-descent.html#cb81-33" tabindex="-1"></a></span>
<span id="cb81-34"><a href="regression-and-gradient-descent.html#cb81-34" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;average of mean height is </span><span class="sc">{</span>mean_store<span class="sc">.</span>mean()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> and std is </span><span class="sc">{</span>mean_store<span class="sc">.</span>std()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb81-35"><a href="regression-and-gradient-descent.html#cb81-35" tabindex="-1"></a></span>
<span id="cb81-36"><a href="regression-and-gradient-descent.html#cb81-36" tabindex="-1"></a><span class="co"># %%[markdown]</span></span>
<span id="cb81-37"><a href="regression-and-gradient-descent.html#cb81-37" tabindex="-1"></a><span class="co"># It is clear that the standard deviation of the mean height depends on: i) the standard deviation of the population height; and ii)</span></span>
<span id="cb81-38"><a href="regression-and-gradient-descent.html#cb81-38" tabindex="-1"></a><span class="co"># n -- the number of observations. The standard deviation of the mean height will be lower if the population standard deviation is lower </span></span>
<span id="cb81-39"><a href="regression-and-gradient-descent.html#cb81-39" tabindex="-1"></a><span class="co"># (meaning that height is relatively more homogeneous). Next, one can lower it by increasing the sample size. </span></span>
<span id="cb81-40"><a href="regression-and-gradient-descent.html#cb81-40" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb81-41"><a href="regression-and-gradient-descent.html#cb81-41" tabindex="-1"></a><span class="co"># Just as the sample mean has a measure for deviation due to sampling variability (standard deviation), $\hat{\beta}}$ too has a measure that we know as standard errors. Simply put, the standard errors measure how fluctuating the estimates of $\beta$ can be given different samples. </span></span>
<span id="cb81-42"><a href="regression-and-gradient-descent.html#cb81-42" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb81-43"><a href="regression-and-gradient-descent.html#cb81-43" tabindex="-1"></a><span class="co"># Right off the start, it should be mentioned that reported standard errors are mostly incorrect. This could be due to several unknown reasons including the functional form of the specified model. But rather than dwelling on why the reported standard errors are incorrect, I want to discuss some known ways to fix the standard errors.</span></span>
<span id="cb81-44"><a href="regression-and-gradient-descent.html#cb81-44" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb81-45"><a href="regression-and-gradient-descent.html#cb81-45" tabindex="-1"></a><span class="co"># First, (recall) we start with the assumption on the error term. We discussed the i.i.d. assumtion of the error term. Again, the i.i.d. assumption states that error term are *independent* and *identically* distributed. The former means that errors are not correlated, whereas the latter means that errors are extracted from the same distribution with the same mean and variance. </span></span>
<span id="cb81-46"><a href="regression-and-gradient-descent.html#cb81-46" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb81-47"><a href="regression-and-gradient-descent.html#cb81-47" tabindex="-1"></a><span class="co"># Under the i.i.d. assumption, errors are homoskedastic. Estimation of standard errors take the following steps.</span></span>
<span id="cb81-48"><a href="regression-and-gradient-descent.html#cb81-48" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb81-49"><a href="regression-and-gradient-descent.html#cb81-49" tabindex="-1"></a><span class="co"># 1. First estimate the regression standard error as: $\sigma_{reg}^2 = \frac{1}{n}\epsilon^{T} \epsilon$.</span></span>
<span id="cb81-50"><a href="regression-and-gradient-descent.html#cb81-50" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb81-51"><a href="regression-and-gradient-descent.html#cb81-51" tabindex="-1"></a><span class="co"># 2. The standard error of estimates then is: $\sqrt{Var(\hat{\beta})} = (X^{T}X)^{-1} \times \sigma_{reg}$, where $X^{T}X$ is the variance-covariance matrix with variance terms in the diagonal. </span></span>
<span id="cb81-52"><a href="regression-and-gradient-descent.html#cb81-52" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb81-53"><a href="regression-and-gradient-descent.html#cb81-53" tabindex="-1"></a><span class="co"># However, both of these assumptions (identical and independently distributed) will mostly likely fail in practice. This means that we&#39;d need to adjust the standard errors appropriately.      </span></span>
<span id="cb81-54"><a href="regression-and-gradient-descent.html#cb81-54" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb81-55"><a href="regression-and-gradient-descent.html#cb81-55" tabindex="-1"></a><span class="co"># Let&#39;s take a classic example between education and earnings. We&#39;ll also consider &#39;ability&#39; as a control variable in the DGP. Of course, this is only a simulation exercise to clarify the context of standard errors.</span></span>
<span id="cb81-56"><a href="regression-and-gradient-descent.html#cb81-56" tabindex="-1"></a><span class="co"># </span></span></code></pre></div>
<pre><code>average of mean height is 175.0002 and std is 0.2389</code></pre>
<div class="float">
<img src="standard_errors_files/standard_errors_5_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="regression-and-gradient-descent.html#cb83-1" tabindex="-1"></a></span>
<span id="cb83-2"><a href="regression-and-gradient-descent.html#cb83-2" tabindex="-1"></a><span class="co"># 1. generate ability score with a mean of 50 and sd of 20.</span></span>
<span id="cb83-3"><a href="regression-and-gradient-descent.html#cb83-3" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb83-4"><a href="regression-and-gradient-descent.html#cb83-4" tabindex="-1"></a>ability <span class="op">=</span> np.random.normal(<span class="dv">50</span>, <span class="dv">20</span>, m)</span>
<span id="cb83-5"><a href="regression-and-gradient-descent.html#cb83-5" tabindex="-1"></a>ability_scaled <span class="op">=</span> (ability <span class="op">-</span> ability.mean()) <span class="op">/</span> ability.std() <span class="co"># ability scaled</span></span>
<span id="cb83-6"><a href="regression-and-gradient-descent.html#cb83-6" tabindex="-1"></a></span>
<span id="cb83-7"><a href="regression-and-gradient-descent.html#cb83-7" tabindex="-1"></a><span class="co"># plot histogram of ability score</span></span>
<span id="cb83-8"><a href="regression-and-gradient-descent.html#cb83-8" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb83-9"><a href="regression-and-gradient-descent.html#cb83-9" tabindex="-1"></a>plt.hist(ability, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&#39;steelblue&#39;</span>, edgecolor<span class="op">=</span><span class="st">&#39;black&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb83-10"><a href="regression-and-gradient-descent.html#cb83-10" tabindex="-1"></a>plt.title(<span class="st">&#39;Distribution of ability&#39;</span>)</span>
<span id="cb83-11"><a href="regression-and-gradient-descent.html#cb83-11" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;ability score&#39;</span>)</span>
<span id="cb83-12"><a href="regression-and-gradient-descent.html#cb83-12" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;frequency&#39;</span>)</span>
<span id="cb83-13"><a href="regression-and-gradient-descent.html#cb83-13" tabindex="-1"></a></span>
<span id="cb83-14"><a href="regression-and-gradient-descent.html#cb83-14" tabindex="-1"></a><span class="co"># 2. education is positively dependent on ability</span></span>
<span id="cb83-15"><a href="regression-and-gradient-descent.html#cb83-15" tabindex="-1"></a>educa <span class="op">=</span> <span class="dv">7</span> <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> ability <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">4</span>, m)</span>
<span id="cb83-16"><a href="regression-and-gradient-descent.html#cb83-16" tabindex="-1"></a>educa[educa<span class="op">&lt;</span><span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb83-17"><a href="regression-and-gradient-descent.html#cb83-17" tabindex="-1"></a>educa_scaled <span class="op">=</span> (educa <span class="op">-</span> educa.mean()) <span class="op">/</span> educa.std() <span class="co"># education scaled</span></span>
<span id="cb83-18"><a href="regression-and-gradient-descent.html#cb83-18" tabindex="-1"></a><span class="co">#plt.hist(educa)</span></span>
<span id="cb83-19"><a href="regression-and-gradient-descent.html#cb83-19" tabindex="-1"></a></span>
<span id="cb83-20"><a href="regression-and-gradient-descent.html#cb83-20" tabindex="-1"></a><span class="co"># 3. generate income</span></span>
<span id="cb83-21"><a href="regression-and-gradient-descent.html#cb83-21" tabindex="-1"></a><span class="co"># Income comes from distribution with varying standard deviation</span></span>
<span id="cb83-22"><a href="regression-and-gradient-descent.html#cb83-22" tabindex="-1"></a><span class="co"># this marks heteroskedasticity</span></span>
<span id="cb83-23"><a href="regression-and-gradient-descent.html#cb83-23" tabindex="-1"></a>error_term <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">500</span> <span class="op">+</span> (<span class="dv">200</span><span class="op">*</span>educa))</span>
<span id="cb83-24"><a href="regression-and-gradient-descent.html#cb83-24" tabindex="-1"></a></span>
<span id="cb83-25"><a href="regression-and-gradient-descent.html#cb83-25" tabindex="-1"></a><span class="co"># generate income using the following DGP </span></span>
<span id="cb83-26"><a href="regression-and-gradient-descent.html#cb83-26" tabindex="-1"></a>income <span class="op">=</span> <span class="dv">40000</span> <span class="op">+</span> <span class="dv">10000</span> <span class="op">*</span> educa_scaled <span class="op">+</span> <span class="dv">700</span> <span class="op">*</span> ability_scaled <span class="op">+</span> np.array(error_term).ravel()</span>
<span id="cb83-27"><a href="regression-and-gradient-descent.html#cb83-27" tabindex="-1"></a>true_coefficients <span class="op">=</span> np.array([<span class="dv">40000</span>, <span class="dv">10000</span>, <span class="dv">700</span>])</span>
<span id="cb83-28"><a href="regression-and-gradient-descent.html#cb83-28" tabindex="-1"></a></span>
<span id="cb83-29"><a href="regression-and-gradient-descent.html#cb83-29" tabindex="-1"></a><span class="co"># scatter plot between schooling and education </span></span>
<span id="cb83-30"><a href="regression-and-gradient-descent.html#cb83-30" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb83-31"><a href="regression-and-gradient-descent.html#cb83-31" tabindex="-1"></a>plt.scatter(educa, income)</span>
<span id="cb83-32"><a href="regression-and-gradient-descent.html#cb83-32" tabindex="-1"></a>plt.title(<span class="st">&#39;Scatter plot of education and income&#39;</span>)</span>
<span id="cb83-33"><a href="regression-and-gradient-descent.html#cb83-33" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle <span class="op">=</span><span class="st">&#39;dashed&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>)</span>
<span id="cb83-34"><a href="regression-and-gradient-descent.html#cb83-34" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Schooling&#39;</span>)</span>
<span id="cb83-35"><a href="regression-and-gradient-descent.html#cb83-35" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Earnings&#39;</span>)</span>
<span id="cb83-36"><a href="regression-and-gradient-descent.html#cb83-36" tabindex="-1"></a></span>
<span id="cb83-37"><a href="regression-and-gradient-descent.html#cb83-37" tabindex="-1"></a><span class="co"># scatter plot between schooling and error</span></span>
<span id="cb83-38"><a href="regression-and-gradient-descent.html#cb83-38" tabindex="-1"></a>error_true <span class="op">=</span> income <span class="op">-</span> (<span class="dv">40000</span> <span class="op">+</span> <span class="dv">10000</span> <span class="op">*</span> educa_scaled <span class="op">+</span> <span class="dv">700</span> <span class="op">*</span> ability_scaled)</span>
<span id="cb83-39"><a href="regression-and-gradient-descent.html#cb83-39" tabindex="-1"></a></span>
<span id="cb83-40"><a href="regression-and-gradient-descent.html#cb83-40" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb83-41"><a href="regression-and-gradient-descent.html#cb83-41" tabindex="-1"></a>plt.scatter(educa, error_term, alpha <span class="op">=</span> <span class="fl">0.3</span>, color <span class="op">=</span> <span class="st">&#39;red&#39;</span>)</span>
<span id="cb83-42"><a href="regression-and-gradient-descent.html#cb83-42" tabindex="-1"></a>plt.title(<span class="st">&#39;Relationship between schooling and error term </span><span class="ch">\n</span><span class="st"> using the true parameters&#39;</span>)</span>
<span id="cb83-43"><a href="regression-and-gradient-descent.html#cb83-43" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Schooling&#39;</span>)</span>
<span id="cb83-44"><a href="regression-and-gradient-descent.html#cb83-44" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Error&#39;</span>)</span>
<span id="cb83-45"><a href="regression-and-gradient-descent.html#cb83-45" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">&#39;dashed&#39;</span>)</span></code></pre></div>
<div class="float">
<img src="standard_errors_files/standard_errors_6_0.png" alt="png" />
<div class="figcaption">png</div>
</div>
<div class="float">
<img src="standard_errors_files/standard_errors_6_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<div class="float">
<img src="standard_errors_files/standard_errors_6_2.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>The plot above shows that although there exist a positive correlation between schooling and earnings. However, variation in earnings is higher for greater values of education. Using the true coefficients (we have these since this is a simulated DGP) we extract error and plot the relationship between schooling and errors. Here, we see a funnel shaped scatter plot with the mean of error aligned at 0.
This simple example breaks the homoskedasticity assumption. The main point is that the estimation of standard errors need to reflect the fact that error term comes from distribution with different variances. We now have what is known as heteroskedasticity.</p>
<p>Let’s start with estimates using the gradient descent.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="regression-and-gradient-descent.html#cb84-1" tabindex="-1"></a><span class="co"># features</span></span>
<span id="cb84-2"><a href="regression-and-gradient-descent.html#cb84-2" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((educa_scaled.reshape(m,<span class="dv">1</span>), ability_scaled.reshape(m,<span class="dv">1</span>)), axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb84-3"><a href="regression-and-gradient-descent.html#cb84-3" tabindex="-1"></a><span class="co"># Note: no need to scale the features as they already are scaled </span></span>
<span id="cb84-4"><a href="regression-and-gradient-descent.html#cb84-4" tabindex="-1"></a>X <span class="op">=</span> add_dummy_feature(X) <span class="co"># intercept term</span></span>
<span id="cb84-5"><a href="regression-and-gradient-descent.html#cb84-5" tabindex="-1"></a>y <span class="op">=</span> income.reshape((m, <span class="dv">1</span>))</span>
<span id="cb84-6"><a href="regression-and-gradient-descent.html#cb84-6" tabindex="-1"></a>p <span class="op">=</span> X.shape[<span class="dv">1</span>]           <span class="co"># number of features</span></span>
<span id="cb84-7"><a href="regression-and-gradient-descent.html#cb84-7" tabindex="-1"></a></span>
<span id="cb84-8"><a href="regression-and-gradient-descent.html#cb84-8" tabindex="-1"></a><span class="co"># initialize theta</span></span>
<span id="cb84-9"><a href="regression-and-gradient-descent.html#cb84-9" tabindex="-1"></a>theta <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, p).reshape((p, <span class="dv">1</span>))</span>
<span id="cb84-10"><a href="regression-and-gradient-descent.html#cb84-10" tabindex="-1"></a><span class="co"># learning rate</span></span>
<span id="cb84-11"><a href="regression-and-gradient-descent.html#cb84-11" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb84-12"><a href="regression-and-gradient-descent.html#cb84-12" tabindex="-1"></a><span class="co"># number of iterations</span></span>
<span id="cb84-13"><a href="regression-and-gradient-descent.html#cb84-13" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb84-14"><a href="regression-and-gradient-descent.html#cb84-14" tabindex="-1"></a></span>
<span id="cb84-15"><a href="regression-and-gradient-descent.html#cb84-15" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb84-16"><a href="regression-and-gradient-descent.html#cb84-16" tabindex="-1"></a></span>
<span id="cb84-17"><a href="regression-and-gradient-descent.html#cb84-17" tabindex="-1"></a>    <span class="co"># get the gradient and adjust theta against the gradient using the learning rate</span></span>
<span id="cb84-18"><a href="regression-and-gradient-descent.html#cb84-18" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X.T<span class="op">@</span>(y <span class="op">-</span> X<span class="op">@</span>theta )</span>
<span id="cb84-19"><a href="regression-and-gradient-descent.html#cb84-19" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">-</span> eta<span class="op">*</span>gradient </span>
<span id="cb84-20"><a href="regression-and-gradient-descent.html#cb84-20" tabindex="-1"></a> </span>
<span id="cb84-21"><a href="regression-and-gradient-descent.html#cb84-21" tabindex="-1"></a></span>
<span id="cb84-22"><a href="regression-and-gradient-descent.html#cb84-22" tabindex="-1"></a><span class="co"># Use sklearn module to check</span></span>
<span id="cb84-23"><a href="regression-and-gradient-descent.html#cb84-23" tabindex="-1"></a>reg <span class="op">=</span> linear_model.LinearRegression(fit_intercept<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb84-24"><a href="regression-and-gradient-descent.html#cb84-24" tabindex="-1"></a>reg.fit(X, y)</span>
<span id="cb84-25"><a href="regression-and-gradient-descent.html#cb84-25" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The estimates from sklearn is: </span><span class="sc">{</span>reg<span class="sc">.</span>coef_<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb84-26"><a href="regression-and-gradient-descent.html#cb84-26" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The estimates from manual GD is: </span><span class="sc">{</span>theta<span class="sc">.</span>reshape((<span class="dv">1</span>, <span class="dv">3</span>))<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>) </span></code></pre></div>
<pre><code>The estimates from sklearn is: [[40015.7547 10015.2551   714.2199]]
The estimates from manual GD is: [[40015.7547 10015.2551   714.2199]]</code></pre>
<p>We area simply running gradient descent in the code above. This should be familiar to you from previous lectures. Now, we’d want to move on to the standard errors to assess precision of these estimates. First, let’s explore the graphical relationship between education and residuals. Note that I’ve distinguised error vs. residuals – the former is what you get from using true coefficients (you never see this in practice), while the latter use the estimates.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="regression-and-gradient-descent.html#cb86-1" tabindex="-1"></a><span class="co"># Get residuals (note that we are using the estimated thetas)</span></span>
<span id="cb86-2"><a href="regression-and-gradient-descent.html#cb86-2" tabindex="-1"></a>residual <span class="op">=</span> y <span class="op">-</span> X <span class="op">@</span> theta</span>
<span id="cb86-3"><a href="regression-and-gradient-descent.html#cb86-3" tabindex="-1"></a></span>
<span id="cb86-4"><a href="regression-and-gradient-descent.html#cb86-4" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb86-5"><a href="regression-and-gradient-descent.html#cb86-5" tabindex="-1"></a>plt.scatter(educa, residual, alpha <span class="op">=</span> <span class="fl">0.3</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb86-6"><a href="regression-and-gradient-descent.html#cb86-6" tabindex="-1"></a>plt.title(<span class="st">&#39;Relationship between education and residuals&#39;</span>)</span>
<span id="cb86-7"><a href="regression-and-gradient-descent.html#cb86-7" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;education&#39;</span>)</span>
<span id="cb86-8"><a href="regression-and-gradient-descent.html#cb86-8" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;residuals&#39;</span>)</span>
<span id="cb86-9"><a href="regression-and-gradient-descent.html#cb86-9" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">&#39;dashed&#39;</span>)</span>
<span id="cb86-10"><a href="regression-and-gradient-descent.html#cb86-10" tabindex="-1"></a></span>
<span id="cb86-11"><a href="regression-and-gradient-descent.html#cb86-11" tabindex="-1"></a><span class="co"># %%[markdown]</span></span>
<span id="cb86-12"><a href="regression-and-gradient-descent.html#cb86-12" tabindex="-1"></a><span class="co"># As you can see we&#39;ve got a funnel shaped relationship between education and residual, again signaling heteroskedasticity. This is precisely coming from differing variance of error term based on education. This won&#39;t affect the estimates but will affect the standard errors. Note that there are many tests for homoskedasticity vs heteroskedasticity. But in practice, the case of homoskedasticity almost always fails. The simple way to see it is to plot the residuals with the variable of concern. If you have a funnel looking shape, then you&#39;ve got the case of heteroskedasticity.  </span></span>
<span id="cb86-13"><a href="regression-and-gradient-descent.html#cb86-13" tabindex="-1"></a></span>
<span id="cb86-14"><a href="regression-and-gradient-descent.html#cb86-14" tabindex="-1"></a><span class="co"># First, let&#39;s do some benchmarking by estimating the standard errors under the homoskedasticity assumption -- error terms have the same variance (which is incorrect in this case and in practice).   </span></span></code></pre></div>
<div class="float">
<img src="standard_errors_files/standard_errors_10_0.png" alt="png" />
<div class="figcaption">png</div>
</div>
<div class="sourceCode" id="cb87"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="regression-and-gradient-descent.html#cb87-1" tabindex="-1"></a><span class="co"># 1. Get the standard error of the regression</span></span>
<span id="cb87-2"><a href="regression-and-gradient-descent.html#cb87-2" tabindex="-1"></a>error <span class="op">=</span> (X <span class="op">@</span> theta <span class="op">-</span> y)</span>
<span id="cb87-3"><a href="regression-and-gradient-descent.html#cb87-3" tabindex="-1"></a>error_sq <span class="op">=</span> error.T <span class="op">@</span> error</span>
<span id="cb87-4"><a href="regression-and-gradient-descent.html#cb87-4" tabindex="-1"></a>sigma_sq <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (X.shape[<span class="dv">0</span>] <span class="op">-</span><span class="dv">3</span>) <span class="op">*</span> error_sq</span>
<span id="cb87-5"><a href="regression-and-gradient-descent.html#cb87-5" tabindex="-1"></a>se_reg <span class="op">=</span> np.sqrt(sigma_sq)</span>
<span id="cb87-6"><a href="regression-and-gradient-descent.html#cb87-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard error of the regression is: </span><span class="sc">{</span>se_reg<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb87-7"><a href="regression-and-gradient-descent.html#cb87-7" tabindex="-1"></a></span>
<span id="cb87-8"><a href="regression-and-gradient-descent.html#cb87-8" tabindex="-1"></a><span class="co"># 2. get standard errors of the respective coefficients </span></span>
<span id="cb87-9"><a href="regression-and-gradient-descent.html#cb87-9" tabindex="-1"></a>var_cov <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">*</span> sigma_sq</span>
<span id="cb87-10"><a href="regression-and-gradient-descent.html#cb87-10" tabindex="-1"></a>manual_se <span class="op">=</span> np.sqrt(np.diag(var_cov))</span>
<span id="cb87-11"><a href="regression-and-gradient-descent.html#cb87-11" tabindex="-1"></a></span>
<span id="cb87-12"><a href="regression-and-gradient-descent.html#cb87-12" tabindex="-1"></a></span>
<span id="cb87-13"><a href="regression-and-gradient-descent.html#cb87-13" tabindex="-1"></a><span class="co"># compare it with standard errors from statsmodels</span></span>
<span id="cb87-14"><a href="regression-and-gradient-descent.html#cb87-14" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb87-15"><a href="regression-and-gradient-descent.html#cb87-15" tabindex="-1"></a><span class="co"># se from stats model</span></span>
<span id="cb87-16"><a href="regression-and-gradient-descent.html#cb87-16" tabindex="-1"></a>X_sm <span class="op">=</span> sm.add_constant(X)   <span class="co"># add intercept</span></span>
<span id="cb87-17"><a href="regression-and-gradient-descent.html#cb87-17" tabindex="-1"></a>model <span class="op">=</span> sm.OLS(y, X_sm).fit()</span>
<span id="cb87-18"><a href="regression-and-gradient-descent.html#cb87-18" tabindex="-1"></a></span>
<span id="cb87-19"><a href="regression-and-gradient-descent.html#cb87-19" tabindex="-1"></a></span>
<span id="cb87-20"><a href="regression-and-gradient-descent.html#cb87-20" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The estimates from statmodels: </span><span class="sc">{</span>model<span class="sc">.</span>params<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)   <span class="co"># coefficients</span></span>
<span id="cb87-21"><a href="regression-and-gradient-descent.html#cb87-21" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The estimates from sklearn is: </span><span class="sc">{</span>reg<span class="sc">.</span>coef_<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb87-22"><a href="regression-and-gradient-descent.html#cb87-22" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The estimates from manual GD is: </span><span class="sc">{</span>theta<span class="sc">.</span>reshape((<span class="dv">1</span>, <span class="dv">3</span>))<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&#39;</span>) </span>
<span id="cb87-23"><a href="regression-and-gradient-descent.html#cb87-23" tabindex="-1"></a></span>
<span id="cb87-24"><a href="regression-and-gradient-descent.html#cb87-24" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard errors from statmodel under homoskedasticity: </span><span class="sc">{</span>model<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)      <span class="co"># standard errors</span></span>
<span id="cb87-25"><a href="regression-and-gradient-descent.html#cb87-25" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard errors (manual estimation) under homoskedasticity: </span><span class="sc">{</span>manual_se<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb87-26"><a href="regression-and-gradient-descent.html#cb87-26" tabindex="-1"></a></span>
<span id="cb87-27"><a href="regression-and-gradient-descent.html#cb87-27" tabindex="-1"></a></span>
<span id="cb87-28"><a href="regression-and-gradient-descent.html#cb87-28" tabindex="-1"></a><span class="co"># %%[markdown]</span></span>
<span id="cb87-29"><a href="regression-and-gradient-descent.html#cb87-29" tabindex="-1"></a><span class="co"># Now that we have estimated standard errors under the homoskedasticity assumption let&#39;s see how we can fix this. Before we move on, I want to reiterate that the origin of heteroskedasicity is due to error terms coming from the distribution of different variance. In that regard, we want to account for this in our estimation of standard errors. </span></span>
<span id="cb87-30"><a href="regression-and-gradient-descent.html#cb87-30" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb87-31"><a href="regression-and-gradient-descent.html#cb87-31" tabindex="-1"></a><span class="co"># To do so, we&#39;ll use a sandwich method, which you should&#39;ve heard from you previous classes. So what does it entail? Basically, we&#39;d want to form weights using the size of the error term. </span></span>
<span id="cb87-32"><a href="regression-and-gradient-descent.html#cb87-32" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb87-33"><a href="regression-and-gradient-descent.html#cb87-33" tabindex="-1"></a><span class="co"># Let&#39;s just derive the sandwich estimator:</span></span>
<span id="cb87-34"><a href="regression-and-gradient-descent.html#cb87-34" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb87-35"><a href="regression-and-gradient-descent.html#cb87-35" tabindex="-1"></a><span class="co"># \begin{align}</span></span>
<span id="cb87-36"><a href="regression-and-gradient-descent.html#cb87-36" tabindex="-1"></a><span class="co"># \hat{\beta} = (X^{T}X)^{-1}X^{T}Y \\</span></span>
<span id="cb87-37"><a href="regression-and-gradient-descent.html#cb87-37" tabindex="-1"></a><span class="co"># \hat{\beta} = (X^{T}X)^{-1}X^{T}(X\beta + \epsilon) \\</span></span>
<span id="cb87-38"><a href="regression-and-gradient-descent.html#cb87-38" tabindex="-1"></a><span class="co"># \hat{\beta} = \beta + (X^{T}X)^{-1}X^{T}\epsilon</span></span>
<span id="cb87-39"><a href="regression-and-gradient-descent.html#cb87-39" tabindex="-1"></a><span class="co"># \end{align}</span></span>
<span id="cb87-40"><a href="regression-and-gradient-descent.html#cb87-40" tabindex="-1"></a></span>
<span id="cb87-41"><a href="regression-and-gradient-descent.html#cb87-41" tabindex="-1"></a><span class="co"># The end line is the starting point. Note that $Var(a)=0$ and $Var(ax) = a^{2}Var(x)$, where $a$ is a constant and $x$ is a random variable. Let&#39;s then take the variance of $\hat{\beta}$:</span></span>
<span id="cb87-42"><a href="regression-and-gradient-descent.html#cb87-42" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb87-43"><a href="regression-and-gradient-descent.html#cb87-43" tabindex="-1"></a><span class="co"># \begin{align}</span></span>
<span id="cb87-44"><a href="regression-and-gradient-descent.html#cb87-44" tabindex="-1"></a><span class="co"># Var(\hat{\beta}) = Var((X^{T}X)^{-1}X^{T}\epsilon) \\</span></span>
<span id="cb87-45"><a href="regression-and-gradient-descent.html#cb87-45" tabindex="-1"></a><span class="co"># = (X^{T}X)^{-1}Var(X^{T}\epsilon)(X^{T}X)^{-1} \\</span></span>
<span id="cb87-46"><a href="regression-and-gradient-descent.html#cb87-46" tabindex="-1"></a><span class="co"># = (X^{T}X)^{-1}X^{T}Var(\epsilon)X(X^{T}X)^{-1} \\</span></span>
<span id="cb87-47"><a href="regression-and-gradient-descent.html#cb87-47" tabindex="-1"></a><span class="co"># = (X^{T}X)^{-1}X^{T} \Omega X(X^{T}X)^{-1}</span></span>
<span id="cb87-48"><a href="regression-and-gradient-descent.html#cb87-48" tabindex="-1"></a><span class="co"># \end{align}</span></span>
<span id="cb87-49"><a href="regression-and-gradient-descent.html#cb87-49" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb87-50"><a href="regression-and-gradient-descent.html#cb87-50" tabindex="-1"></a><span class="co"># Here, $\Omega$ is the scaling term and it is $\epsilon\epsilon^{T}.I$, which is a $n \times n$ diagonal matrix. We don&#39;t observe this, so replace this with the sample counterpart $\hat{\Omega} = \hat{\epsilon}\hat{\epsilon}^{T}I$. Notice that under the case of homoskedasticity $\Omega = \sigma^2 I$, which collapses $Var{\hat{\beta}}$ to $(X^{T}X)^{-1} \sigma^2$. This is what we used to estimate standard errors under homoskedasticity.</span></span>
<span id="cb87-51"><a href="regression-and-gradient-descent.html#cb87-51" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb87-52"><a href="regression-and-gradient-descent.html#cb87-52" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb87-53"><a href="regression-and-gradient-descent.html#cb87-53" tabindex="-1"></a><span class="co"># Let&#39;s estimate the standard errors accounting for heteroskedasicity. I&#39;m going to divide this into bun and stuffing part as I write the code.</span></span></code></pre></div>
<pre><code>standard error of the regression is: [[3001.23166629]] 
 

The estimates from statmodels: [40015.7547 10015.2551   714.2199]
The estimates from sklearn is: [[40015.7547 10015.2551   714.2199]]
The estimates from manual GD is: [[40015.7547 10015.2551   714.2199]] 
 

standard errors from statmodel under homoskedasticity: [30.0123 33.6297 33.6297]
standard errors (manual estimation) under homoskedasticity: [30.0123 33.6297 33.6297] 
 </code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="regression-and-gradient-descent.html#cb89-1" tabindex="-1"></a>bun <span class="op">=</span> np.linalg.inv(X.T<span class="op">@</span>X)</span>
<span id="cb89-2"><a href="regression-and-gradient-descent.html#cb89-2" tabindex="-1"></a>omega <span class="op">=</span> np.diag((residual<span class="op">**</span><span class="dv">2</span>).ravel()) <span class="co"># n * n diagonal matrix</span></span>
<span id="cb89-3"><a href="regression-and-gradient-descent.html#cb89-3" tabindex="-1"></a>stuff <span class="op">=</span> X.T <span class="op">@</span> omega <span class="op">@</span> X</span>
<span id="cb89-4"><a href="regression-and-gradient-descent.html#cb89-4" tabindex="-1"></a></span>
<span id="cb89-5"><a href="regression-and-gradient-descent.html#cb89-5" tabindex="-1"></a><span class="co"># get the variance covariance matrix</span></span>
<span id="cb89-6"><a href="regression-and-gradient-descent.html#cb89-6" tabindex="-1"></a>var_cov <span class="op">=</span> bun <span class="op">@</span> stuff <span class="op">@</span> bun</span>
<span id="cb89-7"><a href="regression-and-gradient-descent.html#cb89-7" tabindex="-1"></a>robust_se <span class="op">=</span> np.sqrt(np.diag(var_cov))</span>
<span id="cb89-8"><a href="regression-and-gradient-descent.html#cb89-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;heteroskedasticity robust standard error: </span><span class="sc">{</span>robust_se<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb89-9"><a href="regression-and-gradient-descent.html#cb89-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;se under homoskedastic assumption: </span><span class="sc">{</span>manual_se<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb89-10"><a href="regression-and-gradient-descent.html#cb89-10" tabindex="-1"></a></span>
<span id="cb89-11"><a href="regression-and-gradient-descent.html#cb89-11" tabindex="-1"></a><span class="co"># robust se using sm</span></span>
<span id="cb89-12"><a href="regression-and-gradient-descent.html#cb89-12" tabindex="-1"></a>model2 <span class="op">=</span> sm.OLS(y, X).fit(cov_type<span class="op">=</span><span class="st">&#39;HC0&#39;</span>)</span>
<span id="cb89-13"><a href="regression-and-gradient-descent.html#cb89-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;standard errors from stats model under heteroskedasticity: </span><span class="sc">{</span>model2<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>heteroskedasticity robust standard error: [30.0078 35.5333 33.4547]
se under homoskedastic assumption: [30.0123 33.6297 33.6297]
standard errors from stats model under heteroskedasticity: [30.0078 35.5333 33.4547]</code></pre>
<p>If you compare the standard errors under homoskedasticity versus heteroskedasticity, you will see that standard errors under heteroskedasticity are larger, particularly for the education estimate.</p>
<p>Note that there are various forms of heteroskedasicity robust standard error including “HC1”, “HC2”, and “HC3”. Here, we’ve used the “H0” type – you can dig deeper according to your need.</p>
<p>Let’s now discuss the case where error terms are not independent and are correlated. One can think of this in a geospatial form – the unexplained portion of income for people living in a particular area can be correlated. For example, if you have people living in Des Moines, Iowa and NYC, you probably will think that income is spatially correlated due to local market conditions, taste, cost of living and other unobserved factors attributing to spatial clustering.</p>
<p>In the following simulation exercise, we’ll incorporate this cluter-type correlation in the error terms. Specifically, we’ll build error as:</p>
<p><span class="math display">\[\begin{equation}
\epsilon_{ic} = u_{c} + v_{i}
\end{equation}\]</span></p>
<p>Essentially, the error term consists of: i) <span class="math inline">\(u_c\)</span> – the cluster specific shock (all units within a specific cluster gets the same value); and ii) individual specific term. Both <span class="math inline">\(u_c\)</span> and <span class="math inline">\(v_i\)</span> will come from a normal distribution with mean 0 and standard deviation 3,000 and 1,000, respectively.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="regression-and-gradient-descent.html#cb91-1" tabindex="-1"></a><span class="co"># number of clusters</span></span>
<span id="cb91-2"><a href="regression-and-gradient-descent.html#cb91-2" tabindex="-1"></a>nc <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb91-3"><a href="regression-and-gradient-descent.html#cb91-3" tabindex="-1"></a></span>
<span id="cb91-4"><a href="regression-and-gradient-descent.html#cb91-4" tabindex="-1"></a><span class="co"># number of units within cluster </span></span>
<span id="cb91-5"><a href="regression-and-gradient-descent.html#cb91-5" tabindex="-1"></a>ni <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb91-6"><a href="regression-and-gradient-descent.html#cb91-6" tabindex="-1"></a></span>
<span id="cb91-7"><a href="regression-and-gradient-descent.html#cb91-7" tabindex="-1"></a><span class="co"># total number of units</span></span>
<span id="cb91-8"><a href="regression-and-gradient-descent.html#cb91-8" tabindex="-1"></a>m <span class="op">=</span> nc <span class="op">*</span> ni</span>
<span id="cb91-9"><a href="regression-and-gradient-descent.html#cb91-9" tabindex="-1"></a></span>
<span id="cb91-10"><a href="regression-and-gradient-descent.html#cb91-10" tabindex="-1"></a><span class="co"># get the cluster shock</span></span>
<span id="cb91-11"><a href="regression-and-gradient-descent.html#cb91-11" tabindex="-1"></a>cluster_shock <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">3000</span>, nc)</span>
<span id="cb91-12"><a href="regression-and-gradient-descent.html#cb91-12" tabindex="-1"></a>cluster_index <span class="op">=</span> np.repeat(np.linspace(<span class="dv">1</span>, nc), ni).astype(<span class="st">&#39;int&#39;</span>)</span>
<span id="cb91-13"><a href="regression-and-gradient-descent.html#cb91-13" tabindex="-1"></a>u_shock <span class="op">=</span> cluster_shock[cluster_index<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb91-14"><a href="regression-and-gradient-descent.html#cb91-14" tabindex="-1"></a></span>
<span id="cb91-15"><a href="regression-and-gradient-descent.html#cb91-15" tabindex="-1"></a><span class="co"># get the individuals shock</span></span>
<span id="cb91-16"><a href="regression-and-gradient-descent.html#cb91-16" tabindex="-1"></a><span class="co"># in this case cluster shock dominates individual shock</span></span>
<span id="cb91-17"><a href="regression-and-gradient-descent.html#cb91-17" tabindex="-1"></a>v_shock <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1000</span>, m)</span>
<span id="cb91-18"><a href="regression-and-gradient-descent.html#cb91-18" tabindex="-1"></a></span>
<span id="cb91-19"><a href="regression-and-gradient-descent.html#cb91-19" tabindex="-1"></a>error <span class="op">=</span> u_shock <span class="op">+</span> v_shock</span></code></pre></div>
<p>Let’s estimate the model and plot the relationship between education and residuals by some clusters.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="regression-and-gradient-descent.html#cb92-1" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((educa_scaled.reshape((m, <span class="dv">1</span>)), ability_scaled.reshape((m,<span class="dv">1</span>))), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb92-2"><a href="regression-and-gradient-descent.html#cb92-2" tabindex="-1"></a>X <span class="op">=</span> add_dummy_feature(X)</span>
<span id="cb92-3"><a href="regression-and-gradient-descent.html#cb92-3" tabindex="-1"></a>y_c <span class="op">=</span> <span class="dv">40000</span> <span class="op">+</span> <span class="dv">10000</span> <span class="op">*</span> educa_scaled <span class="op">+</span> <span class="dv">700</span> <span class="op">*</span> ability_scaled <span class="op">+</span> error.ravel()</span>
<span id="cb92-4"><a href="regression-and-gradient-descent.html#cb92-4" tabindex="-1"></a>y_c <span class="op">=</span> y_c.reshape((m, <span class="dv">1</span>))</span>
<span id="cb92-5"><a href="regression-and-gradient-descent.html#cb92-5" tabindex="-1"></a></span>
<span id="cb92-6"><a href="regression-and-gradient-descent.html#cb92-6" tabindex="-1"></a><span class="co"># initialize theta</span></span>
<span id="cb92-7"><a href="regression-and-gradient-descent.html#cb92-7" tabindex="-1"></a>theta <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, p).reshape((p, <span class="dv">1</span>))</span>
<span id="cb92-8"><a href="regression-and-gradient-descent.html#cb92-8" tabindex="-1"></a><span class="co"># learning rate</span></span>
<span id="cb92-9"><a href="regression-and-gradient-descent.html#cb92-9" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb92-10"><a href="regression-and-gradient-descent.html#cb92-10" tabindex="-1"></a><span class="co"># number of iterations</span></span>
<span id="cb92-11"><a href="regression-and-gradient-descent.html#cb92-11" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb92-12"><a href="regression-and-gradient-descent.html#cb92-12" tabindex="-1"></a></span>
<span id="cb92-13"><a href="regression-and-gradient-descent.html#cb92-13" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb92-14"><a href="regression-and-gradient-descent.html#cb92-14" tabindex="-1"></a>    <span class="co"># get the gradient and adjust theta against the gradient using the learning rate</span></span>
<span id="cb92-15"><a href="regression-and-gradient-descent.html#cb92-15" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X.T<span class="op">@</span>(y_c <span class="op">-</span> X<span class="op">@</span>theta)</span>
<span id="cb92-16"><a href="regression-and-gradient-descent.html#cb92-16" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">-</span> eta<span class="op">*</span>gradient </span>
<span id="cb92-17"><a href="regression-and-gradient-descent.html#cb92-17" tabindex="-1"></a></span>
<span id="cb92-18"><a href="regression-and-gradient-descent.html#cb92-18" tabindex="-1"></a><span class="bu">print</span>(theta)</span>
<span id="cb92-19"><a href="regression-and-gradient-descent.html#cb92-19" tabindex="-1"></a></span>
<span id="cb92-20"><a href="regression-and-gradient-descent.html#cb92-20" tabindex="-1"></a>resid_clus <span class="op">=</span> y_c <span class="op">-</span> X<span class="op">@</span>theta </span>
<span id="cb92-21"><a href="regression-and-gradient-descent.html#cb92-21" tabindex="-1"></a></span>
<span id="cb92-22"><a href="regression-and-gradient-descent.html#cb92-22" tabindex="-1"></a></span>
<span id="cb92-23"><a href="regression-and-gradient-descent.html#cb92-23" tabindex="-1"></a><span class="co"># then re-generate income </span></span>
<span id="cb92-24"><a href="regression-and-gradient-descent.html#cb92-24" tabindex="-1"></a></span>
<span id="cb92-25"><a href="regression-and-gradient-descent.html#cb92-25" tabindex="-1"></a>error_clus <span class="op">=</span> income <span class="op">-</span> (<span class="dv">40000</span> <span class="op">+</span> <span class="dv">10000</span> <span class="op">*</span> educa_scaled <span class="op">+</span> <span class="dv">700</span> <span class="op">*</span> ability_scaled)</span>
<span id="cb92-26"><a href="regression-and-gradient-descent.html#cb92-26" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb92-27"><a href="regression-and-gradient-descent.html#cb92-27" tabindex="-1"></a>plt.scatter(educa[cluster_index<span class="op">==</span><span class="dv">1</span>], resid_clus[cluster_index<span class="op">==</span><span class="dv">1</span>], color <span class="op">=</span> <span class="st">&#39;red&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, label<span class="op">=</span><span class="st">&#39;Cluster 1&#39;</span>)</span>
<span id="cb92-28"><a href="regression-and-gradient-descent.html#cb92-28" tabindex="-1"></a>plt.scatter(educa[cluster_index<span class="op">==</span><span class="dv">10</span>], resid_clus[cluster_index<span class="op">==</span><span class="dv">10</span>], color <span class="op">=</span> <span class="st">&#39;blue&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, label<span class="op">=</span><span class="st">&#39;Cluster 10&#39;</span>)</span>
<span id="cb92-29"><a href="regression-and-gradient-descent.html#cb92-29" tabindex="-1"></a>plt.scatter(educa[cluster_index<span class="op">==</span><span class="dv">20</span>], resid_clus[cluster_index<span class="op">==</span><span class="dv">20</span>], color <span class="op">=</span> <span class="st">&#39;green&#39;</span>, alpha <span class="op">=</span> <span class="fl">0.3</span>, label<span class="op">=</span><span class="st">&#39;Cluster 20&#39;</span>)</span>
<span id="cb92-30"><a href="regression-and-gradient-descent.html#cb92-30" tabindex="-1"></a>plt.legend()</span>
<span id="cb92-31"><a href="regression-and-gradient-descent.html#cb92-31" tabindex="-1"></a>plt.title(<span class="st">&#39;Relationship between schooling and residuals </span><span class="ch">\n</span><span class="st"> by some chosen cluster&#39;</span>)</span>
<span id="cb92-32"><a href="regression-and-gradient-descent.html#cb92-32" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;schooling&#39;</span>)</span>
<span id="cb92-33"><a href="regression-and-gradient-descent.html#cb92-33" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;residuals&#39;</span>)</span></code></pre></div>
<pre><code>[[39649.37831353]
 [10027.34709599]
 [  695.56370296]]





Text(0, 0.5, &#39;residuals&#39;)</code></pre>
<div class="float">
<img src="standard_errors_files/standard_errors_16_2.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>In the figure above, we see that residuals across different clusters are grouped at certain residual values. This depicts the clustering problem. Now, we need to adjust our the standard errors to account for this kind of clustering. The variance now takes the form:</p>
<p><span class="math display">\[\begin{equation}
Var(\hat \beta) = (X^{T}X)^{-1} \bigg(\sum_{c=1}^{G} X^{T}_c \epsilon_c \epsilon^{T}_c X_c \bigg) (X^{T}X)^{-1}
\end{equation}\]</span></p>
<p>Let’s estimate this!!</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="regression-and-gradient-descent.html#cb94-1" tabindex="-1"></a>bun <span class="op">=</span> np.linalg.inv(X.T<span class="op">@</span>X)</span>
<span id="cb94-2"><a href="regression-and-gradient-descent.html#cb94-2" tabindex="-1"></a>stuff <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>]<span class="op">*</span>X.shape[<span class="dv">1</span>]).reshape((X.shape[<span class="dv">1</span>], X.shape[<span class="dv">1</span>]))</span>
<span id="cb94-3"><a href="regression-and-gradient-descent.html#cb94-3" tabindex="-1"></a></span>
<span id="cb94-4"><a href="regression-and-gradient-descent.html#cb94-4" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, nc<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb94-5"><a href="regression-and-gradient-descent.html#cb94-5" tabindex="-1"></a></span>
<span id="cb94-6"><a href="regression-and-gradient-descent.html#cb94-6" tabindex="-1"></a>    X_c <span class="op">=</span> X[cluster_index<span class="op">==</span>i]</span>
<span id="cb94-7"><a href="regression-and-gradient-descent.html#cb94-7" tabindex="-1"></a>    res_c <span class="op">=</span> resid_clus[cluster_index<span class="op">==</span>i]</span>
<span id="cb94-8"><a href="regression-and-gradient-descent.html#cb94-8" tabindex="-1"></a></span>
<span id="cb94-9"><a href="regression-and-gradient-descent.html#cb94-9" tabindex="-1"></a>    mid <span class="op">=</span> X_c.T<span class="op">@</span>res_c<span class="op">@</span>res_c.T<span class="op">@</span>X_c</span>
<span id="cb94-10"><a href="regression-and-gradient-descent.html#cb94-10" tabindex="-1"></a>    stuff <span class="op">+=</span> mid</span>
<span id="cb94-11"><a href="regression-and-gradient-descent.html#cb94-11" tabindex="-1"></a></span>
<span id="cb94-12"><a href="regression-and-gradient-descent.html#cb94-12" tabindex="-1"></a>var_cov_clus <span class="op">=</span> bun<span class="op">@</span>stuff<span class="op">@</span>bun</span>
<span id="cb94-13"><a href="regression-and-gradient-descent.html#cb94-13" tabindex="-1"></a>se_clus <span class="op">=</span> np.sqrt(np.diag(var_cov_clus))</span>
<span id="cb94-14"><a href="regression-and-gradient-descent.html#cb94-14" tabindex="-1"></a></span>
<span id="cb94-15"><a href="regression-and-gradient-descent.html#cb94-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Cluster robust standard error is: </span><span class="sc">{</span>se_clus<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb94-16"><a href="regression-and-gradient-descent.html#cb94-16" tabindex="-1"></a></span>
<span id="cb94-17"><a href="regression-and-gradient-descent.html#cb94-17" tabindex="-1"></a><span class="co"># small sample correction term</span></span>
<span id="cb94-18"><a href="regression-and-gradient-descent.html#cb94-18" tabindex="-1"></a>correction <span class="op">=</span> ((nc<span class="op">/</span>(nc<span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> (m<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>(m<span class="op">-</span>p))</span>
<span id="cb94-19"><a href="regression-and-gradient-descent.html#cb94-19" tabindex="-1"></a>var_cov_clust_corr <span class="op">=</span> var_cov_clus <span class="op">*</span> correction</span>
<span id="cb94-20"><a href="regression-and-gradient-descent.html#cb94-20" tabindex="-1"></a>se_clus_correct <span class="op">=</span> np.sqrt(np.diag(var_cov_clust_corr))</span>
<span id="cb94-21"><a href="regression-and-gradient-descent.html#cb94-21" tabindex="-1"></a></span>
<span id="cb94-22"><a href="regression-and-gradient-descent.html#cb94-22" tabindex="-1"></a>model_clus <span class="op">=</span> sm.OLS(y_c, X).fit(cov_type<span class="op">=</span><span class="st">&#39;cluster&#39;</span>, cov_kwds<span class="op">=</span>{<span class="st">&#39;groups&#39;</span>: cluster_index})</span>
<span id="cb94-23"><a href="regression-and-gradient-descent.html#cb94-23" tabindex="-1"></a>model_noclus <span class="op">=</span> sm.OLS(y_c, X).fit()</span>
<span id="cb94-24"><a href="regression-and-gradient-descent.html#cb94-24" tabindex="-1"></a></span>
<span id="cb94-25"><a href="regression-and-gradient-descent.html#cb94-25" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Clustered standard errors (not small sample corrected): </span><span class="sc">{</span>se_clus<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb94-26"><a href="regression-and-gradient-descent.html#cb94-26" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Clustered se corrected for small sample: </span><span class="sc">{</span>se_clus_correct<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb94-27"><a href="regression-and-gradient-descent.html#cb94-27" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Clustered standard error from statsmodel: </span><span class="sc">{</span>model_clus<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb94-28"><a href="regression-and-gradient-descent.html#cb94-28" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;standard error without clustering from statsmodel: </span><span class="sc">{</span>model_noclus<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>Cluster robust standard error is: [421.13627575  28.40377396  32.01738254]
Clustered standard errors (not small sample corrected): [421.1363  28.4038  32.0174]
Clustered se corrected for small sample: [425.4544  28.695   32.3457]
Clustered standard error from statsmodel: [425.4544  28.695   32.3457]
standard error without clustering from statsmodel: [31.4168 35.2034 35.2034]</code></pre>
<p>We’ve now computed the clustered standard errors and compared it with those from statsmodel. They are very exactly the same. However, clustered standard errors are larger compared to unclustered standard errors.</p>
<p><strong>Bootstrapped standard errors</strong></p>
<p>Bootstapping can lead to a convenient way of obtaining standard errors. The bootstrapping process assumes
the data as the population and resamples <span class="math inline">\(k\)</span> number of times from the population (with replacement) and re-estimates the coefficients on every sample. You’ll then have <span class="math inline">\(k\)</span> number of estimates. The standard error
is simply the standard deviation of the estimates.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="regression-and-gradient-descent.html#cb96-1" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb96-2"><a href="regression-and-gradient-descent.html#cb96-2" tabindex="-1"></a></span>
<span id="cb96-3"><a href="regression-and-gradient-descent.html#cb96-3" tabindex="-1"></a>rep <span class="op">=</span> <span class="dv">199</span></span>
<span id="cb96-4"><a href="regression-and-gradient-descent.html#cb96-4" tabindex="-1"></a><span class="co"># learning rate</span></span>
<span id="cb96-5"><a href="regression-and-gradient-descent.html#cb96-5" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">1e-2</span></span>
<span id="cb96-6"><a href="regression-and-gradient-descent.html#cb96-6" tabindex="-1"></a><span class="co"># number of iterations</span></span>
<span id="cb96-7"><a href="regression-and-gradient-descent.html#cb96-7" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb96-8"><a href="regression-and-gradient-descent.html#cb96-8" tabindex="-1"></a></span>
<span id="cb96-9"><a href="regression-and-gradient-descent.html#cb96-9" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, rep):</span>
<span id="cb96-10"><a href="regression-and-gradient-descent.html#cb96-10" tabindex="-1"></a></span>
<span id="cb96-11"><a href="regression-and-gradient-descent.html#cb96-11" tabindex="-1"></a>    boot_index <span class="op">=</span> np.random.choice(X.shape[<span class="dv">0</span>], size<span class="op">=</span>X.shape[<span class="dv">0</span>], replace<span class="op">=</span><span class="va">True</span>).astype(<span class="st">&#39;int&#39;</span>)</span>
<span id="cb96-12"><a href="regression-and-gradient-descent.html#cb96-12" tabindex="-1"></a>    boot_index <span class="op">=</span> np.array(boot_index) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb96-13"><a href="regression-and-gradient-descent.html#cb96-13" tabindex="-1"></a>    X_boot <span class="op">=</span> X[boot_index] </span>
<span id="cb96-14"><a href="regression-and-gradient-descent.html#cb96-14" tabindex="-1"></a>    y_boot <span class="op">=</span> y_c[boot_index]</span>
<span id="cb96-15"><a href="regression-and-gradient-descent.html#cb96-15" tabindex="-1"></a></span>
<span id="cb96-16"><a href="regression-and-gradient-descent.html#cb96-16" tabindex="-1"></a>    <span class="co"># initialize theta</span></span>
<span id="cb96-17"><a href="regression-and-gradient-descent.html#cb96-17" tabindex="-1"></a>    theta <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, p).reshape((p, <span class="dv">1</span>))</span>
<span id="cb96-18"><a href="regression-and-gradient-descent.html#cb96-18" tabindex="-1"></a></span>
<span id="cb96-19"><a href="regression-and-gradient-descent.html#cb96-19" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, epochs):</span>
<span id="cb96-20"><a href="regression-and-gradient-descent.html#cb96-20" tabindex="-1"></a>    <span class="co"># get the gradient and adjust theta against the gradient using the learning rate</span></span>
<span id="cb96-21"><a href="regression-and-gradient-descent.html#cb96-21" tabindex="-1"></a>        gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>X_boot.shape[<span class="dv">0</span>] <span class="op">*</span> X_boot.T<span class="op">@</span>(y_boot <span class="op">-</span> X_boot<span class="op">@</span>theta)</span>
<span id="cb96-22"><a href="regression-and-gradient-descent.html#cb96-22" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> eta<span class="op">*</span>gradient </span>
<span id="cb96-23"><a href="regression-and-gradient-descent.html#cb96-23" tabindex="-1"></a></span>
<span id="cb96-24"><a href="regression-and-gradient-descent.html#cb96-24" tabindex="-1"></a>    <span class="cf">if</span> k <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb96-25"><a href="regression-and-gradient-descent.html#cb96-25" tabindex="-1"></a>        theta_store <span class="op">=</span> theta.ravel()</span>
<span id="cb96-26"><a href="regression-and-gradient-descent.html#cb96-26" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb96-27"><a href="regression-and-gradient-descent.html#cb96-27" tabindex="-1"></a>        theta_new <span class="op">=</span> theta.ravel()</span>
<span id="cb96-28"><a href="regression-and-gradient-descent.html#cb96-28" tabindex="-1"></a>        theta_store <span class="op">=</span> np.vstack((theta_store, theta_new))    </span>
<span id="cb96-29"><a href="regression-and-gradient-descent.html#cb96-29" tabindex="-1"></a></span>
<span id="cb96-30"><a href="regression-and-gradient-descent.html#cb96-30" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;Rep </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss"> estimate: </span><span class="sc">{</span>theta<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb96-31"><a href="regression-and-gradient-descent.html#cb96-31" tabindex="-1"></a>    </span>
<span id="cb96-32"><a href="regression-and-gradient-descent.html#cb96-32" tabindex="-1"></a></span>
<span id="cb96-33"><a href="regression-and-gradient-descent.html#cb96-33" tabindex="-1"></a>boot_se <span class="op">=</span> theta_store.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb96-34"><a href="regression-and-gradient-descent.html#cb96-34" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;bootstrapped non-clustered version of se: </span><span class="sc">{</span>boot_se<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb96-35"><a href="regression-and-gradient-descent.html#cb96-35" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;standard error without clustering from statsmodel: </span><span class="sc">{</span>model_noclus<span class="sc">.</span>bse<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>Rep 0 estimate: [[39634.20759692]
 [ 9972.15137597]
 [  652.72915823]]


Rep 1 estimate: [[39621.9140577 ]
 [10046.57035788]
 [  672.42939652]]


Rep 2 estimate: [[39623.08461529]
 [ 9981.58228619]
 [  670.98263527]]


Rep 3 estimate: [[39637.55183125]
 [10001.7489246 ]
 [  662.05036779]]


Rep 4 estimate: [[39695.44764121]
 [10041.16930021]
 [  647.56837512]]


Rep 5 estimate: [[39615.34205097]
 [10017.89522589]
 [  686.87890025]]


Rep 6 estimate: [[39678.63656974]
 [10036.74032889]
 [  707.02820496]]


Rep 7 estimate: [[39635.53503934]
 [ 9995.33859027]
 [  690.70481666]]


Rep 8 estimate: [[39600.00610224]
 [10028.68218574]
 [  702.75449706]]


Rep 9 estimate: [[39707.94055818]
 [10059.68129534]
 [  654.26712762]]


Rep 10 estimate: [[39612.10414709]
 [ 9981.12202919]
 [  736.70234899]]


Rep 11 estimate: [[39681.05918001]
 [ 9975.17250646]
 [  709.55558489]]


Rep 12 estimate: [[39647.47856378]
 [ 9981.73243325]
 [  753.13733332]]


Rep 13 estimate: [[39667.34485868]
 [10036.61524871]
 [  687.09562445]]


Rep 14 estimate: [[39659.16998691]
 [10000.52337148]
 [  710.01139155]]


Rep 15 estimate: [[39639.29382108]
 [10082.30415622]
 [  742.44583502]]


Rep 16 estimate: [[39625.32899474]
 [ 9987.89864738]
 [  748.9561938 ]]


Rep 17 estimate: [[39667.92479251]
 [10000.07091612]
 [  682.58096323]]


Rep 18 estimate: [[39647.64304062]
 [10065.09464912]
 [  672.04108689]]


Rep 19 estimate: [[39610.70165961]
 [10018.89623775]
 [  729.73636443]]


Rep 20 estimate: [[39617.45842608]
 [10069.92475955]
 [  685.70389042]]


Rep 21 estimate: [[39588.24995471]
 [10013.35529898]
 [  689.32223213]]


Rep 22 estimate: [[39658.37619605]
 [10072.95445493]
 [  696.82674721]]


Rep 23 estimate: [[39687.52536611]
 [10040.72734011]
 [  649.30189093]]


Rep 24 estimate: [[39666.07008584]
 [10043.61662354]
 [  666.85956965]]


Rep 25 estimate: [[39603.38815643]
 [10047.87591532]
 [  632.34129952]]


Rep 26 estimate: [[39657.30936341]
 [10031.24586445]
 [  715.40765174]]


Rep 27 estimate: [[39615.99259115]
 [ 9957.31296536]
 [  696.13705344]]


Rep 28 estimate: [[39643.66488259]
 [10008.39182557]
 [  730.86219762]]


Rep 29 estimate: [[39675.34722975]
 [10081.34286247]
 [  676.3526735 ]]


Rep 30 estimate: [[39614.46013655]
 [10001.8207831 ]
 [  719.41863241]]


Rep 31 estimate: [[39640.25866004]
 [10021.40479851]
 [  704.30228288]]


Rep 32 estimate: [[39612.09199402]
 [10033.38174397]
 [  663.34553222]]


Rep 33 estimate: [[39683.79571144]
 [ 9973.46820969]
 [  723.40389677]]


Rep 34 estimate: [[39686.12970807]
 [ 9956.55977931]
 [  744.53260858]]


Rep 35 estimate: [[39632.88902075]
 [10036.84480922]
 [  689.57898674]]


Rep 36 estimate: [[39713.00878458]
 [10015.81757479]
 [  715.55775228]]


Rep 37 estimate: [[39671.48133869]
 [10107.90762039]
 [  671.13913505]]


Rep 38 estimate: [[39643.65792854]
 [10058.90937064]
 [  693.25339089]]


Rep 39 estimate: [[39703.82590057]
 [10076.31928723]
 [  654.39904957]]


Rep 40 estimate: [[39643.81718312]
 [ 9964.54652872]
 [  698.7648128 ]]


Rep 41 estimate: [[39689.97372164]
 [10000.7727396 ]
 [  687.52259812]]


Rep 42 estimate: [[39612.87625102]
 [10004.74660384]
 [  709.37496101]]


Rep 43 estimate: [[39671.32538866]
 [10028.43288591]
 [  700.32820727]]


Rep 44 estimate: [[39630.74865614]
 [10100.3991206 ]
 [  611.18128762]]


Rep 45 estimate: [[39622.90791883]
 [10071.05706214]
 [  673.05439047]]


Rep 46 estimate: [[39604.76657256]
 [10057.85387121]
 [  628.8394797 ]]


Rep 47 estimate: [[39641.07825529]
 [10049.36557045]
 [  730.63707784]]


Rep 48 estimate: [[39691.79768444]
 [10002.91419186]
 [  693.30898309]]


Rep 49 estimate: [[39662.42792761]
 [10037.23799975]
 [  713.29822611]]


Rep 50 estimate: [[39663.69589865]
 [10074.31186539]
 [  645.45476732]]


Rep 51 estimate: [[39680.76408045]
 [ 9987.93311561]
 [  760.82821243]]


Rep 52 estimate: [[39619.97082001]
 [10037.60657886]
 [  661.40091985]]


Rep 53 estimate: [[39624.9279004 ]
 [10024.43239989]
 [  745.91481201]]


Rep 54 estimate: [[39586.35708561]
 [10076.95450602]
 [  629.20304524]]


Rep 55 estimate: [[39679.83165912]
 [10071.96718542]
 [  644.02298151]]


Rep 56 estimate: [[39601.42306921]
 [10091.46830495]
 [  664.50994212]]


Rep 57 estimate: [[39738.84348681]
 [10028.95597784]
 [  695.86288615]]


Rep 58 estimate: [[39647.87951559]
 [ 9998.21533346]
 [  661.00115565]]


Rep 59 estimate: [[39647.8723745 ]
 [10016.47193961]
 [  639.26241395]]


Rep 60 estimate: [[39642.84507487]
 [10034.21172881]
 [  653.12708525]]


Rep 61 estimate: [[39648.71749172]
 [10053.26102583]
 [  676.84555042]]


Rep 62 estimate: [[39654.7967103 ]
 [10045.72267797]
 [  666.79901184]]


Rep 63 estimate: [[39612.28274296]
 [10011.06062689]
 [  697.86622241]]


Rep 64 estimate: [[39614.60384791]
 [10021.96923194]
 [  722.94352736]]


Rep 65 estimate: [[39612.37330471]
 [10064.7123475 ]
 [  669.34877168]]


Rep 66 estimate: [[39673.08721685]
 [10012.1679277 ]
 [  745.4580107 ]]


Rep 67 estimate: [[39618.32940955]
 [10013.97419028]
 [  668.39873329]]


Rep 68 estimate: [[39628.51457672]
 [10011.12953266]
 [  696.81355033]]


Rep 69 estimate: [[39658.02181461]
 [10068.86929084]
 [  689.81993823]]


Rep 70 estimate: [[39637.89056608]
 [10052.13019266]
 [  715.32750376]]


Rep 71 estimate: [[39679.5330422 ]
 [10100.65291174]
 [  677.75745628]]


Rep 72 estimate: [[39692.65248798]
 [10000.46060365]
 [  702.46626574]]


Rep 73 estimate: [[39658.81539007]
 [10024.15467442]
 [  645.60990573]]


Rep 74 estimate: [[39642.66877888]
 [10023.86808413]
 [  723.51478569]]


Rep 75 estimate: [[39695.40950835]
 [10019.72084127]
 [  664.74411942]]


Rep 76 estimate: [[39637.94858277]
 [10012.92585604]
 [  709.17713849]]


Rep 77 estimate: [[39622.50035905]
 [10046.02128644]
 [  675.20862677]]


Rep 78 estimate: [[39628.14242031]
 [10080.18874293]
 [  622.71631863]]


Rep 79 estimate: [[39682.81735629]
 [10041.9898932 ]
 [  692.45776192]]


Rep 80 estimate: [[39675.97498997]
 [10039.89993909]
 [  694.07466724]]


Rep 81 estimate: [[39638.87090673]
 [10077.1659821 ]
 [  686.19713262]]


Rep 82 estimate: [[39630.96885157]
 [10022.10255276]
 [  726.14058914]]


Rep 83 estimate: [[39636.3769543 ]
 [10030.72330636]
 [  705.44437295]]


Rep 84 estimate: [[39624.18801433]
 [10010.07055701]
 [  675.1970448 ]]


Rep 85 estimate: [[39707.51948677]
 [10071.49412137]
 [  701.92964802]]


Rep 86 estimate: [[39615.4515855 ]
 [10017.99562684]
 [  716.18090475]]


Rep 87 estimate: [[39608.54965838]
 [10050.41049065]
 [  697.41637554]]


Rep 88 estimate: [[39691.98713524]
 [10098.09824757]
 [  713.48827376]]


Rep 89 estimate: [[39595.55443741]
 [10047.83576716]
 [  668.21668283]]


Rep 90 estimate: [[39731.69916921]
 [10016.69969976]
 [  668.1809089 ]]


Rep 91 estimate: [[39652.0587333 ]
 [10013.15558147]
 [  666.57907701]]


Rep 92 estimate: [[39625.19746259]
 [10006.67630543]
 [  676.34971174]]


Rep 93 estimate: [[39623.1732082 ]
 [10056.00513643]
 [  657.67129513]]


Rep 94 estimate: [[39669.56381442]
 [10011.80050511]
 [  755.80095212]]


Rep 95 estimate: [[39611.31590999]
 [10039.15137472]
 [  702.07703054]]


Rep 96 estimate: [[39612.57842562]
 [10051.82340555]
 [  703.45902349]]


Rep 97 estimate: [[39692.78602923]
 [10042.04713415]
 [  677.38841659]]


Rep 98 estimate: [[39661.76137101]
 [10059.24880542]
 [  682.59772266]]


Rep 99 estimate: [[39606.1170304 ]
 [10060.23053653]
 [  670.5534201 ]]


Rep 100 estimate: [[39713.9395932 ]
 [10042.55208298]
 [  716.8405365 ]]


Rep 101 estimate: [[39679.54740889]
 [10023.6023723 ]
 [  675.70266313]]


Rep 102 estimate: [[39669.29585361]
 [10075.18985606]
 [  670.30338812]]


Rep 103 estimate: [[39617.53461811]
 [10025.62430093]
 [  657.2055975 ]]


Rep 104 estimate: [[39630.54394129]
 [10016.80633441]
 [  674.27638597]]


Rep 105 estimate: [[39675.24635594]
 [10051.38483056]
 [  655.39525373]]


Rep 106 estimate: [[39650.47580971]
 [10015.50862261]
 [  744.81596155]]


Rep 107 estimate: [[39681.66575401]
 [10009.71142048]
 [  635.38642252]]


Rep 108 estimate: [[39646.16404817]
 [10098.75866793]
 [  656.33659447]]


Rep 109 estimate: [[39673.45866978]
 [ 9980.98604561]
 [  709.54582486]]


Rep 110 estimate: [[39683.01458711]
 [10060.47250086]
 [  659.34579579]]


Rep 111 estimate: [[39615.95093122]
 [10085.35188812]
 [  642.85064756]]


Rep 112 estimate: [[39685.07117305]
 [10073.87300929]
 [  675.62633986]]


Rep 113 estimate: [[39700.80508502]
 [10025.39673853]
 [  723.95671698]]


Rep 114 estimate: [[39654.13705739]
 [10062.99956193]
 [  705.49724421]]


Rep 115 estimate: [[39642.98033672]
 [10046.23254871]
 [  669.82054311]]


Rep 116 estimate: [[39673.93569365]
 [10044.52343701]
 [  730.61526628]]


Rep 117 estimate: [[39567.36170446]
 [10079.10737707]
 [  702.04875241]]


Rep 118 estimate: [[39640.99781573]
 [10049.8079462 ]
 [  696.77124729]]


Rep 119 estimate: [[39652.924091  ]
 [ 9966.1126205 ]
 [  690.08012937]]


Rep 120 estimate: [[39640.84772593]
 [10017.88419631]
 [  680.06462807]]


Rep 121 estimate: [[39648.81492862]
 [10085.39461703]
 [  631.33819534]]


Rep 122 estimate: [[39617.66740925]
 [10001.81992616]
 [  738.36649983]]


Rep 123 estimate: [[39663.81801586]
 [10016.87701287]
 [  719.76040622]]


Rep 124 estimate: [[39577.83973312]
 [ 9996.32517714]
 [  670.45491691]]


Rep 125 estimate: [[39694.78878319]
 [ 9952.60675322]
 [  718.64306284]]


Rep 126 estimate: [[39660.56138757]
 [10004.64619447]
 [  656.04524912]]


Rep 127 estimate: [[39651.64358692]
 [10048.61973202]
 [  654.07486899]]


Rep 128 estimate: [[39689.03339121]
 [10032.29573575]
 [  738.45004651]]


Rep 129 estimate: [[39691.52021343]
 [10052.08173475]
 [  659.96646377]]


Rep 130 estimate: [[39652.85760954]
 [ 9986.7354964 ]
 [  709.03049442]]


Rep 131 estimate: [[39632.99566456]
 [ 9966.98461718]
 [  732.63392343]]


Rep 132 estimate: [[39668.93435262]
 [10036.96414159]
 [  649.0258689 ]]


Rep 133 estimate: [[39670.47972999]
 [10089.33536851]
 [  644.24850734]]


Rep 134 estimate: [[39640.55040963]
 [10053.44319661]
 [  674.20945602]]


Rep 135 estimate: [[39623.85153201]
 [ 9994.22996327]
 [  695.09818855]]


Rep 136 estimate: [[39645.97319946]
 [ 9971.11629625]
 [  721.15848313]]


Rep 137 estimate: [[39658.9619191 ]
 [10019.48005858]
 [  661.48300874]]


Rep 138 estimate: [[39642.57914846]
 [ 9997.19070144]
 [  667.73498415]]


Rep 139 estimate: [[39637.4290495 ]
 [10034.32711427]
 [  661.76117443]]


Rep 140 estimate: [[39628.80713831]
 [10017.68117611]
 [  680.75140228]]


Rep 141 estimate: [[39674.93078111]
 [10013.53733556]
 [  721.24820465]]


Rep 142 estimate: [[39651.40792942]
 [10025.72045083]
 [  709.04701078]]


Rep 143 estimate: [[39639.10392042]
 [10016.2437193 ]
 [  724.07604731]]


Rep 144 estimate: [[39646.46862892]
 [10025.67431022]
 [  688.40242255]]


Rep 145 estimate: [[39650.14696411]
 [10005.51665637]
 [  712.88464213]]


Rep 146 estimate: [[39646.92820979]
 [ 9954.15976451]
 [  776.37739858]]


Rep 147 estimate: [[39620.3282916 ]
 [10042.6185706 ]
 [  654.70339839]]


Rep 148 estimate: [[39650.55655917]
 [ 9991.17781519]
 [  645.11118853]]


Rep 149 estimate: [[39667.55051198]
 [ 9990.73834714]
 [  738.68975989]]


Rep 150 estimate: [[39671.71292371]
 [ 9977.57126474]
 [  728.57423806]]


Rep 151 estimate: [[39637.46442206]
 [10060.23241637]
 [  663.34468336]]


Rep 152 estimate: [[39681.52863218]
 [10054.86696935]
 [  655.32975744]]


Rep 153 estimate: [[39647.28083936]
 [10116.25440095]
 [  639.00191274]]


Rep 154 estimate: [[39620.82929953]
 [10047.83682386]
 [  717.52890504]]


Rep 155 estimate: [[39647.51544097]
 [10036.1348693 ]
 [  759.02598785]]


Rep 156 estimate: [[39658.71758628]
 [10057.79740778]
 [  673.53520113]]


Rep 157 estimate: [[39652.42180026]
 [10039.6446241 ]
 [  652.37900513]]


Rep 158 estimate: [[39622.38554988]
 [ 9991.14210226]
 [  766.88430685]]


Rep 159 estimate: [[39651.29699042]
 [10065.91893929]
 [  689.47118359]]


Rep 160 estimate: [[39675.69878036]
 [10068.01885768]
 [  687.51871023]]


Rep 161 estimate: [[39631.29501096]
 [ 9912.34553365]
 [  767.78971857]]


Rep 162 estimate: [[39653.05962809]
 [10024.96701508]
 [  717.29791508]]


Rep 163 estimate: [[39666.19510328]
 [10005.02761342]
 [  706.49793799]]


Rep 164 estimate: [[39656.32635251]
 [10024.47016931]
 [  695.98769732]]


Rep 165 estimate: [[39702.17892756]
 [10052.94497995]
 [  706.22741806]]


Rep 166 estimate: [[39606.48521272]
 [10010.67628793]
 [  714.82999807]]


Rep 167 estimate: [[39652.91873686]
 [10028.93993568]
 [  672.76873611]]


Rep 168 estimate: [[39641.79602234]
 [10000.60435574]
 [  685.80996805]]


Rep 169 estimate: [[39696.57476929]
 [ 9945.83651273]
 [  720.77406998]]


Rep 170 estimate: [[39607.16983671]
 [10027.29761694]
 [  708.6226797 ]]


Rep 171 estimate: [[39589.956391  ]
 [ 9976.67312017]
 [  709.95922716]]


Rep 172 estimate: [[39680.0983667 ]
 [10011.88421355]
 [  642.28909189]]


Rep 173 estimate: [[39623.31804032]
 [10082.066283  ]
 [  656.84713371]]


Rep 174 estimate: [[39726.15662337]
 [ 9963.56427099]
 [  685.90386222]]


Rep 175 estimate: [[39678.70850114]
 [10023.98807359]
 [  661.27963856]]


Rep 176 estimate: [[39612.72240517]
 [10013.51792832]
 [  701.60793869]]


Rep 177 estimate: [[39651.28878098]
 [10032.53114015]
 [  718.29337211]]


Rep 178 estimate: [[39651.79886099]
 [10025.78545733]
 [  711.18636276]]


Rep 179 estimate: [[39668.81176963]
 [ 9950.64830118]
 [  707.17029946]]


Rep 180 estimate: [[39664.55033915]
 [10025.69574577]
 [  682.17967506]]


Rep 181 estimate: [[39678.45863918]
 [10000.94471527]
 [  714.3528562 ]]


Rep 182 estimate: [[39623.72938936]
 [10009.48496587]
 [  793.69376095]]


Rep 183 estimate: [[39638.09616265]
 [10031.80512906]
 [  714.30847955]]


Rep 184 estimate: [[39658.07866713]
 [10045.5638743 ]
 [  702.43641255]]


Rep 185 estimate: [[39651.57617443]
 [10039.40055719]
 [  696.51313447]]


Rep 186 estimate: [[39644.5923245 ]
 [ 9999.10838089]
 [  703.16412766]]


Rep 187 estimate: [[39667.01121342]
 [ 9994.92130014]
 [  707.2300142 ]]


Rep 188 estimate: [[39655.6793376 ]
 [ 9993.46295671]
 [  667.46454071]]


Rep 189 estimate: [[39674.40927094]
 [10038.5271482 ]
 [  689.60601406]]


Rep 190 estimate: [[39670.97584095]
 [ 9989.99235183]
 [  689.55471574]]


Rep 191 estimate: [[39656.01227867]
 [ 9993.23920429]
 [  763.2792197 ]]


Rep 192 estimate: [[39632.08646723]
 [10037.74031865]
 [  709.21037716]]


Rep 193 estimate: [[39672.99028294]
 [10050.50258356]
 [  735.72762695]]


Rep 194 estimate: [[39725.59632485]
 [ 9997.65628356]
 [  732.66952791]]


Rep 195 estimate: [[39684.14531155]
 [10002.98869804]
 [  717.85523503]]


Rep 196 estimate: [[39674.37506247]
 [10014.25014105]
 [  720.57318313]]


Rep 197 estimate: [[39616.73965371]
 [10013.33283978]
 [  631.645646  ]]


Rep 198 estimate: [[39638.25336916]
 [10025.39517299]
 [  707.12836366]]
bootstrapped non-clustered version of se: [30.7524 34.9706 33.2355]
standard error without clustering from statsmodel: [31.4168 35.2034 35.2034]</code></pre>
<p>There you have it – the bootstrapped version of the standard errors. Note that we’ve not accounted for the clusters and one can do that by sampling clusters (with replacement) rather than units.</p>

<p>Logistic regression</p>
<p><strong>Course:</strong> Causal Inference</p>
<p><strong>Topic:</strong> Logistic Regression</p>
<p>Say, you need to predict the probability of someone being in a good vs. bad health (binary outcome)
given a set of
inputs: i) college education (yes or no); ii) income (high vs. low); iii) insured vs uninsured;
and iv) stress level (continuous variable).</p>
<p>For the sake of simplicity, we are going to assume a super simple DGP as follows:</p>
<ol style="list-style-type: decimal">
<li>College education has a positive effect on health (coefficient = 0.1)</li>
<li>High income has a positive effect on health (coefficient = 0.2)</li>
<li>40 percent more people from higher income households have college education</li>
<li>Insurance has a positive effect on health (coefficient = 0.05)</li>
<li>60 percent more people from low income households are likely to be stressed</li>
<li>Stress has a negative effect on health (coefficient = -1)</li>
</ol>
<p><strong>Note:</strong> We know from the previous lecture that LPM estimates directly gives us the marginal
effects – the coefficients (at least theoretically) are interpreted as the effect of marginal changes
in <span class="math inline">\(X\)</span>s on <span class="math inline">\(y\)</span>. LPM estimates are straight forward and easy to interpret as most often we are
concerned with marginal effects from a policy standpoint.
However, the coefficients pertaining to logistic regression are not marginal effects.
This will be clearer as we proceed.</p>
<p>We looked at the Linear Probability Model (LPM) in the previous lecture – the estimates from a
properly specified model were close to the true parameters.
What if we have to estimate probability
of someone being in good health? In this case, LPM does
not gurantee that the probabilities are restricted between 0 and 1.</p>
<p>Logistic regression is a tool that is used to model binary outcome and used for classification purposes.
It uses a logistic function to restrict
probabilities between values of 0 and 1. So how does it work?</p>
<p>Essentially, the primary goal here is to predict probabilities of a binary event.
The true probability is written as a function of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(X\)</span>:
<span class="math display">\[
p = h_\theta(X) = \sigma(\theta X)
\]</span></p>
<p><span class="math inline">\(\theta\)</span> is a vector of true parameters – they govern the DGP, and probabilites
are the function of the true coefficients and inputs.
Specifically, <span class="math inline">\(\sigma(.)\)</span> is the logistic function, defined as:</p>
<p><span class="math display">\[
\sigma(z) = \frac{1}{(1 + exp(-z))}
\]</span></p>
<p>This <span class="math inline">\(\sigma(.)\)</span> is also known as the sigmoid function and <span class="math inline">\(z=\theta X\)</span> is often
known as the logit. The logit is a
linear combination of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(X\)</span>. By nature of the logistic function, the output is restricted
between 0 and 1.<br />
To see the logistic function closely, let’s take a look at the following graph.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="regression-and-gradient-descent.html#cb98-1" tabindex="-1"></a></span>
<span id="cb98-2"><a href="regression-and-gradient-descent.html#cb98-2" tabindex="-1"></a><span class="co"># import necessary libraries</span></span>
<span id="cb98-3"><a href="regression-and-gradient-descent.html#cb98-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np    </span>
<span id="cb98-4"><a href="regression-and-gradient-descent.html#cb98-4" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb98-5"><a href="regression-and-gradient-descent.html#cb98-5" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb98-6"><a href="regression-and-gradient-descent.html#cb98-6" tabindex="-1"></a></span>
<span id="cb98-7"><a href="regression-and-gradient-descent.html#cb98-7" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb98-8"><a href="regression-and-gradient-descent.html#cb98-8" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb98-9"><a href="regression-and-gradient-descent.html#cb98-9" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> add_dummy_feature</span>
<span id="cb98-10"><a href="regression-and-gradient-descent.html#cb98-10" tabindex="-1"></a></span>
<span id="cb98-11"><a href="regression-and-gradient-descent.html#cb98-11" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb98-12"><a href="regression-and-gradient-descent.html#cb98-12" tabindex="-1"></a></span>
<span id="cb98-13"><a href="regression-and-gradient-descent.html#cb98-13" tabindex="-1"></a><span class="co"># generate numbers from -5 to 5</span></span>
<span id="cb98-14"><a href="regression-and-gradient-descent.html#cb98-14" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1000</span>)</span>
<span id="cb98-15"><a href="regression-and-gradient-descent.html#cb98-15" tabindex="-1"></a><span class="bu">print</span>(z[<span class="dv">0</span>:<span class="dv">20</span>])</span>
<span id="cb98-16"><a href="regression-and-gradient-descent.html#cb98-16" tabindex="-1"></a><span class="co"># compute logistic values (note that these are probabilities)</span></span>
<span id="cb98-17"><a href="regression-and-gradient-descent.html#cb98-17" tabindex="-1"></a>sigma_z <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb98-18"><a href="regression-and-gradient-descent.html#cb98-18" tabindex="-1"></a></span>
<span id="cb98-19"><a href="regression-and-gradient-descent.html#cb98-19" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb98-20"><a href="regression-and-gradient-descent.html#cb98-20" tabindex="-1"></a>plt.plot(z, sigma_z)</span>
<span id="cb98-21"><a href="regression-and-gradient-descent.html#cb98-21" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;z&#39;</span>)</span>
<span id="cb98-22"><a href="regression-and-gradient-descent.html#cb98-22" tabindex="-1"></a>plt.title(<span class="st">&#39;A sigmoid function&#39;</span>)</span>
<span id="cb98-23"><a href="regression-and-gradient-descent.html#cb98-23" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;probability&#39;</span>)</span>
<span id="cb98-24"><a href="regression-and-gradient-descent.html#cb98-24" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>[-5.         -4.98998999 -4.97997998 -4.96996997 -4.95995996 -4.94994995
 -4.93993994 -4.92992993 -4.91991992 -4.90990991 -4.8998999  -4.88988989
 -4.87987988 -4.86986987 -4.85985986 -4.84984985 -4.83983984 -4.82982983
 -4.81981982 -4.80980981]</code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_1_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Note that the logistic function is S-shaped – negative logit (z) values will have
probabilities less than 0.5,
whereas the positive z values will have
probablities greater than 0.5. Also, probabilities on the vertical axis are
constrained between 0 and 1, as they should be.</p>
<p>The inputs in the sigmoid function is: <span class="math inline">\(z=\theta X\)</span>, which will help us attain probabilities.
<span class="math inline">\(X\)</span> and <span class="math inline">\(\theta\)</span> are the features (covariates) and the
parameters of interest, respectively.</p>
<p>Using these probability values, one can classify. For example:</p>
<p><span class="math inline">\(y_i = 1\)</span> if <span class="math inline">\(\hat{p_i}\geq 0.5\)</span> or else 0.</p>
<p>Our goal is to come up with the estimates of <span class="math inline">\(\theta\)</span>. After we have <span class="math inline">\(\hat{\theta}\)</span>, we can obtain
probabilities, perform classification based on them, or use probability estimates for downstream analysis.</p>
<p><strong>The Loss Function</strong></p>
<p>To do so, we will start with a loss function. Consider the following:</p>
<p><span class="math inline">\(C = -\log(\hat{p_i})\)</span> if <span class="math inline">\(y_i=1\)</span></p>
<p><span class="math inline">\(C = -\log(1-\hat{p_i})\)</span> if <span class="math inline">\(y_i=0\)</span></p>
<p>Generally speaking, you want the model to come up with higher probabilities for observations with
<span class="math inline">\(y_i=1\)</span> and lower probabilities for <span class="math inline">\(y_i=0\)</span>. With this in mind, consider what might happen
if <span class="math inline">\(\hat{p_i}\)</span> is small vs large (say, 0.05 vs 0.95) when <span class="math inline">\(y_i=1\)</span>.
This will inflate the loss in the former case but reduce it in the latter. The case is
reversed for <span class="math inline">\(y_i=0\)</span>; higher probabilities will yield higher loss; whereas, lower probabilities
will yield lower loss values. So, lower
probabilities are ‘shunned’ for observations with <span class="math inline">\(y_i=1\)</span>, and higher probabilities are penalized
more for observations with <span class="math inline">\(y=0\)</span>.</p>
<p>We put this logic together and come up with the following cross-entropy loss function:</p>
<p><span class="math display">\[
C_{\theta} = -\frac{1}{n} \sum_i^{n} [y_i \times \log(\hat{p_i}) + (1-y_i) \times \log(1-\hat{p_i})]
\]</span></p>
<p>Recall:
<span class="math display">\[
p = \sigma(\theta X) = \frac{1}{(1 + exp(-\theta X))}
\]</span></p>
<p>The <em>objective</em> is to get the estimates of <span class="math inline">\(\theta\)</span> that minimizes the loss function. Turns out that the
loss function above don’t have an analytical or a closed form solution. However, the function is convex, which
means that we can use gradient descent to estimate <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Using Gradient Descent</strong></p>
<p>Let’s first simulate the data following the DGP mentioned above.
Note that the functional that’s used to simulate the
outcome variable (health) will depend on probability values obtained from the
logistic function.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="regression-and-gradient-descent.html#cb100-1" tabindex="-1"></a></span>
<span id="cb100-2"><a href="regression-and-gradient-descent.html#cb100-2" tabindex="-1"></a><span class="co"># ----------------------------</span></span>
<span id="cb100-3"><a href="regression-and-gradient-descent.html#cb100-3" tabindex="-1"></a></span>
<span id="cb100-4"><a href="regression-and-gradient-descent.html#cb100-4" tabindex="-1"></a><span class="co"># A. Simulate data </span></span>
<span id="cb100-5"><a href="regression-and-gradient-descent.html#cb100-5" tabindex="-1"></a></span>
<span id="cb100-6"><a href="regression-and-gradient-descent.html#cb100-6" tabindex="-1"></a><span class="co"># ----------------------------</span></span>
<span id="cb100-7"><a href="regression-and-gradient-descent.html#cb100-7" tabindex="-1"></a></span>
<span id="cb100-8"><a href="regression-and-gradient-descent.html#cb100-8" tabindex="-1"></a><span class="co"># 1. College education has a positive effect on health (coefficient = 0.1)</span></span>
<span id="cb100-9"><a href="regression-and-gradient-descent.html#cb100-9" tabindex="-1"></a><span class="co"># 2. High income has a positive effect on health (coefficient = 0.2)</span></span>
<span id="cb100-10"><a href="regression-and-gradient-descent.html#cb100-10" tabindex="-1"></a><span class="co"># 3. 40 percent more people from higher income households have college education</span></span>
<span id="cb100-11"><a href="regression-and-gradient-descent.html#cb100-11" tabindex="-1"></a><span class="co"># 4. Insurance has a positive effect on health (coefficient = 0.05)</span></span>
<span id="cb100-12"><a href="regression-and-gradient-descent.html#cb100-12" tabindex="-1"></a><span class="co"># 5. Stress has a negative effect on health (coefficient = -1)</span></span>
<span id="cb100-13"><a href="regression-and-gradient-descent.html#cb100-13" tabindex="-1"></a></span>
<span id="cb100-14"><a href="regression-and-gradient-descent.html#cb100-14" tabindex="-1"></a><span class="co"># number of obs</span></span>
<span id="cb100-15"><a href="regression-and-gradient-descent.html#cb100-15" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb100-16"><a href="regression-and-gradient-descent.html#cb100-16" tabindex="-1"></a></span>
<span id="cb100-17"><a href="regression-and-gradient-descent.html#cb100-17" tabindex="-1"></a><span class="co"># 1. income </span></span>
<span id="cb100-18"><a href="regression-and-gradient-descent.html#cb100-18" tabindex="-1"></a>income_log <span class="op">=</span> np.random.lognormal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb100-19"><a href="regression-and-gradient-descent.html#cb100-19" tabindex="-1"></a>income <span class="op">=</span> income_log <span class="op">*</span> <span class="dv">20000</span></span>
<span id="cb100-20"><a href="regression-and-gradient-descent.html#cb100-20" tabindex="-1"></a>ln_income <span class="op">=</span> np.log(income)</span>
<span id="cb100-21"><a href="regression-and-gradient-descent.html#cb100-21" tabindex="-1"></a><span class="co"># categorize high vs low income based on median income</span></span>
<span id="cb100-22"><a href="regression-and-gradient-descent.html#cb100-22" tabindex="-1"></a>high_income <span class="op">=</span> (income<span class="op">&gt;=</span>np.median(income)).astype(<span class="st">&#39;int&#39;</span>)</span>
<span id="cb100-23"><a href="regression-and-gradient-descent.html#cb100-23" tabindex="-1"></a>low_income <span class="op">=</span> (income<span class="op">&lt;</span>np.median(income)).astype(<span class="st">&#39;int&#39;</span>)</span>
<span id="cb100-24"><a href="regression-and-gradient-descent.html#cb100-24" tabindex="-1"></a></span>
<span id="cb100-25"><a href="regression-and-gradient-descent.html#cb100-25" tabindex="-1"></a><span class="co"># 2. college </span></span>
<span id="cb100-26"><a href="regression-and-gradient-descent.html#cb100-26" tabindex="-1"></a><span class="kw">def</span> gen_college(prob):</span>
<span id="cb100-27"><a href="regression-and-gradient-descent.html#cb100-27" tabindex="-1"></a>    col <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, prob, <span class="dv">1</span>)</span>
<span id="cb100-28"><a href="regression-and-gradient-descent.html#cb100-28" tabindex="-1"></a>    <span class="cf">return</span> col</span>
<span id="cb100-29"><a href="regression-and-gradient-descent.html#cb100-29" tabindex="-1"></a></span>
<span id="cb100-30"><a href="regression-and-gradient-descent.html#cb100-30" tabindex="-1"></a>college <span class="op">=</span> []</span>
<span id="cb100-31"><a href="regression-and-gradient-descent.html#cb100-31" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb100-32"><a href="regression-and-gradient-descent.html#cb100-32" tabindex="-1"></a>    <span class="co"># 40% more people from high income group will have college degree</span></span>
<span id="cb100-33"><a href="regression-and-gradient-descent.html#cb100-33" tabindex="-1"></a>    college_i <span class="op">=</span> gen_college(<span class="fl">0.2</span> <span class="op">+</span> <span class="fl">0.4</span><span class="op">*</span>high_income[i])</span>
<span id="cb100-34"><a href="regression-and-gradient-descent.html#cb100-34" tabindex="-1"></a>    college.append(college_i)</span>
<span id="cb100-35"><a href="regression-and-gradient-descent.html#cb100-35" tabindex="-1"></a></span>
<span id="cb100-36"><a href="regression-and-gradient-descent.html#cb100-36" tabindex="-1"></a>college <span class="op">=</span> np.array(college).ravel()</span>
<span id="cb100-37"><a href="regression-and-gradient-descent.html#cb100-37" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;mean of college: </span><span class="sc">{</span>college<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb100-38"><a href="regression-and-gradient-descent.html#cb100-38" tabindex="-1"></a></span>
<span id="cb100-39"><a href="regression-and-gradient-descent.html#cb100-39" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;share college for high income: </span><span class="sc">{</span>np<span class="sc">.</span>mean(college[high_income <span class="op">==</span> <span class="dv">1</span>])<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb100-40"><a href="regression-and-gradient-descent.html#cb100-40" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;share college for low income: </span><span class="sc">{</span>np<span class="sc">.</span>mean(college[high_income <span class="op">==</span> <span class="dv">0</span>])<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb100-41"><a href="regression-and-gradient-descent.html#cb100-41" tabindex="-1"></a></span>
<span id="cb100-42"><a href="regression-and-gradient-descent.html#cb100-42" tabindex="-1"></a><span class="co"># 3. Stress </span></span>
<span id="cb100-43"><a href="regression-and-gradient-descent.html#cb100-43" tabindex="-1"></a><span class="kw">def</span> gen_stress(prob):</span>
<span id="cb100-44"><a href="regression-and-gradient-descent.html#cb100-44" tabindex="-1"></a>    p <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, prob, <span class="dv">1</span>)</span>
<span id="cb100-45"><a href="regression-and-gradient-descent.html#cb100-45" tabindex="-1"></a>    <span class="cf">return</span> p</span>
<span id="cb100-46"><a href="regression-and-gradient-descent.html#cb100-46" tabindex="-1"></a></span>
<span id="cb100-47"><a href="regression-and-gradient-descent.html#cb100-47" tabindex="-1"></a>stress <span class="op">=</span> []</span>
<span id="cb100-48"><a href="regression-and-gradient-descent.html#cb100-48" tabindex="-1"></a></span>
<span id="cb100-49"><a href="regression-and-gradient-descent.html#cb100-49" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb100-50"><a href="regression-and-gradient-descent.html#cb100-50" tabindex="-1"></a>    <span class="co"># 60% more people in low income will be stressed</span></span>
<span id="cb100-51"><a href="regression-and-gradient-descent.html#cb100-51" tabindex="-1"></a>    stress_i <span class="op">=</span> gen_stress(<span class="fl">0.6</span><span class="op">*</span>low_income[i])</span>
<span id="cb100-52"><a href="regression-and-gradient-descent.html#cb100-52" tabindex="-1"></a>    stress.append(stress_i)</span>
<span id="cb100-53"><a href="regression-and-gradient-descent.html#cb100-53" tabindex="-1"></a></span>
<span id="cb100-54"><a href="regression-and-gradient-descent.html#cb100-54" tabindex="-1"></a><span class="co"># a continuous stress variable dependent on income status</span></span>
<span id="cb100-55"><a href="regression-and-gradient-descent.html#cb100-55" tabindex="-1"></a>stress <span class="op">=</span> np.array(stress).ravel() <span class="op">+</span> np.random.normal(<span class="dv">3</span>, <span class="dv">1</span>, n)<span class="op">*</span>low_income <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb100-56"><a href="regression-and-gradient-descent.html#cb100-56" tabindex="-1"></a></span>
<span id="cb100-57"><a href="regression-and-gradient-descent.html#cb100-57" tabindex="-1"></a><span class="co"># histogram of the stress index</span></span>
<span id="cb100-58"><a href="regression-and-gradient-descent.html#cb100-58" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb100-59"><a href="regression-and-gradient-descent.html#cb100-59" tabindex="-1"></a>plt.hist(stress, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&quot;steelblue&quot;</span>, edgecolor<span class="op">=</span><span class="st">&quot;black&quot;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb100-60"><a href="regression-and-gradient-descent.html#cb100-60" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;stress index&#39;</span>)</span>
<span id="cb100-61"><a href="regression-and-gradient-descent.html#cb100-61" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;frequency&#39;</span>)</span>
<span id="cb100-62"><a href="regression-and-gradient-descent.html#cb100-62" tabindex="-1"></a>plt.title(<span class="st">&#39;Histogram of stress index&#39;</span>)</span>
<span id="cb100-63"><a href="regression-and-gradient-descent.html#cb100-63" tabindex="-1"></a>plt.show()</span>
<span id="cb100-64"><a href="regression-and-gradient-descent.html#cb100-64" tabindex="-1"></a></span>
<span id="cb100-65"><a href="regression-and-gradient-descent.html#cb100-65" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;average stress index for low income group: </span><span class="sc">{</span>stress[high_income<span class="op">==</span><span class="dv">0</span>]<span class="sc">.</span>mean()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb100-66"><a href="regression-and-gradient-descent.html#cb100-66" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;average stress index for high income group: </span><span class="sc">{</span>stress[high_income<span class="op">==</span><span class="dv">1</span>]<span class="sc">.</span>mean()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb100-67"><a href="regression-and-gradient-descent.html#cb100-67" tabindex="-1"></a></span>
<span id="cb100-68"><a href="regression-and-gradient-descent.html#cb100-68" tabindex="-1"></a><span class="co"># 4. Insurance (exogeneous -- does not depend on other Xs)</span></span>
<span id="cb100-69"><a href="regression-and-gradient-descent.html#cb100-69" tabindex="-1"></a>insurance <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.3</span>, n)</span>
<span id="cb100-70"><a href="regression-and-gradient-descent.html#cb100-70" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;fraction insured: </span><span class="sc">{</span>insurance<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb100-71"><a href="regression-and-gradient-descent.html#cb100-71" tabindex="-1"></a></span>
<span id="cb100-72"><a href="regression-and-gradient-descent.html#cb100-72" tabindex="-1"></a><span class="co"># 5. health (Y variable)</span></span>
<span id="cb100-73"><a href="regression-and-gradient-descent.html#cb100-73" tabindex="-1"></a><span class="kw">def</span> gen_health(prob):</span>
<span id="cb100-74"><a href="regression-and-gradient-descent.html#cb100-74" tabindex="-1"></a>    h <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, prob, <span class="dv">1</span>) <span class="co"># these probabilities are going to come from the logistic function</span></span>
<span id="cb100-75"><a href="regression-and-gradient-descent.html#cb100-75" tabindex="-1"></a>    <span class="cf">return</span> h</span>
<span id="cb100-76"><a href="regression-and-gradient-descent.html#cb100-76" tabindex="-1"></a></span>
<span id="cb100-77"><a href="regression-and-gradient-descent.html#cb100-77" tabindex="-1"></a><span class="co"># ----------------------------------------------------</span></span>
<span id="cb100-78"><a href="regression-and-gradient-descent.html#cb100-78" tabindex="-1"></a></span>
<span id="cb100-79"><a href="regression-and-gradient-descent.html#cb100-79" tabindex="-1"></a></span>
<span id="cb100-80"><a href="regression-and-gradient-descent.html#cb100-80" tabindex="-1"></a><span class="co"># Logistic regression using the gradient descent </span></span>
<span id="cb100-81"><a href="regression-and-gradient-descent.html#cb100-81" tabindex="-1"></a></span>
<span id="cb100-82"><a href="regression-and-gradient-descent.html#cb100-82" tabindex="-1"></a></span>
<span id="cb100-83"><a href="regression-and-gradient-descent.html#cb100-83" tabindex="-1"></a><span class="co"># ----------------------------------------------------</span></span>
<span id="cb100-84"><a href="regression-and-gradient-descent.html#cb100-84" tabindex="-1"></a><span class="co"># define the logistic function</span></span>
<span id="cb100-85"><a href="regression-and-gradient-descent.html#cb100-85" tabindex="-1"></a><span class="kw">def</span> sigma(<span class="bu">input</span>):</span>
<span id="cb100-86"><a href="regression-and-gradient-descent.html#cb100-86" tabindex="-1"></a>    logistic <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="bu">input</span>)) </span>
<span id="cb100-87"><a href="regression-and-gradient-descent.html#cb100-87" tabindex="-1"></a>    <span class="cf">return</span> logistic</span>
<span id="cb100-88"><a href="regression-and-gradient-descent.html#cb100-88" tabindex="-1"></a></span>
<span id="cb100-89"><a href="regression-and-gradient-descent.html#cb100-89" tabindex="-1"></a><span class="co"># true thetas governing the DGP</span></span>
<span id="cb100-90"><a href="regression-and-gradient-descent.html#cb100-90" tabindex="-1"></a>theta_true <span class="op">=</span> np.array([<span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.05</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb100-91"><a href="regression-and-gradient-descent.html#cb100-91" tabindex="-1"></a>sigma(theta_true)</span>
<span id="cb100-92"><a href="regression-and-gradient-descent.html#cb100-92" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((college.reshape((n, <span class="dv">1</span>)), </span>
<span id="cb100-93"><a href="regression-and-gradient-descent.html#cb100-93" tabindex="-1"></a>                    high_income.reshape((n, <span class="dv">1</span>)), </span>
<span id="cb100-94"><a href="regression-and-gradient-descent.html#cb100-94" tabindex="-1"></a>                    insurance.reshape((n, <span class="dv">1</span>)), </span>
<span id="cb100-95"><a href="regression-and-gradient-descent.html#cb100-95" tabindex="-1"></a>                    stress.reshape((n, <span class="dv">1</span>))),</span>
<span id="cb100-96"><a href="regression-and-gradient-descent.html#cb100-96" tabindex="-1"></a>                    axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb100-97"><a href="regression-and-gradient-descent.html#cb100-97" tabindex="-1"></a>X <span class="op">=</span> add_dummy_feature(X) <span class="co"># this adds 1 in the first column (intercept)</span></span>
<span id="cb100-98"><a href="regression-and-gradient-descent.html#cb100-98" tabindex="-1"></a>z <span class="op">=</span> X <span class="op">@</span> theta_true.reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb100-99"><a href="regression-and-gradient-descent.html#cb100-99" tabindex="-1"></a>prob_logit <span class="op">=</span> sigma(z)  <span class="co"># output true probabilities</span></span>
<span id="cb100-100"><a href="regression-and-gradient-descent.html#cb100-100" tabindex="-1"></a></span>
<span id="cb100-101"><a href="regression-and-gradient-descent.html#cb100-101" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: PROBABILITIES COME FROM THE LOGISTIC FUNCTION. THIS IS THE KEY TO SIMULATE LOGISTIC REGRESSION.</span></span>
<span id="cb100-102"><a href="regression-and-gradient-descent.html#cb100-102" tabindex="-1"></a><span class="co"># Step 1: Calculate linear combination (logit): z = X @ theta</span></span>
<span id="cb100-103"><a href="regression-and-gradient-descent.html#cb100-103" tabindex="-1"></a><span class="co"># Step 2: Transform to probabilities: p = sigma(z) = 1/(1 + exp(-z))</span></span>
<span id="cb100-104"><a href="regression-and-gradient-descent.html#cb100-104" tabindex="-1"></a><span class="co"># Step 3: Generate binary outcomes using these probabilities using a binomial dist.</span></span>
<span id="cb100-105"><a href="regression-and-gradient-descent.html#cb100-105" tabindex="-1"></a></span>
<span id="cb100-106"><a href="regression-and-gradient-descent.html#cb100-106" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb100-107"><a href="regression-and-gradient-descent.html#cb100-107" tabindex="-1"></a>plt.hist(prob_logit, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&quot;steelblue&quot;</span>, edgecolor<span class="op">=</span><span class="st">&quot;black&quot;</span>)</span>
<span id="cb100-108"><a href="regression-and-gradient-descent.html#cb100-108" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="dv">0</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb100-109"><a href="regression-and-gradient-descent.html#cb100-109" tabindex="-1"></a>plt.title(<span class="st">&#39;True probabilities using true theta values&#39;</span>)</span>
<span id="cb100-110"><a href="regression-and-gradient-descent.html#cb100-110" tabindex="-1"></a>plt.show()</span>
<span id="cb100-111"><a href="regression-and-gradient-descent.html#cb100-111" tabindex="-1"></a></span>
<span id="cb100-112"><a href="regression-and-gradient-descent.html#cb100-112" tabindex="-1"></a><span class="co"># generate health using probabilities</span></span>
<span id="cb100-113"><a href="regression-and-gradient-descent.html#cb100-113" tabindex="-1"></a>health <span class="op">=</span> []</span>
<span id="cb100-114"><a href="regression-and-gradient-descent.html#cb100-114" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb100-115"><a href="regression-and-gradient-descent.html#cb100-115" tabindex="-1"></a>    health_i <span class="op">=</span> gen_health(prob_logit[i]) <span class="co"># AGAIN, THESE PROBABILITIES COME FROM THE LOGISTIC FUNCTION</span></span>
<span id="cb100-116"><a href="regression-and-gradient-descent.html#cb100-116" tabindex="-1"></a>    health.append(health_i)</span>
<span id="cb100-117"><a href="regression-and-gradient-descent.html#cb100-117" tabindex="-1"></a></span>
<span id="cb100-118"><a href="regression-and-gradient-descent.html#cb100-118" tabindex="-1"></a>health <span class="op">=</span> np.array(health).ravel()</span></code></pre></div>
<pre><code>mean of college: 0.40291


share college for high income: 0.603
share college for low income: 0.20282</code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_3_2.png" alt="png" />
<div class="figcaption">png</div>
</div>
<pre><code>average stress index for low income group: 3.6107
average stress index for high income group: 0.0004
fraction insured: 0.30029</code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_3_4.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Since, this is a simulation, we know the true probabilities generated using the
true coefficients and the DGP. From a practitioner’s standpoint, we won’t know the
true probabilities in non-experimental settings, since we don’t know the true DGP to begin with. We have to estimate
them.
Let’s print out our some summary measures on health.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="regression-and-gradient-descent.html#cb103-1" tabindex="-1"></a><span class="co">#print(f&quot;y variable: {health} \n&quot;)</span></span>
<span id="cb103-2"><a href="regression-and-gradient-descent.html#cb103-2" tabindex="-1"></a><span class="co">#print(f&quot;X matrix: {X}&quot;)</span></span>
<span id="cb103-3"><a href="regression-and-gradient-descent.html#cb103-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;fraction with good health: </span><span class="sc">{</span>health<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb103-4"><a href="regression-and-gradient-descent.html#cb103-4" tabindex="-1"></a></span>
<span id="cb103-5"><a href="regression-and-gradient-descent.html#cb103-5" tabindex="-1"></a><span class="co"># create a stress band around the mean for no college, low income and uninsured</span></span>
<span id="cb103-6"><a href="regression-and-gradient-descent.html#cb103-6" tabindex="-1"></a>mean_stress_baseline <span class="op">=</span> stress[(college<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> (high_income<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> (insurance<span class="op">==</span><span class="dv">0</span>)].mean()</span>
<span id="cb103-7"><a href="regression-and-gradient-descent.html#cb103-7" tabindex="-1"></a>stress_tolerance <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># within ±0.5 of mean</span></span>
<span id="cb103-8"><a href="regression-and-gradient-descent.html#cb103-8" tabindex="-1"></a>stress_band <span class="op">=</span> (np.<span class="bu">abs</span>(stress <span class="op">-</span> mean_stress_baseline) <span class="op">&lt;=</span> stress_tolerance)</span>
<span id="cb103-9"><a href="regression-and-gradient-descent.html#cb103-9" tabindex="-1"></a></span>
<span id="cb103-10"><a href="regression-and-gradient-descent.html#cb103-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;fraction with good health among no school, low income, and uninsured: </span><span class="sc">{</span>np<span class="sc">.</span>mean(health[(college<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> </span>
<span id="cb103-11"><a href="regression-and-gradient-descent.html#cb103-11" tabindex="-1"></a>                                                                                               (high_income<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> </span>
<span id="cb103-12"><a href="regression-and-gradient-descent.html#cb103-12" tabindex="-1"></a>                                                                                               (insurance<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> </span>
<span id="cb103-13"><a href="regression-and-gradient-descent.html#cb103-13" tabindex="-1"></a>                                                                                               (stress_band)])<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>fraction with good health: 0.34948
fraction with good health among no school, low income, and uninsured: 0.0353</code></pre>
<p>The true <span class="math inline">\(\theta\)</span> values are <span class="math inline">\([\theta_0=0.3, \theta_1=0.1, \theta_2=0.2, \theta_3=0.05, \theta_4=-1]\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>0.3 is the intercept coefficient, representing people in no college,
low income, and uninsured group.</p></li>
<li><p>0.1 corresponds to college coefficient.</p></li>
<li><p>0.2 corresponds to high income coefficient.</p></li>
<li><p>0.05 corresponds to insurance coefficient.</p></li>
<li><p>-1 corresponds to stress coefficient.</p></li>
</ol>
<p>Note that 3.61 percent of people who have no schooling, are of low income, are uninsured and around the mean stress
index are in good health. This pertains to true <span class="math inline">\(\theta\)</span> of 0.3. Let’s convert this value
into probability using:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="regression-and-gradient-descent.html#cb105-1" tabindex="-1"></a>p_gh <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="fl">0.3</span> <span class="op">+</span> np.mean(stress[(college<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> (high_income<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> (insurance<span class="op">==</span><span class="dv">0</span>)])))</span>
<span id="cb105-2"><a href="regression-and-gradient-descent.html#cb105-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;the conversion of theta = 0.3 + mean stress value to prob: </span><span class="sc">{</span>p_gh<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>the conversion of theta = 0.3 + mean stress value to prob: 0.035 </code></pre>
<p>Notice that according to the DGP, around 3.55 percent of people in the population with no college,
low income, uninsured, and of the mean stress value are in good health.
This is close to what we have in our sample. Hence,
it is important to recognize that <span class="math inline">\(\theta\)</span> values are coefficients and in the case of logistic
regression; they are different from probabilities.</p>
<p><strong>Gradient Descent</strong></p>
<p>Let’s move on to the gradient descent and its usage in estimating <span class="math inline">\(\theta\)</span>.</p>
<p>Simply put, gradient is a vector of the partial derivatives of the loss function with respect to each
<span class="math inline">\(\theta\)</span> stacked together.</p>
<p><span class="math display">\[
\text{gradient} = \begin{bmatrix} \frac{\partial C}{\partial \theta_0} \\ \frac{\partial C}{\partial \theta_1} \\ \frac{\partial C}{\partial \theta_2} \\ \frac{\partial C}{\partial \theta_3} \end{bmatrix}
\]</span></p>
<p>Before we get to the gradient of the logistic kind, let’s stack the loss function using matrices:</p>
<p><span class="math display">\[C_{\theta} = -\frac{1}{n} [Y^{t} \log(p) + (1-Y^{t}) \log(1- p)]\]</span></p>
<p>Replacing <span class="math inline">\(p= \sigma(\theta X)\)</span>, we have:</p>
<p><span class="math display">\[C_{\theta} = -\frac{1}{n} [Y^{t} \log(\sigma(X\theta)) + (1-Y^{t}) \log(1-\sigma(X\theta)]\]</span></p>
<p><span class="math inline">\(C\)</span> will be a scalar.
Next, get <span class="math inline">\(\frac{\partial C}{\partial \theta}\)</span>.</p>
<p>But first, here are the dimensions of terms in the RHS:</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(X: (n\times 5)\)</span></p></li>
<li><p><span class="math inline">\(Y^{T}: (1\times n)\)</span></p></li>
<li><p><span class="math inline">\(\theta : (5\times 1)\)</span></p></li>
<li><p><span class="math inline">\(X\theta : (n\times 1)\)</span></p></li>
<li><p><span class="math inline">\(Y^{t} \log(\sigma(X\theta)): scalar\)</span></p></li>
</ol>
<p>Taking the partial derivative of the newly formatted cost function <span class="math inline">\(C\)</span> with
respect to <span class="math inline">\(\theta\)</span>, you get the gradient vector
as follows:</p>
<p><span class="math inline">\(\frac{\partial C}{\partial \theta} = \frac{1}{n} X^{T}(\sigma(X \theta) - Y)\)</span>.</p>
<p>where, <span class="math inline">\(X^{T}\)</span> is a <span class="math inline">\(5\times n\)</span> matrix and <span class="math inline">\((X^{T}\sigma(X \theta) - Y)\)</span> is a <span class="math inline">\(n \times 1\)</span> matrix.
I solved for the partial using
the brute force chain rule. One thing to note while solving is a small trick below:</p>
<p><span class="math inline">\(\frac{exp(\theta X)}{1 + exp(\theta X)} = \frac{1 + exp(\theta X) -1}{1 + exp(\theta X)}\)</span>. This results to:
<span class="math inline">\(1 - \frac{1}{1 + exp(\theta X)} = 1 - \sigma(\theta X)\)</span>.</p>
<p>This is getting into minute little details. You can escape this
and just take the word for the gradient or you could try it all out. Upto you!</p>
<p>Now that we are through with all this, the gradient descent algorithm is straight forward.</p>
<p><strong>Gradient Descent Algorithm</strong></p>
<ol style="list-style-type: decimal">
<li><p>Initialize the <span class="math inline">\(\theta_{gd}\)</span> values. I’ve used values from the normal distribution.</p></li>
<li><p>Initialize the learning rate – <span class="math inline">\(\eta\)</span> and the number of interations (epochs). We set <span class="math inline">\(\eta = 0.5\)</span> and epochs=50.</p></li>
<li><p>Compute the gradient. Call this <span class="math inline">\(gd_i\)</span>.</p></li>
<li><p>Adjust <span class="math inline">\(\theta\)</span> using the gradient and the learning rate as: <span class="math inline">\(\theta_{gd} = \theta_{gd} - \eta \times gd_i\)</span>. Note that
we have to move against the gradient; hence, the negative.</p></li>
<li><p>Iterate steps 3 and 4 for <span class="math inline">\(iter=epochs\)</span> number of times or until the algorithm converges.</p></li>
</ol>
<div class="sourceCode" id="cb107"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb107-1"><a href="regression-and-gradient-descent.html#cb107-1" tabindex="-1"></a><span class="co"># Gradient Descent </span></span>
<span id="cb107-2"><a href="regression-and-gradient-descent.html#cb107-2" tabindex="-1"></a>theta_gd <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">5</span>)    <span class="co"># initial theta values from the normal dist.</span></span>
<span id="cb107-3"><a href="regression-and-gradient-descent.html#cb107-3" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-15</span>                         <span class="co"># to prevent overflow coming from logit values close to 0.</span></span>
<span id="cb107-4"><a href="regression-and-gradient-descent.html#cb107-4" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">50</span>                           <span class="co"># number of iterations</span></span>
<span id="cb107-5"><a href="regression-and-gradient-descent.html#cb107-5" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.5</span>                               <span class="co"># learning rate</span></span>
<span id="cb107-6"><a href="regression-and-gradient-descent.html#cb107-6" tabindex="-1"></a>loss <span class="op">=</span> []</span>
<span id="cb107-7"><a href="regression-and-gradient-descent.html#cb107-7" tabindex="-1"></a></span>
<span id="cb107-8"><a href="regression-and-gradient-descent.html#cb107-8" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb107-9"><a href="regression-and-gradient-descent.html#cb107-9" tabindex="-1"></a></span>
<span id="cb107-10"><a href="regression-and-gradient-descent.html#cb107-10" tabindex="-1"></a>    z <span class="op">=</span> np.clip(X <span class="op">@</span> theta_gd.reshape((<span class="dv">5</span>, <span class="dv">1</span>)), <span class="op">-</span><span class="dv">500</span>, <span class="dv">500</span>)</span>
<span id="cb107-11"><a href="regression-and-gradient-descent.html#cb107-11" tabindex="-1"></a>    gradient_i <span class="op">=</span> ((sigma(z) <span class="op">-</span> health.reshape((n, <span class="dv">1</span>))).transpose() <span class="op">@</span> X) <span class="op">/</span> n <span class="co"># caculate the gradient</span></span>
<span id="cb107-12"><a href="regression-and-gradient-descent.html#cb107-12" tabindex="-1"></a>    theta_gd <span class="op">=</span> theta_gd <span class="op">-</span> eta<span class="op">*</span>gradient_i                                   <span class="co"># adjust theta by moving opposite to the gradient</span></span>
<span id="cb107-13"><a href="regression-and-gradient-descent.html#cb107-13" tabindex="-1"></a>    loss_i <span class="op">=</span> np.mean(health<span class="op">*</span>(<span class="op">-</span>np.log(sigma(z)<span class="op">+</span>epsilon).ravel()) <span class="op">+</span>          <span class="co"># calculate loss</span></span>
<span id="cb107-14"><a href="regression-and-gradient-descent.html#cb107-14" tabindex="-1"></a>                    (<span class="dv">1</span><span class="op">-</span>health)<span class="op">*</span>(<span class="op">-</span>np.log(<span class="dv">1</span><span class="op">-</span>sigma(z)<span class="op">+</span>epsilon).ravel()))</span>
<span id="cb107-15"><a href="regression-and-gradient-descent.html#cb107-15" tabindex="-1"></a>    loss.append(loss_i)                                                     <span class="co"># append loss</span></span>
<span id="cb107-16"><a href="regression-and-gradient-descent.html#cb107-16" tabindex="-1"></a></span>
<span id="cb107-17"><a href="regression-and-gradient-descent.html#cb107-17" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;theta estimates from gradient descent: </span><span class="sc">{</span>theta_gd<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb107-18"><a href="regression-and-gradient-descent.html#cb107-18" tabindex="-1"></a></span>
<span id="cb107-19"><a href="regression-and-gradient-descent.html#cb107-19" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb107-20"><a href="regression-and-gradient-descent.html#cb107-20" tabindex="-1"></a>plt.plot(np.linspace(<span class="dv">1</span>, epochs, epochs), np.array(loss).ravel())</span>
<span id="cb107-21"><a href="regression-and-gradient-descent.html#cb107-21" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;epochs&#39;</span>)</span>
<span id="cb107-22"><a href="regression-and-gradient-descent.html#cb107-22" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;cross-entropy loss&#39;</span>)</span>
<span id="cb107-23"><a href="regression-and-gradient-descent.html#cb107-23" tabindex="-1"></a>plt.title(<span class="st">&quot;Loss with respect to iterations&quot;</span>)</span>
<span id="cb107-24"><a href="regression-and-gradient-descent.html#cb107-24" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>theta estimates from gradient descent: [[-0.296  -0.2989  1.1594 -0.1458 -0.8166]] 
 </code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_9_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>We’ve now estimated the <span class="math inline">\(\theta\)</span> using gradient descent. Let’s verify our results using the
in-built library in sklearn that estimates the Logistic Regression.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="regression-and-gradient-descent.html#cb109-1" tabindex="-1"></a><span class="co"># compare estimates from sklearn</span></span>
<span id="cb109-2"><a href="regression-and-gradient-descent.html#cb109-2" tabindex="-1"></a>mod <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span>epochs, fit_intercept<span class="op">=</span><span class="va">False</span>, C<span class="op">=</span>np.inf)</span>
<span id="cb109-3"><a href="regression-and-gradient-descent.html#cb109-3" tabindex="-1"></a>mod_fit <span class="op">=</span> mod.fit(X, health)</span>
<span id="cb109-4"><a href="regression-and-gradient-descent.html#cb109-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Estimates from sklearn: </span><span class="sc">{</span>mod_fit<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb109-5"><a href="regression-and-gradient-descent.html#cb109-5" tabindex="-1"></a></span>
<span id="cb109-6"><a href="regression-and-gradient-descent.html#cb109-6" tabindex="-1"></a>results <span class="op">=</span> {</span>
<span id="cb109-7"><a href="regression-and-gradient-descent.html#cb109-7" tabindex="-1"></a>            <span class="st">&quot;GD&quot;</span>: theta_gd.ravel().<span class="bu">round</span>(<span class="dv">3</span>),</span>
<span id="cb109-8"><a href="regression-and-gradient-descent.html#cb109-8" tabindex="-1"></a>            <span class="st">&quot;SK&quot;</span>: mod_fit.coef_.ravel().<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb109-9"><a href="regression-and-gradient-descent.html#cb109-9" tabindex="-1"></a>}</span>
<span id="cb109-10"><a href="regression-and-gradient-descent.html#cb109-10" tabindex="-1"></a></span>
<span id="cb109-11"><a href="regression-and-gradient-descent.html#cb109-11" tabindex="-1"></a>pd.DataFrame(results)</span></code></pre></div>
<pre><code>Estimates from sklearn: [[ 0.32179444  0.08030071  0.20651284  0.01809187 -0.99105091]]


/home/vinish/Dropbox/Machine Learning/myenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1170: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters
  warnings.warn(</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
GD
</th>
<th>
SK
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
-0.296
</td>
<td>
0.322
</td>
</tr>
<tr>
<th>
1
</th>
<td>
-0.299
</td>
<td>
0.080
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1.159
</td>
<td>
0.207
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-0.146
</td>
<td>
0.018
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-0.817
</td>
<td>
-0.991
</td>
</tr>
</tbody>
</table>
</div>
<p>Let’s estimate the model using the LPM – note that this is a wrong functional form at use.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="regression-and-gradient-descent.html#cb111-1" tabindex="-1"></a><span class="co"># linear regression</span></span>
<span id="cb111-2"><a href="regression-and-gradient-descent.html#cb111-2" tabindex="-1"></a>mod_linear <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">False</span>) <span class="co"># we dont want to double fit the intercept; X already contains it </span></span>
<span id="cb111-3"><a href="regression-and-gradient-descent.html#cb111-3" tabindex="-1"></a>mod_linear <span class="op">=</span> mod_linear.fit(X, health)</span>
<span id="cb111-4"><a href="regression-and-gradient-descent.html#cb111-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Estimates from linear reg: </span><span class="sc">{</span>mod_linear<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>Estimates from linear reg: [ 0.44695463  0.01138116  0.16453216  0.0015579  -0.10234894]</code></pre>
<p>The estimates from the gradient descent and sklearn are virtually similar. Note that the interpretation of <span class="math inline">\(\theta\)</span> estimates
are not equivalent to marginal effects as they are in the LPM set up. Recall, in the case of logistic regression:
<span class="math inline">\(\hat{p} = \frac{1}{(1 + exp(-\theta X))}\)</span>. Hence, we need to translate <span class="math inline">\(\theta\)</span> into marginal effects
before comparing them with LPM’s estimates. Calculation of marginal effect needs to be with respect to a given benchmark.
There is nothing wrong with creating a benchmark.
Say, we consider person A: with no college, low income, uninsured, and stress level around the mean (for the group with
no college, uninsured, and low income) as this benchmark person and the marginal effects
are computed with respect to this person.</p>
<p>The following code translates <span class="math inline">\(\theta\)</span> into marginal effect.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="regression-and-gradient-descent.html#cb113-1" tabindex="-1"></a><span class="kw">def</span> fun_me(theta_vals, person):</span>
<span id="cb113-2"><a href="regression-and-gradient-descent.html#cb113-2" tabindex="-1"></a>    logit <span class="op">=</span> (theta_vals <span class="op">@</span> person).ravel()</span>
<span id="cb113-3"><a href="regression-and-gradient-descent.html#cb113-3" tabindex="-1"></a>    p <span class="op">=</span> sigma(logit)</span>
<span id="cb113-4"><a href="regression-and-gradient-descent.html#cb113-4" tabindex="-1"></a>    <span class="cf">return</span> p</span>
<span id="cb113-5"><a href="regression-and-gradient-descent.html#cb113-5" tabindex="-1"></a></span>
<span id="cb113-6"><a href="regression-and-gradient-descent.html#cb113-6" tabindex="-1"></a><span class="co"># create person A: without college, low income, and uninsured   </span></span>
<span id="cb113-7"><a href="regression-and-gradient-descent.html#cb113-7" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: This will be our benchmark person.</span></span>
<span id="cb113-8"><a href="regression-and-gradient-descent.html#cb113-8" tabindex="-1"></a>person_A <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, mean_stress_baseline]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb113-9"><a href="regression-and-gradient-descent.html#cb113-9" tabindex="-1"></a>prob_health_A <span class="op">=</span> fun_me(theta_gd, person_A)</span>
<span id="cb113-10"><a href="regression-and-gradient-descent.html#cb113-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The probability that person A is in good health is:</span><span class="sc">{</span>prob_health_A<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb113-11"><a href="regression-and-gradient-descent.html#cb113-11" tabindex="-1"></a></span>
<span id="cb113-12"><a href="regression-and-gradient-descent.html#cb113-12" tabindex="-1"></a><span class="co"># Person B: with college but low income and uninsured</span></span>
<span id="cb113-13"><a href="regression-and-gradient-descent.html#cb113-13" tabindex="-1"></a>person_B <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, mean_stress_baseline]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb113-14"><a href="regression-and-gradient-descent.html#cb113-14" tabindex="-1"></a>prob_health_B <span class="op">=</span> fun_me(theta_gd, person_B)</span>
<span id="cb113-15"><a href="regression-and-gradient-descent.html#cb113-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The probability that person B is in good health is: </span><span class="sc">{</span>prob_health_B<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb113-16"><a href="regression-and-gradient-descent.html#cb113-16" tabindex="-1"></a></span>
<span id="cb113-17"><a href="regression-and-gradient-descent.html#cb113-17" tabindex="-1"></a><span class="co"># Person C: with high income but without college and uninsured</span></span>
<span id="cb113-18"><a href="regression-and-gradient-descent.html#cb113-18" tabindex="-1"></a>person_C <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, mean_stress_baseline]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb113-19"><a href="regression-and-gradient-descent.html#cb113-19" tabindex="-1"></a>prob_health_C <span class="op">=</span> fun_me(theta_gd, person_C)</span>
<span id="cb113-20"><a href="regression-and-gradient-descent.html#cb113-20" tabindex="-1"></a><span class="bu">print</span>(prob_health_C)</span>
<span id="cb113-21"><a href="regression-and-gradient-descent.html#cb113-21" tabindex="-1"></a></span>
<span id="cb113-22"><a href="regression-and-gradient-descent.html#cb113-22" tabindex="-1"></a><span class="co"># Person D: with insurance but without college and low income</span></span>
<span id="cb113-23"><a href="regression-and-gradient-descent.html#cb113-23" tabindex="-1"></a>person_D <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, mean_stress_baseline]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb113-24"><a href="regression-and-gradient-descent.html#cb113-24" tabindex="-1"></a>prob_health_D <span class="op">=</span> fun_me(theta_gd, person_D)</span>
<span id="cb113-25"><a href="regression-and-gradient-descent.html#cb113-25" tabindex="-1"></a><span class="bu">print</span>(prob_health_D)</span>
<span id="cb113-26"><a href="regression-and-gradient-descent.html#cb113-26" tabindex="-1"></a></span>
<span id="cb113-27"><a href="regression-and-gradient-descent.html#cb113-27" tabindex="-1"></a><span class="co"># Person E: Same as Person A but one unit increase in stress for marginal effects</span></span>
<span id="cb113-28"><a href="regression-and-gradient-descent.html#cb113-28" tabindex="-1"></a>person_E <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, mean_stress_baseline <span class="op">+</span> <span class="dv">1</span>]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb113-29"><a href="regression-and-gradient-descent.html#cb113-29" tabindex="-1"></a>prob_health_E <span class="op">=</span> fun_me(theta_gd, person_E)</span>
<span id="cb113-30"><a href="regression-and-gradient-descent.html#cb113-30" tabindex="-1"></a><span class="bu">print</span>(prob_health_E)</span></code></pre></div>
<pre><code>The probability that person A is in good health is:[0.03732477] 

The probability that person B is in good health is: [0.02795158] 

[0.11001232]
[0.0324238]
[0.01684625]</code></pre>
<p>Since we are using Person A as the benchmark, we can compute marginal probabilities simply by
substracting probabilities.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="regression-and-gradient-descent.html#cb115-1" tabindex="-1"></a>me_B_A <span class="op">=</span> prob_health_B <span class="op">-</span> prob_health_A </span>
<span id="cb115-2"><a href="regression-and-gradient-descent.html#cb115-2" tabindex="-1"></a>me_C_A <span class="op">=</span> prob_health_C <span class="op">-</span> prob_health_A </span>
<span id="cb115-3"><a href="regression-and-gradient-descent.html#cb115-3" tabindex="-1"></a>me_D_A <span class="op">=</span> prob_health_D <span class="op">-</span> prob_health_A</span>
<span id="cb115-4"><a href="regression-and-gradient-descent.html#cb115-4" tabindex="-1"></a>me_E_A <span class="op">=</span> prob_health_E <span class="op">-</span> prob_health_A</span>
<span id="cb115-5"><a href="regression-and-gradient-descent.html#cb115-5" tabindex="-1"></a></span>
<span id="cb115-6"><a href="regression-and-gradient-descent.html#cb115-6" tabindex="-1"></a>me <span class="op">=</span> np.array([prob_health_A, me_B_A, me_C_A, me_D_A, me_E_A])</span>
<span id="cb115-7"><a href="regression-and-gradient-descent.html#cb115-7" tabindex="-1"></a></span>
<span id="cb115-8"><a href="regression-and-gradient-descent.html#cb115-8" tabindex="-1"></a>results_me_lpm <span class="op">=</span> {</span>
<span id="cb115-9"><a href="regression-and-gradient-descent.html#cb115-9" tabindex="-1"></a>                    <span class="st">&quot;ME:logistic&quot;</span>: me.ravel().<span class="bu">round</span>(<span class="dv">3</span>),</span>
<span id="cb115-10"><a href="regression-and-gradient-descent.html#cb115-10" tabindex="-1"></a>                    <span class="st">&quot;ME:LPM&quot;</span>: mod_linear.coef_.ravel().<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb115-11"><a href="regression-and-gradient-descent.html#cb115-11" tabindex="-1"></a>}</span>
<span id="cb115-12"><a href="regression-and-gradient-descent.html#cb115-12" tabindex="-1"></a></span>
<span id="cb115-13"><a href="regression-and-gradient-descent.html#cb115-13" tabindex="-1"></a>pd.DataFrame(results_me_lpm)</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
ME:logistic
</th>
<th>
ME:LPM
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.037
</td>
<td>
0.447
</td>
</tr>
<tr>
<th>
1
</th>
<td>
-0.009
</td>
<td>
0.011
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.073
</td>
<td>
0.165
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-0.005
</td>
<td>
0.002
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-0.020
</td>
<td>
-0.102
</td>
</tr>
</tbody>
</table>
</div>
<p>The marginal effects from LPM and logistic regression are shown in the table.
Let’s take a look at predicted probabilities from both LPM and logistic regression models:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="regression-and-gradient-descent.html#cb116-1" tabindex="-1"></a><span class="co"># prediction from LPM</span></span>
<span id="cb116-2"><a href="regression-and-gradient-descent.html#cb116-2" tabindex="-1"></a>prob_linear <span class="op">=</span> mod_linear.predict(X) <span class="co"># LPM directly gives probabilites</span></span>
<span id="cb116-3"><a href="regression-and-gradient-descent.html#cb116-3" tabindex="-1"></a><span class="bu">print</span>(prob_linear) </span>
<span id="cb116-4"><a href="regression-and-gradient-descent.html#cb116-4" tabindex="-1"></a></span>
<span id="cb116-5"><a href="regression-and-gradient-descent.html#cb116-5" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb116-6"><a href="regression-and-gradient-descent.html#cb116-6" tabindex="-1"></a>plt.hist(prob_linear, bins <span class="op">=</span> <span class="dv">30</span>, color<span class="op">=</span><span class="st">&quot;red&quot;</span>, edgecolor<span class="op">=</span><span class="st">&quot;black&quot;</span>, alpha<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb116-7"><a href="regression-and-gradient-descent.html#cb116-7" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;probability&#39;</span>)</span>
<span id="cb116-8"><a href="regression-and-gradient-descent.html#cb116-8" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;frequency&#39;</span>)</span>
<span id="cb116-9"><a href="regression-and-gradient-descent.html#cb116-9" tabindex="-1"></a>plt.title(<span class="st">&#39;Predicted probabilities from LPM&#39;</span>)</span>
<span id="cb116-10"><a href="regression-and-gradient-descent.html#cb116-10" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">&quot;--&quot;</span>)</span>
<span id="cb116-11"><a href="regression-and-gradient-descent.html#cb116-11" tabindex="-1"></a>plt.show()</span>
<span id="cb116-12"><a href="regression-and-gradient-descent.html#cb116-12" tabindex="-1"></a></span>
<span id="cb116-13"><a href="regression-and-gradient-descent.html#cb116-13" tabindex="-1"></a><span class="co"># predictions from logistic</span></span>
<span id="cb116-14"><a href="regression-and-gradient-descent.html#cb116-14" tabindex="-1"></a>prob_logit_predict <span class="op">=</span> mod_fit.predict_proba(X) <span class="co"># need to call .predict_proba() to get probabilities from logistic model</span></span>
<span id="cb116-15"><a href="regression-and-gradient-descent.html#cb116-15" tabindex="-1"></a></span>
<span id="cb116-16"><a href="regression-and-gradient-descent.html#cb116-16" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb116-17"><a href="regression-and-gradient-descent.html#cb116-17" tabindex="-1"></a>plt.hist(prob_logit_predict[:,<span class="dv">1</span>], bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&quot;steelblue&quot;</span>, edgecolor<span class="op">=</span><span class="st">&quot;black&quot;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb116-18"><a href="regression-and-gradient-descent.html#cb116-18" tabindex="-1"></a><span class="co">#plt.hist(prob_logit, bins=30, color=&quot;red&quot;, edgecolor=&quot;black&quot;, alpha=0.3)</span></span>
<span id="cb116-19"><a href="regression-and-gradient-descent.html#cb116-19" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;probability&#39;</span>)</span>
<span id="cb116-20"><a href="regression-and-gradient-descent.html#cb116-20" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;frequency&#39;</span>)</span>
<span id="cb116-21"><a href="regression-and-gradient-descent.html#cb116-21" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb116-22"><a href="regression-and-gradient-descent.html#cb116-22" tabindex="-1"></a>plt.title(<span class="st">&#39;Predicted probabilities from Logistic Regression&#39;</span>)</span>
<span id="cb116-23"><a href="regression-and-gradient-descent.html#cb116-23" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>[ 0.29927004  0.64684742  0.74602907 ... -0.02085397  0.80991622
  0.74443065]</code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_19_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_19_2.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>We see that the predicted probabilities from LPM are negative (this can’t happen), whereas those from the logistic
model closely mimic the actual probabilities. Hence, if the goal is to attain probabilities then logistic regression
is clearly better than LPM.</p>
<p><strong>LPM vs Logistic Regression</strong></p>
<p>From a practitioner’s perspective, one can get by using LPM if the goal is to infer causality alone and you
aren’t concerned about predicting probabilities. It is simple, easy to interpret, and will require low computational power.
It does mean that you should, but you can get by. However, if the goal is to predict, say probability of the binary outcome,
then LPM is a no-go.</p>
<p>In the world of causality, the importance of estimating the probability of someone being treated (vs untreated) cannot be overstated.
We know this as propensity scores. Logistic regression can be a good starting model while estimating propensity scores.</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="an-exercise.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="causal-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  },
  "toolbar": {
    "position": "static"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
