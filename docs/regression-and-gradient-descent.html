<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Regression and Gradient Descent | Causal Inference</title>
  <meta name="description" content="4 Regression and Gradient Descent | Causal Inference" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Regression and Gradient Descent | Causal Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Regression and Gradient Descent | Causal Inference" />
  
  
  

<meta name="author" content="Vinish Shrestha" />


<meta name="date" content="2026-02-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="an-exercise.html"/>
<link rel="next" href="standard-errors-1.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="assets/kePrint-0.0.1/kePrint.js"></script>
<link href="assets/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="a-lab-experiment.html"><a href="a-lab-experiment.html"><i class="fa fa-check"></i><b>2.1</b> A lab experiment</a></li>
<li class="chapter" data-level="2.2" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.2</b> Challenges</a></li>
<li class="chapter" data-level="2.3" data-path="dag-directed-acyclic-graph.html"><a href="dag-directed-acyclic-graph.html"><i class="fa fa-check"></i><b>2.3</b> DAG (Directed Acyclic Graph)</a></li>
<li class="chapter" data-level="2.4" data-path="a-simulated-dgp.html"><a href="a-simulated-dgp.html"><i class="fa fa-check"></i><b>2.4</b> A simulated DGP</a></li>
<li class="chapter" data-level="2.5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="why-regression.html"><a href="why-regression.html"><i class="fa fa-check"></i><b>3</b> Why Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-best-fit-line.html"><a href="the-best-fit-line.html"><i class="fa fa-check"></i><b>3.1</b> The best-fit line</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression-specification.html"><a href="linear-regression-specification.html"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Specification</a></li>
<li class="chapter" data-level="3.3" data-path="law-of-iterated-expectation.html"><a href="law-of-iterated-expectation.html"><i class="fa fa-check"></i><b>3.3</b> Law of iterated expectation</a></li>
<li class="chapter" data-level="3.4" data-path="error-term.html"><a href="error-term.html"><i class="fa fa-check"></i><b>3.4</b> Error term</a></li>
<li class="chapter" data-level="3.5" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>3.5</b> Decomposition</a></li>
<li class="chapter" data-level="3.6" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>3.6</b> Estimation</a></li>
<li class="chapter" data-level="3.7" data-path="running-a-regression.html"><a href="running-a-regression.html"><i class="fa fa-check"></i><b>3.7</b> Running a regression</a></li>
<li class="chapter" data-level="3.8" data-path="standard-errors.html"><a href="standard-errors.html"><i class="fa fa-check"></i><b>3.8</b> Standard Errors</a></li>
<li class="chapter" data-level="3.9" data-path="an-exercise.html"><a href="an-exercise.html"><i class="fa fa-check"></i><b>3.9</b> An exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression-and-gradient-descent.html"><a href="regression-and-gradient-descent.html"><i class="fa fa-check"></i><b>4</b> Regression and Gradient Descent</a></li>
<li class="chapter" data-level="5" data-path="standard-errors-1.html"><a href="standard-errors-1.html"><i class="fa fa-check"></i><b>5</b> Standard Errors</a></li>
<li class="chapter" data-level="6" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>6</b> Logistic regression</a></li>
<li class="chapter" data-level="7" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>7</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="7.1" data-path="potential-outcome-framework-neyman-rubin-causal-model.html"><a href="potential-outcome-framework-neyman-rubin-causal-model.html"><i class="fa fa-check"></i><b>7.1</b> Potential Outcome Framework: Neyman-Rubin Causal Model</a></li>
<li class="chapter" data-level="7.2" data-path="average-treatment-effect-ate.html"><a href="average-treatment-effect-ate.html"><i class="fa fa-check"></i><b>7.2</b> Average treatment effect (ATE)</a></li>
<li class="chapter" data-level="7.3" data-path="rct.html"><a href="rct.html"><i class="fa fa-check"></i><b>7.3</b> RCT</a></li>
<li class="chapter" data-level="7.4" data-path="average-treatment-effect-on-the-treated-att.html"><a href="average-treatment-effect-on-the-treated-att.html"><i class="fa fa-check"></i><b>7.4</b> Average treatment effect on the treated (ATT)</a></li>
<li class="chapter" data-level="7.5" data-path="an-estimation-example.html"><a href="an-estimation-example.html"><i class="fa fa-check"></i><b>7.5</b> An estimation example</a></li>
<li class="chapter" data-level="7.6" data-path="unconfoundedness-assumption.html"><a href="unconfoundedness-assumption.html"><i class="fa fa-check"></i><b>7.6</b> Unconfoundedness assumption</a></li>
<li class="chapter" data-level="7.7" data-path="discussion-1.html"><a href="discussion-1.html"><i class="fa fa-check"></i><b>7.7</b> Discussion</a></li>
<li class="chapter" data-level="7.8" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>7.8</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ipw-and-aipw.html"><a href="ipw-and-aipw.html"><i class="fa fa-check"></i><b>8</b> IPW and AIPW</a>
<ul>
<li class="chapter" data-level="8.1" data-path="a-simple-example.html"><a href="a-simple-example.html"><i class="fa fa-check"></i><b>8.1</b> A simple example</a></li>
<li class="chapter" data-level="8.2" data-path="aggregated-estimator.html"><a href="aggregated-estimator.html"><i class="fa fa-check"></i><b>8.2</b> Aggregated Estimator</a></li>
<li class="chapter" data-level="8.3" data-path="propensity-score.html"><a href="propensity-score.html"><i class="fa fa-check"></i><b>8.3</b> Propensity score</a></li>
<li class="chapter" data-level="8.4" data-path="estimation-of-propensity-score.html"><a href="estimation-of-propensity-score.html"><i class="fa fa-check"></i><b>8.4</b> Estimation of propensity score</a></li>
<li class="chapter" data-level="8.5" data-path="using-cross-fitting-to-predict-propensity-score.html"><a href="using-cross-fitting-to-predict-propensity-score.html"><i class="fa fa-check"></i><b>8.5</b> Using cross-fitting to predict propensity score</a></li>
<li class="chapter" data-level="8.6" data-path="propensity-score-stratification.html"><a href="propensity-score-stratification.html"><i class="fa fa-check"></i><b>8.6</b> Propensity score stratification</a></li>
<li class="chapter" data-level="8.7" data-path="inverse-probability-weighting-ipw.html"><a href="inverse-probability-weighting-ipw.html"><i class="fa fa-check"></i><b>8.7</b> Inverse Probability Weighting (IPW)</a></li>
<li class="chapter" data-level="8.8" data-path="comparing-ipw-with-aggregated-estimate.html"><a href="comparing-ipw-with-aggregated-estimate.html"><i class="fa fa-check"></i><b>8.8</b> Comparing IPW with Aggregated Estimate</a></li>
<li class="chapter" data-level="8.9" data-path="aipw-and-estimation.html"><a href="aipw-and-estimation.html"><i class="fa fa-check"></i><b>8.9</b> AIPW and Estimation</a></li>
<li class="chapter" data-level="8.10" data-path="assessing-balance.html"><a href="assessing-balance.html"><i class="fa fa-check"></i><b>8.10</b> Assessing Balance</a></li>
<li class="chapter" data-level="8.11" data-path="cross-fitting.html"><a href="cross-fitting.html"><i class="fa fa-check"></i><b>8.11</b> Cross-fitting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="difference-in-differences.html"><a href="difference-in-differences.html"><i class="fa fa-check"></i><b>9</b> Difference in Differences</a>
<ul>
<li class="chapter" data-level="9.1" data-path="a-quick-introduction.html"><a href="a-quick-introduction.html"><i class="fa fa-check"></i><b>9.1</b> A Quick Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="set-up.html"><a href="set-up.html"><i class="fa fa-check"></i><b>9.2</b> Set up</a></li>
<li class="chapter" data-level="9.3" data-path="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><a href="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><i class="fa fa-check"></i><b>9.3</b> An example: Evaluating the impact of Medicaid expansion on uninsured rate</a></li>
<li class="chapter" data-level="9.4" data-path="naive-estimator.html"><a href="naive-estimator.html"><i class="fa fa-check"></i><b>9.4</b> Naive estimator</a></li>
<li class="chapter" data-level="9.5" data-path="canonical-difference-in-differences-framework.html"><a href="canonical-difference-in-differences-framework.html"><i class="fa fa-check"></i><b>9.5</b> Canonical Difference in Differences Framework</a></li>
<li class="chapter" data-level="9.6" data-path="did-in-multi-period-set-up.html"><a href="did-in-multi-period-set-up.html"><i class="fa fa-check"></i><b>9.6</b> DiD in multi-period set up</a></li>
<li class="chapter" data-level="9.7" data-path="conditional-parallel-trend-assumption.html"><a href="conditional-parallel-trend-assumption.html"><i class="fa fa-check"></i><b>9.7</b> Conditional Parallel Trend Assumption</a></li>
<li class="chapter" data-level="9.8" data-path="some-concerns-with-controls.html"><a href="some-concerns-with-controls.html"><i class="fa fa-check"></i><b>9.8</b> Some concerns with controls</a></li>
<li class="chapter" data-level="9.9" data-path="the-2-times-2-difference-in-differences-estimate.html"><a href="the-2-times-2-difference-in-differences-estimate.html"><i class="fa fa-check"></i><b>9.9</b> The <span class="math inline">\(2 \times 2\)</span> Difference-in-Differences Estimate</a></li>
<li class="chapter" data-level="9.10" data-path="event-study-model.html"><a href="event-study-model.html"><i class="fa fa-check"></i><b>9.10</b> Event study model</a></li>
<li class="chapter" data-level="9.11" data-path="two-way-fixed-effect-twfe-revisited.html"><a href="two-way-fixed-effect-twfe-revisited.html"><i class="fa fa-check"></i><b>9.11</b> Two way fixed effect (TWFE) Revisited</a></li>
<li class="chapter" data-level="9.12" data-path="various-ways-of-estimation.html"><a href="various-ways-of-estimation.html"><i class="fa fa-check"></i><b>9.12</b> Various ways of estimation</a></li>
<li class="chapter" data-level="9.13" data-path="multi-period-multi-group-and-variation-in-treatment-timing.html"><a href="multi-period-multi-group-and-variation-in-treatment-timing.html"><i class="fa fa-check"></i><b>9.13</b> Multi Period, Multi Group and Variation in Treatment Timing</a></li>
<li class="chapter" data-level="9.14" data-path="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><a href="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><i class="fa fa-check"></i><b>9.14</b> Problem with TWFE in Multiple Group with Treatment Timing Variation</a></li>
<li class="chapter" data-level="9.15" data-path="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><a href="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><i class="fa fa-check"></i><b>9.15</b> What is TWFE Estimating when there is Treatment Timing Variation?</a></li>
<li class="chapter" data-level="9.16" data-path="assumptions-governing-twfedd-estimate.html"><a href="assumptions-governing-twfedd-estimate.html"><i class="fa fa-check"></i><b>9.16</b> Assumptions governing TWFEDD estimate</a></li>
<li class="chapter" data-level="9.17" data-path="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><a href="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><i class="fa fa-check"></i><b>9.17</b> How Does Treatment Effect Heterogeneity in Time Affect TWFE?</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="causal-forest.html"><a href="causal-forest.html"><i class="fa fa-check"></i><b>10</b> Causal Forest</a>
<ul>
<li class="chapter" data-level="10.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="summary-of-grf.html"><a href="summary-of-grf.html"><i class="fa fa-check"></i><b>10.2</b> Summary of GRF</a></li>
<li class="chapter" data-level="10.3" data-path="motivation-for-causal-forests.html"><a href="motivation-for-causal-forests.html"><i class="fa fa-check"></i><b>10.3</b> Motivation for Causal Forests</a></li>
<li class="chapter" data-level="10.4" data-path="causal-forest-1.html"><a href="causal-forest-1.html"><i class="fa fa-check"></i><b>10.4</b> Causal Forest</a></li>
<li class="chapter" data-level="10.5" data-path="an-example-of-causal-forest.html"><a href="an-example-of-causal-forest.html"><i class="fa fa-check"></i><b>10.5</b> An example of causal forest</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>11</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="11.1" data-path="some-ways-to-estimate-cate.html"><a href="some-ways-to-estimate-cate.html"><i class="fa fa-check"></i><b>11.1</b> Some ways to estimate CATE</a></li>
<li class="chapter" data-level="11.2" data-path="estimation-1.html"><a href="estimation-1.html"><i class="fa fa-check"></i><b>11.2</b> Estimation</a></li>
<li class="chapter" data-level="11.3" data-path="some-remarks-and-questions.html"><a href="some-remarks-and-questions.html"><i class="fa fa-check"></i><b>11.3</b> Some Remarks and Questions</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-and-gradient-descent" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Regression and Gradient Descent<a href="regression-and-gradient-descent.html#regression-and-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Course:</strong> Causal Inference</p>
<p><strong>Topic:</strong> Estimation of linear regression model with and without closed-form solutions</p>
<style>
.jp-Notebook,
.jp-NotebookPanel-notebook {
    max-width: 900px;
    margin: auto;
}

.jp-Cell {
    padding-left: 40px;
    padding-right: 40px;
}
</style>
<style>
@media print {
 body {
   margin: 1in;
 }
}
</style>
<p>Last time we took a look at the method of minimizing the sum of the square of residuals.
Today let’s take a look at two other ways of estimating a linear regression specification:
i) Normal equation method, and ii) Gradient descent. We’ll do these manually and compare our
results using python libraries to see whether we’ve done it correctly.</p>
<p>Lets first load the necessary libraries.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="regression-and-gradient-descent.html#cb56-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb56-2"><a href="regression-and-gradient-descent.html#cb56-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-3"><a href="regression-and-gradient-descent.html#cb56-3" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_regression</span>
<span id="cb56-4"><a href="regression-and-gradient-descent.html#cb56-4" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> add_dummy_feature</span>
<span id="cb56-5"><a href="regression-and-gradient-descent.html#cb56-5" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb56-6"><a href="regression-and-gradient-descent.html#cb56-6" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb56-7"><a href="regression-and-gradient-descent.html#cb56-7" tabindex="-1"></a>root_dir <span class="op">=</span> <span class="st">&quot;/home/vinish/Dropbox/Machine Learning&quot;</span></span></code></pre></div>
<p>sklearn is an open sourced library in python that is mainly built for predictive data analysis, which is
built on top of NumPy, SciPy, and matplotlib. You can get more information about this package on
<a href="https://scikit-learn.org/stable/index.html">sklearn</a>.</p>
<p>We’ll be using simulated data from sklearn’s module called “datasets” by using the make_regression() function. The true model has the following
attributes:<br />
i) 2 informative features (X)<br />
ii) 1 target (Y)<br />
iii) intercept with the coefficient of 10</p>
<p>Let’s use make_regression() to simulate our data.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="regression-and-gradient-descent.html#cb57-1" tabindex="-1"></a>X, y, coefficient <span class="op">=</span> make_regression(n_samples <span class="op">=</span> <span class="dv">1000</span>,</span>
<span id="cb57-2"><a href="regression-and-gradient-descent.html#cb57-2" tabindex="-1"></a>                       n_features <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb57-3"><a href="regression-and-gradient-descent.html#cb57-3" tabindex="-1"></a>                       n_informative <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb57-4"><a href="regression-and-gradient-descent.html#cb57-4" tabindex="-1"></a>                       n_targets <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb57-5"><a href="regression-and-gradient-descent.html#cb57-5" tabindex="-1"></a>                       noise <span class="op">=</span> <span class="dv">1</span>, </span>
<span id="cb57-6"><a href="regression-and-gradient-descent.html#cb57-6" tabindex="-1"></a>                       bias <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb57-7"><a href="regression-and-gradient-descent.html#cb57-7" tabindex="-1"></a>                       random_state <span class="op">=</span> <span class="dv">42</span>, </span>
<span id="cb57-8"><a href="regression-and-gradient-descent.html#cb57-8" tabindex="-1"></a>                       coef <span class="op">=</span> <span class="va">True</span></span>
<span id="cb57-9"><a href="regression-and-gradient-descent.html#cb57-9" tabindex="-1"></a></span>
<span id="cb57-10"><a href="regression-and-gradient-descent.html#cb57-10" tabindex="-1"></a>)</span></code></pre></div>
<p>Note that I’ve set coef = True. This will return the model parameters and the intercept is set at 10.</p>
<p>We take a look at the first five rows.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="regression-and-gradient-descent.html#cb58-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;print features: </span><span class="sc">{</span>X[<span class="dv">0</span>:<span class="dv">5</span>, :]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb58-2"><a href="regression-and-gradient-descent.html#cb58-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;target: </span><span class="sc">{</span>y[<span class="dv">0</span>:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>print features: [[-0.16711808  0.14671369]
 [-0.02090159  0.11732738]
 [ 0.15041891  0.364961  ]
 [ 0.55560447  0.08958068]
 [ 0.05820872 -1.1429703 ]]
target: [ 3.24877735  8.66339401 19.45702327 31.55545159  3.92293402]</code></pre>
<p>And let’s print out the model parameters. Note these are the true coefficients that are used to generate data.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="regression-and-gradient-descent.html#cb60-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;coefficients: </span><span class="sc">{</span>coefficient<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>coefficients: [40.71064891  6.60098441]</code></pre>
<p>Plot the relationship between <span class="math inline">\(X1\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="regression-and-gradient-descent.html#cb62-1" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb62-2"><a href="regression-and-gradient-descent.html#cb62-2" tabindex="-1"></a>plt.scatter(X[:,<span class="dv">1</span>], y)</span>
<span id="cb62-3"><a href="regression-and-gradient-descent.html#cb62-3" tabindex="-1"></a>plt.show()</span>
<span id="cb62-4"><a href="regression-and-gradient-descent.html#cb62-4" tabindex="-1"></a><span class="co">#plt.savefig(root_dir + &quot;/Codes/Output/make_data_scatter.pdf&quot;)</span></span></code></pre></div>
<div class="float">
<img src="01_Regression_Gradient_Descent_files/01_Regression_Gradient_Descent_12_0.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Now, we are ready to discuss two methods. Let’s start with the Normal equation method.</p>
<p><strong>i. Normal Equation</strong></p>
<p>Consider the following regression model specification:</p>
<p><span class="math display">\[
Y_i = \alpha + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i
\]</span></p>
<p>The job is to estimate model parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span>.</p>
<p>Note that <span class="math inline">\(\epsilon_i\)</span> is the error term and we’d want to minimize some version of this. Let’s write out the
error as:</p>
<p><span class="math display">\[
\epsilon_i = Y_i - \alpha - \beta_1 X_{1i} -\beta_2 X_{2i}
\]</span></p>
<p>We know that the error term <span class="math inline">\(\epsilon\)</span> is a <span class="math inline">\(n \times 1\)</span> vector. We can obtain residuals by using estimates of model parameters. Of course, we don’t want to pick any parameters – the estimates should follow some objective.</p>
<p>One idea is to estimate the model parameters with an objective of minimizing the mean of the error. However, this is 0 by construction. So what we’d want to do instead is minimize the mean squared error.</p>
<p><span class="math display">\[
MSE(X, h_{\theta}) = \frac{1}{m} \sum_{i=1}^{m} (\theta^{T} x_i - y_i)^2
\]</span></p>
<p>From the equation above, we know that the MSE is just the mean of the sum of the squared errors. We can write the sum of the squared of errors using the matrix version as:</p>
<p><span class="math display">\[
SSE(X, \theta) = (y - X\theta)^T(y-X\theta)
\]</span></p>
<p>Expanding this and setting the derivatives w.r.t. <span class="math inline">\(\theta\)</span> equal to zero gives:
<span class="math display">\[
\begin{aligned}
SSE(X, \theta) = y^Ty - 2\theta^{T} X^Ty + \theta^{T}X^{T}X\theta \\
\frac{\partial(SSE)}{\partial{\theta}} = -2X^{T}y + 2X^{T}X\theta = 0
\end{aligned}
\]</span></p>
<p>Now solving for <span class="math inline">\(\theta\)</span> gives the normal equation:
<span class="math display">\[
\hat{\theta} = (X^TX)^{-1}X^{T}y
\]</span></p>
<p>Let’s code the normal equation and print out the estimates. Before jumping into estimating the
normal equation, we’ve got to be careful and add the intercept term (all ones) on X as the simulated data from make_regression comes without it. We’ll do this using add_dummy_feature() in sklearn.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="regression-and-gradient-descent.html#cb63-1" tabindex="-1"></a><span class="co"># The normal equation</span></span>
<span id="cb63-2"><a href="regression-and-gradient-descent.html#cb63-2" tabindex="-1"></a>X <span class="op">=</span> add_dummy_feature(X)</span>
<span id="cb63-3"><a href="regression-and-gradient-descent.html#cb63-3" tabindex="-1"></a>theta_best <span class="op">=</span> np.linalg.inv((X.T <span class="op">@</span> X)) <span class="op">@</span> (X.T <span class="op">@</span> y)</span>
<span id="cb63-4"><a href="regression-and-gradient-descent.html#cb63-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;coefficients from normal equation: </span><span class="sc">{</span>theta_best<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>coefficients from normal equation: [10.00156877 40.74650082  6.62076534]</code></pre>
<p><strong>ii. Gradient Descent</strong></p>
<p>Imagine that you are standing at the top of a mountain and want to descend the mountain as quickly as possible. One simple way is to consider a few directions – north, south, east, and west – and evaluate the steepness (gradient). Then you’d want to take a small step towards the steepest direction, pause, and re-evaluate the steepness. Doing this repeatedly gets you to the bottom of the mountain as fast as possible.</p>
<p>The idea of gradient descent is similar in context to the aforementioned analogy. We’ve already been exposed to the idea of MSE and the objective of minimizing MSE. Instead of using the closed form normal equation to solve for the minimum of MSE, gradient descent uses <em>gradient</em> of MSE to adjust the estimates and move closer to the minimum.</p>
<p>The gradient is a vector of partial derivatives of MSE that points to the direction of steepest increase increase in MSE. Hence, to minimize the loss, we’d want to move in opposite direction of the gradient. By repeatedly updating our parameter in this way, we move closer and closer to the minimum of the loss function. To simply the concept, we’ll start with the univariate case without the intercept.</p>
<p><span class="math display">\[
\begin{aligned}
Y_i = \beta X_i + \epsilon_i
\end{aligned}
\]</span></p>
<p>The MSE and the derivative is:</p>
<p><span class="math display">\[
\begin{aligned}
MSE(\beta) = \frac{1}{m} (Y - \beta X)^{T}(Y - \beta X)  \\
\frac{\partial{MSE}}{\partial{\beta}} = -\frac{2}{m} X^{T}(Y - \beta X)
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(\frac{2}{m}X^{T}(Y - \beta X)\)</span> is the gradient of the univariate specification, which informs the direction of the steepest increase in MSE. To reduce MSE, we therefore move in the opposite direction of the gradient.</p>
<p>Now that we have the gradient, the gradient descent algorithm can be set up as follows:
1. Start with an initial guesses of the parameter <span class="math inline">\((\beta_o)\)</span>.
2. Update estimates of parameters by moving to the opposite direction of the gradient.
<span class="math display">\[     
      \beta_{new} = \beta_o - \eta \times gradient_o.
      \]</span>
where, <span class="math inline">\(\eta\)</span> is the <em>learning rate</em>.
3. Re-evaluate the gradient using <span class="math inline">\(\beta_{new}\)</span>.
4. Repeat steps 2 and 3 for a given number of times or until convergence is reached.</p>
<p>Let’s simulate data for univariate model specification to visually see what this looks like.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="regression-and-gradient-descent.html#cb65-1" tabindex="-1"></a>dat_uni <span class="op">=</span> make_regression(</span>
<span id="cb65-2"><a href="regression-and-gradient-descent.html#cb65-2" tabindex="-1"></a>                n_samples<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb65-3"><a href="regression-and-gradient-descent.html#cb65-3" tabindex="-1"></a>                n_features<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb65-4"><a href="regression-and-gradient-descent.html#cb65-4" tabindex="-1"></a>                n_informative<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb65-5"><a href="regression-and-gradient-descent.html#cb65-5" tabindex="-1"></a>                n_targets<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb65-6"><a href="regression-and-gradient-descent.html#cb65-6" tabindex="-1"></a>                bias<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb65-7"><a href="regression-and-gradient-descent.html#cb65-7" tabindex="-1"></a>                noise<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb65-8"><a href="regression-and-gradient-descent.html#cb65-8" tabindex="-1"></a>                random_state<span class="op">=</span><span class="dv">42</span>, </span>
<span id="cb65-9"><a href="regression-and-gradient-descent.html#cb65-9" tabindex="-1"></a>                coef<span class="op">=</span><span class="va">True</span></span>
<span id="cb65-10"><a href="regression-and-gradient-descent.html#cb65-10" tabindex="-1"></a>)</span>
<span id="cb65-11"><a href="regression-and-gradient-descent.html#cb65-11" tabindex="-1"></a></span>
<span id="cb65-12"><a href="regression-and-gradient-descent.html#cb65-12" tabindex="-1"></a>X_uni, y_uni, coef <span class="op">=</span> dat_uni</span>
<span id="cb65-13"><a href="regression-and-gradient-descent.html#cb65-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The first five rows of X: </span><span class="sc">{</span>X_uni[<span class="dv">0</span>:<span class="dv">5</span>,:]<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb65-14"><a href="regression-and-gradient-descent.html#cb65-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The first five y values: </span><span class="sc">{</span>y_uni[<span class="dv">0</span>:<span class="dv">5</span>]<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb65-15"><a href="regression-and-gradient-descent.html#cb65-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The coefficient of univariate model is: </span><span class="sc">{</span>coef<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb65-16"><a href="regression-and-gradient-descent.html#cb65-16" tabindex="-1"></a></span>
<span id="cb65-17"><a href="regression-and-gradient-descent.html#cb65-17" tabindex="-1"></a><span class="co"># set up gradient descent </span></span>
<span id="cb65-18"><a href="regression-and-gradient-descent.html#cb65-18" tabindex="-1"></a>beta <span class="op">=</span> <span class="op">-</span><span class="dv">5</span> <span class="co"># initialize beta</span></span>
<span id="cb65-19"><a href="regression-and-gradient-descent.html#cb65-19" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># learning rate</span></span>
<span id="cb65-20"><a href="regression-and-gradient-descent.html#cb65-20" tabindex="-1"></a>m <span class="op">=</span> X_uni.shape[<span class="dv">0</span>] <span class="co"># number of observations</span></span>
<span id="cb65-21"><a href="regression-and-gradient-descent.html#cb65-21" tabindex="-1"></a>iter_val <span class="op">=</span> <span class="dv">100</span> <span class="co"># number of iteration steps</span></span>
<span id="cb65-22"><a href="regression-and-gradient-descent.html#cb65-22" tabindex="-1"></a>y_uni <span class="op">=</span> y_uni.reshape((m,<span class="dv">1</span>)) <span class="co"># reshape into m*1 vector</span></span>
<span id="cb65-23"><a href="regression-and-gradient-descent.html#cb65-23" tabindex="-1"></a>beta_store <span class="op">=</span> np.ones(m)</span>
<span id="cb65-24"><a href="regression-and-gradient-descent.html#cb65-24" tabindex="-1"></a>loss_store <span class="op">=</span> np.ones(m)</span>
<span id="cb65-25"><a href="regression-and-gradient-descent.html#cb65-25" tabindex="-1"></a></span>
<span id="cb65-26"><a href="regression-and-gradient-descent.html#cb65-26" tabindex="-1"></a><span class="co"># loop </span></span>
<span id="cb65-27"><a href="regression-and-gradient-descent.html#cb65-27" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iter_val):</span>
<span id="cb65-28"><a href="regression-and-gradient-descent.html#cb65-28" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> X_uni.T <span class="op">@</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni)</span>
<span id="cb65-29"><a href="regression-and-gradient-descent.html#cb65-29" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">2</span><span class="op">/</span>m <span class="op">*</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni).T <span class="op">@</span> (y_uni <span class="op">-</span> beta<span class="op">*</span>X_uni)</span>
<span id="cb65-30"><a href="regression-and-gradient-descent.html#cb65-30" tabindex="-1"></a>    beta <span class="op">=</span> beta <span class="op">-</span> eta<span class="op">*</span>gradient</span>
<span id="cb65-31"><a href="regression-and-gradient-descent.html#cb65-31" tabindex="-1"></a>    beta_store[i] <span class="op">=</span> beta.item() <span class="co"># use .item to extract scalar</span></span>
<span id="cb65-32"><a href="regression-and-gradient-descent.html#cb65-32" tabindex="-1"></a>    loss_store[i] <span class="op">=</span> loss.item() <span class="co"># use .item to extract scalar</span></span>
<span id="cb65-33"><a href="regression-and-gradient-descent.html#cb65-33" tabindex="-1"></a></span>
<span id="cb65-34"><a href="regression-and-gradient-descent.html#cb65-34" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;gradient descent at work: </span><span class="sc">{</span>beta_store<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb65-35"><a href="regression-and-gradient-descent.html#cb65-35" tabindex="-1"></a></span>
<span id="cb65-36"><a href="regression-and-gradient-descent.html#cb65-36" tabindex="-1"></a><span class="co"># figure </span></span>
<span id="cb65-37"><a href="regression-and-gradient-descent.html#cb65-37" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb65-38"><a href="regression-and-gradient-descent.html#cb65-38" tabindex="-1"></a>plt.scatter(beta_store, loss_store)</span>
<span id="cb65-39"><a href="regression-and-gradient-descent.html#cb65-39" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;beta value&quot;</span>)</span>
<span id="cb65-40"><a href="regression-and-gradient-descent.html#cb65-40" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;loss&quot;</span>)</span>
<span id="cb65-41"><a href="regression-and-gradient-descent.html#cb65-41" tabindex="-1"></a>plt.title(<span class="st">&quot;Gradient Descent at Work&quot;</span>)</span>
<span id="cb65-42"><a href="regression-and-gradient-descent.html#cb65-42" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>The first five rows of X: [[ 0.19686124]
 [ 0.35711257]
 [-1.91328024]
 [-0.03582604]
 [ 0.76743473]] 
 

The first five y values: [  8.21720459  14.90627167 -79.86242262  -1.49541829  32.03357001] 
 

The coefficient of univariate model is: 41.7411003148779 
 

gradient descent at work: [ 2.73384129  9.18803147 14.57430322 19.06935566 22.82065107 25.95125242
 28.56386053 30.74418321 32.56374696 34.0822434  35.3494875  36.4070518
 37.28963019 38.02617604 38.64085208 39.15382306 39.58191721 39.93917836
 40.23732664 40.48614292 40.69378975 40.86707908 41.01169573 41.13238393
 41.23310291 41.3171568  41.38730303 41.44584277 41.49469646 41.53546675
 41.56949114 41.59788581 41.62158226 41.64135787 41.65786138 41.6716342
 41.68312815 41.6927203  41.70072532 41.70740581 41.71298095 41.71763361
 41.72151644 41.72475682 41.72746103 41.7297178  41.73160117 41.73317291
 41.73448459 41.73557923 41.73649276 41.73725513 41.73789136 41.73842232
 41.73886542 41.73923521 41.73954381 41.73980135 41.74001628 41.74019565
 41.74034533 41.74047025 41.74057451 41.74066151 41.74073411 41.7407947
 41.74084527 41.74088747 41.74092269 41.74095208 41.74097661 41.74099708
 41.74101416 41.74102841 41.74104031 41.74105024 41.74105852 41.74106544
 41.74107121 41.74107603 41.74108004 41.7410834  41.7410862  41.74108853
 41.74109048 41.74109211 41.74109347 41.7410946  41.74109555 41.74109633
 41.74109699 41.74109754 41.741098   41.74109838 41.7410987  41.74109897
 41.74109919 41.74109938 41.74109953 41.74109966] 
 </code></pre>
<div class="float">
<img src="01_Regression_Gradient_Descent_files/01_Regression_Gradient_Descent_16_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Here, we see that the algorithm converges at the estimated <span class="math inline">\(\beta\)</span> little over 40. Let’s print out the best beta from gradient descent and the true parameter for comparison.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="regression-and-gradient-descent.html#cb67-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;True parameter of the univariate model: </span><span class="sc">{</span>coef<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> &quot;</span>)</span>
<span id="cb67-2"><a href="regression-and-gradient-descent.html#cb67-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Best estimate from gradient descent: </span><span class="sc">{</span>beta<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>) </span></code></pre></div>
<pre><code>True parameter of the univariate model: 41.7411003148779 
 
 
Best estimate from gradient descent: [[41.74109966]] 
 </code></pre>
<p>See that the estimate obtained from gradient descent is close to the true parameter.</p>
<p><strong>Multivariate model</strong></p>
<p>The gradient descent works similarly in case of multivariate model specification except that we’ll have a vector of partial derivatives. The multivariate model specified at the very begining is:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i = \alpha + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i \\
MSE(\theta) = \frac{1}{m} (Y - X \theta)^{T}(Y - X \theta)
\end{aligned}
\]</span></p>
<p>I’ve expressed MSE in matrix form, where <span class="math inline">\(\theta\)</span> incorportates the vector of parameters: <span class="math inline">\(\theta = [\alpha, \; \beta_1, \; \beta_2]^{T}\)</span>.</p>
<p>The gradient vector is given as:
<span class="math display">\[
\frac{\partial MSE}{\partial \theta} = \frac{1}{m} 2 X^{T}(Y - X \theta)
\]</span></p>
<p>The gradient vector is of dimension <span class="math inline">\(3 \times 1\)</span> and stacks all partials of MSE with respect to <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>.</p>
<p>Let’s code the gradient descent algorithm and print out both the true parameters and their estimates.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="regression-and-gradient-descent.html#cb69-1" tabindex="-1"></a><span class="co"># Use the gradient descent algorithm</span></span>
<span id="cb69-2"><a href="regression-and-gradient-descent.html#cb69-2" tabindex="-1"></a>m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb69-3"><a href="regression-and-gradient-descent.html#cb69-3" tabindex="-1"></a>y <span class="op">=</span> y.reshape((m, <span class="dv">1</span>))</span>
<span id="cb69-4"><a href="regression-and-gradient-descent.html#cb69-4" tabindex="-1"></a>theta <span class="op">=</span> np.random.randn(<span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb69-5"><a href="regression-and-gradient-descent.html#cb69-5" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb69-6"><a href="regression-and-gradient-descent.html#cb69-6" tabindex="-1"></a></span>
<span id="cb69-7"><a href="regression-and-gradient-descent.html#cb69-7" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb69-8"><a href="regression-and-gradient-descent.html#cb69-8" tabindex="-1"></a>    gradient <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="op">/</span> m <span class="op">*</span> X.T <span class="op">@</span> (y <span class="op">-</span> X <span class="op">@</span> theta)</span>
<span id="cb69-9"><a href="regression-and-gradient-descent.html#cb69-9" tabindex="-1"></a>    theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> gradient </span>
<span id="cb69-10"><a href="regression-and-gradient-descent.html#cb69-10" tabindex="-1"></a></span>
<span id="cb69-11"><a href="regression-and-gradient-descent.html#cb69-11" tabindex="-1"></a><span class="co"># True coefficients </span></span>
<span id="cb69-12"><a href="regression-and-gradient-descent.html#cb69-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;True coefficients: </span><span class="sc">{</span>np<span class="sc">.</span>hstack([<span class="dv">10</span>, coefficient])<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb69-13"><a href="regression-and-gradient-descent.html#cb69-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Coefficients from gradient descent: </span><span class="sc">{</span>theta<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>True coefficients: [10.         40.71064891  6.60098441]
Coefficients from gradient descent: [[10.00156879]
 [40.74650076]
 [ 6.62076533]]</code></pre>
<p>Note that these coefficients are exactly similar to those obtained from the normal equation method. We can also make sure that we’ve done the estimation correctly by comparing the estimates with those obtained from built in module in sklearn used for purposes of estimating linear regression models.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="regression-and-gradient-descent.html#cb71-1" tabindex="-1"></a><span class="co"># sklearn</span></span>
<span id="cb71-2"><a href="regression-and-gradient-descent.html#cb71-2" tabindex="-1"></a>reg <span class="op">=</span> linear_model.LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb71-3"><a href="regression-and-gradient-descent.html#cb71-3" tabindex="-1"></a>reg.fit(X, y)</span>
<span id="cb71-4"><a href="regression-and-gradient-descent.html#cb71-4" tabindex="-1"></a>best_theta_coef <span class="op">=</span> reg.coef_</span>
<span id="cb71-5"><a href="regression-and-gradient-descent.html#cb71-5" tabindex="-1"></a>best_int_coef <span class="op">=</span> reg.intercept_</span>
<span id="cb71-6"><a href="regression-and-gradient-descent.html#cb71-6" tabindex="-1"></a>best_theta_coef <span class="op">=</span> np.concatenate([best_int_coef, best_theta_coef[:, <span class="dv">1</span>:<span class="dv">3</span>].ravel()], axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb71-7"><a href="regression-and-gradient-descent.html#cb71-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;coefficients from sklearn: </span><span class="sc">{</span>best_theta_coef<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>coefficients from sklearn: [10.00156877 40.74650082  6.62076534]</code></pre>
<p>Not too bad!!</p>
<p><strong>Standard Error</strong></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="regression-and-gradient-descent.html#cb73-1" tabindex="-1"></a><span class="co"># ----------------------------------------------</span></span>
<span id="cb73-2"><a href="regression-and-gradient-descent.html#cb73-2" tabindex="-1"></a></span>
<span id="cb73-3"><a href="regression-and-gradient-descent.html#cb73-3" tabindex="-1"></a></span>
<span id="cb73-4"><a href="regression-and-gradient-descent.html#cb73-4" tabindex="-1"></a><span class="co"># Standard errors </span></span>
<span id="cb73-5"><a href="regression-and-gradient-descent.html#cb73-5" tabindex="-1"></a></span>
<span id="cb73-6"><a href="regression-and-gradient-descent.html#cb73-6" tabindex="-1"></a></span>
<span id="cb73-7"><a href="regression-and-gradient-descent.html#cb73-7" tabindex="-1"></a><span class="co"># ----------------------------------------------</span></span>
<span id="cb73-8"><a href="regression-and-gradient-descent.html#cb73-8" tabindex="-1"></a></span>
<span id="cb73-9"><a href="regression-and-gradient-descent.html#cb73-9" tabindex="-1"></a><span class="co"># 1. Get the standard error of the regression</span></span>
<span id="cb73-10"><a href="regression-and-gradient-descent.html#cb73-10" tabindex="-1"></a></span>
<span id="cb73-11"><a href="regression-and-gradient-descent.html#cb73-11" tabindex="-1"></a>error <span class="op">=</span> (X <span class="op">@</span> theta <span class="op">-</span> y)</span>
<span id="cb73-12"><a href="regression-and-gradient-descent.html#cb73-12" tabindex="-1"></a>error_sq <span class="op">=</span> error.T <span class="op">@</span> error</span>
<span id="cb73-13"><a href="regression-and-gradient-descent.html#cb73-13" tabindex="-1"></a>sigma_sq <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (m <span class="op">-</span><span class="dv">3</span>) <span class="op">*</span> error_sq</span>
<span id="cb73-14"><a href="regression-and-gradient-descent.html#cb73-14" tabindex="-1"></a>se_reg <span class="op">=</span> np.sqrt(sigma_sq)</span>
<span id="cb73-15"><a href="regression-and-gradient-descent.html#cb73-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard error of the regression is: </span><span class="sc">{</span>se_reg<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb73-16"><a href="regression-and-gradient-descent.html#cb73-16" tabindex="-1"></a></span>
<span id="cb73-17"><a href="regression-and-gradient-descent.html#cb73-17" tabindex="-1"></a><span class="co"># 2. get standard errors of the respective coefficients </span></span>
<span id="cb73-18"><a href="regression-and-gradient-descent.html#cb73-18" tabindex="-1"></a>var_cov <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">*</span> sigma_sq</span>
<span id="cb73-19"><a href="regression-and-gradient-descent.html#cb73-19" tabindex="-1"></a>manual_se <span class="op">=</span> np.sqrt(np.diag(var_cov))</span>
<span id="cb73-20"><a href="regression-and-gradient-descent.html#cb73-20" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard errors of coefficients (manual estimation): </span><span class="sc">{</span>manual_se<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb73-21"><a href="regression-and-gradient-descent.html#cb73-21" tabindex="-1"></a></span>
<span id="cb73-22"><a href="regression-and-gradient-descent.html#cb73-22" tabindex="-1"></a><span class="co"># se from stats model</span></span>
<span id="cb73-23"><a href="regression-and-gradient-descent.html#cb73-23" tabindex="-1"></a>X_sm <span class="op">=</span> sm.add_constant(X)   <span class="co"># add intercept</span></span>
<span id="cb73-24"><a href="regression-and-gradient-descent.html#cb73-24" tabindex="-1"></a>model <span class="op">=</span> sm.OLS(y, X_sm).fit()</span>
<span id="cb73-25"><a href="regression-and-gradient-descent.html#cb73-25" tabindex="-1"></a></span>
<span id="cb73-26"><a href="regression-and-gradient-descent.html#cb73-26" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;coefficients from statmodels: </span><span class="sc">{</span>model<span class="sc">.</span>params<span class="sc">}</span><span class="ss">&quot;</span>)   <span class="co"># coefficients</span></span>
<span id="cb73-27"><a href="regression-and-gradient-descent.html#cb73-27" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;standard errors from statmodels: </span><span class="sc">{</span>model<span class="sc">.</span>bse<span class="sc">}</span><span class="ss">&quot;</span>)      <span class="co"># standard errors</span></span></code></pre></div>
<pre><code>standard error of the regression is: [[0.98562573]]
standard errors of coefficients (manual estimation): [0.03123574 0.03242909 0.03072431]
coefficients from statmodels: [10.00156877 40.74650082  6.62076534]
standard errors from statmodels: [0.03123574 0.03242909 0.03072431]</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"></code></pre></div>

<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"></code></pre></div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="an-exercise.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="standard-errors-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  },
  "toolbar": {
    "position": "static"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
