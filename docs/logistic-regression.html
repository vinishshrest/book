<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Logistic regression | Causal Inference</title>
  <meta name="description" content="5 Logistic regression | Causal Inference" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Logistic regression | Causal Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Logistic regression | Causal Inference" />
  
  
  

<meta name="author" content="Vinish Shrestha" />


<meta name="date" content="2026-02-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-and-gradient-descent.html"/>
<link rel="next" href="causal-inference.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="assets/kePrint-0.0.1/kePrint.js"></script>
<link href="assets/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="a-lab-experiment.html"><a href="a-lab-experiment.html"><i class="fa fa-check"></i><b>2.1</b> A lab experiment</a></li>
<li class="chapter" data-level="2.2" data-path="challenges.html"><a href="challenges.html"><i class="fa fa-check"></i><b>2.2</b> Challenges</a></li>
<li class="chapter" data-level="2.3" data-path="dag-directed-acyclic-graph.html"><a href="dag-directed-acyclic-graph.html"><i class="fa fa-check"></i><b>2.3</b> DAG (Directed Acyclic Graph)</a></li>
<li class="chapter" data-level="2.4" data-path="a-simulated-dgp.html"><a href="a-simulated-dgp.html"><i class="fa fa-check"></i><b>2.4</b> A simulated DGP</a></li>
<li class="chapter" data-level="2.5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="why-regression.html"><a href="why-regression.html"><i class="fa fa-check"></i><b>3</b> Why Regression?</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-best-fit-line.html"><a href="the-best-fit-line.html"><i class="fa fa-check"></i><b>3.1</b> The best-fit line</a></li>
<li class="chapter" data-level="3.2" data-path="linear-regression-specification.html"><a href="linear-regression-specification.html"><i class="fa fa-check"></i><b>3.2</b> Linear Regression Specification</a></li>
<li class="chapter" data-level="3.3" data-path="law-of-iterated-expectation.html"><a href="law-of-iterated-expectation.html"><i class="fa fa-check"></i><b>3.3</b> Law of iterated expectation</a></li>
<li class="chapter" data-level="3.4" data-path="error-term.html"><a href="error-term.html"><i class="fa fa-check"></i><b>3.4</b> Error term</a></li>
<li class="chapter" data-level="3.5" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>3.5</b> Decomposition</a></li>
<li class="chapter" data-level="3.6" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>3.6</b> Estimation</a></li>
<li class="chapter" data-level="3.7" data-path="running-a-regression.html"><a href="running-a-regression.html"><i class="fa fa-check"></i><b>3.7</b> Running a regression</a></li>
<li class="chapter" data-level="3.8" data-path="standard-errors.html"><a href="standard-errors.html"><i class="fa fa-check"></i><b>3.8</b> Standard Errors</a></li>
<li class="chapter" data-level="3.9" data-path="an-exercise.html"><a href="an-exercise.html"><i class="fa fa-check"></i><b>3.9</b> An exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression-and-gradient-descent.html"><a href="regression-and-gradient-descent.html"><i class="fa fa-check"></i><b>4</b> Regression and Gradient Descent</a></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic regression</a></li>
<li class="chapter" data-level="6" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>6</b> Causal Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="potential-outcome-framework-neyman-rubin-causal-model.html"><a href="potential-outcome-framework-neyman-rubin-causal-model.html"><i class="fa fa-check"></i><b>6.1</b> Potential Outcome Framework: Neyman-Rubin Causal Model</a></li>
<li class="chapter" data-level="6.2" data-path="average-treatment-effect-ate.html"><a href="average-treatment-effect-ate.html"><i class="fa fa-check"></i><b>6.2</b> Average treatment effect (ATE)</a></li>
<li class="chapter" data-level="6.3" data-path="rct.html"><a href="rct.html"><i class="fa fa-check"></i><b>6.3</b> RCT</a></li>
<li class="chapter" data-level="6.4" data-path="average-treatment-effect-on-the-treated-att.html"><a href="average-treatment-effect-on-the-treated-att.html"><i class="fa fa-check"></i><b>6.4</b> Average treatment effect on the treated (ATT)</a></li>
<li class="chapter" data-level="6.5" data-path="an-estimation-example.html"><a href="an-estimation-example.html"><i class="fa fa-check"></i><b>6.5</b> An estimation example</a></li>
<li class="chapter" data-level="6.6" data-path="unconfoundedness-assumption.html"><a href="unconfoundedness-assumption.html"><i class="fa fa-check"></i><b>6.6</b> Unconfoundedness assumption</a></li>
<li class="chapter" data-level="6.7" data-path="discussion-1.html"><a href="discussion-1.html"><i class="fa fa-check"></i><b>6.7</b> Discussion</a></li>
<li class="chapter" data-level="6.8" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>6.8</b> Reference</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ipw-and-aipw.html"><a href="ipw-and-aipw.html"><i class="fa fa-check"></i><b>7</b> IPW and AIPW</a>
<ul>
<li class="chapter" data-level="7.1" data-path="a-simple-example.html"><a href="a-simple-example.html"><i class="fa fa-check"></i><b>7.1</b> A simple example</a></li>
<li class="chapter" data-level="7.2" data-path="aggregated-estimator.html"><a href="aggregated-estimator.html"><i class="fa fa-check"></i><b>7.2</b> Aggregated Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="propensity-score.html"><a href="propensity-score.html"><i class="fa fa-check"></i><b>7.3</b> Propensity score</a></li>
<li class="chapter" data-level="7.4" data-path="estimation-of-propensity-score.html"><a href="estimation-of-propensity-score.html"><i class="fa fa-check"></i><b>7.4</b> Estimation of propensity score</a></li>
<li class="chapter" data-level="7.5" data-path="using-cross-fitting-to-predict-propensity-score.html"><a href="using-cross-fitting-to-predict-propensity-score.html"><i class="fa fa-check"></i><b>7.5</b> Using cross-fitting to predict propensity score</a></li>
<li class="chapter" data-level="7.6" data-path="propensity-score-stratification.html"><a href="propensity-score-stratification.html"><i class="fa fa-check"></i><b>7.6</b> Propensity score stratification</a></li>
<li class="chapter" data-level="7.7" data-path="inverse-probability-weighting-ipw.html"><a href="inverse-probability-weighting-ipw.html"><i class="fa fa-check"></i><b>7.7</b> Inverse Probability Weighting (IPW)</a></li>
<li class="chapter" data-level="7.8" data-path="comparing-ipw-with-aggregated-estimate.html"><a href="comparing-ipw-with-aggregated-estimate.html"><i class="fa fa-check"></i><b>7.8</b> Comparing IPW with Aggregated Estimate</a></li>
<li class="chapter" data-level="7.9" data-path="aipw-and-estimation.html"><a href="aipw-and-estimation.html"><i class="fa fa-check"></i><b>7.9</b> AIPW and Estimation</a></li>
<li class="chapter" data-level="7.10" data-path="assessing-balance.html"><a href="assessing-balance.html"><i class="fa fa-check"></i><b>7.10</b> Assessing Balance</a></li>
<li class="chapter" data-level="7.11" data-path="cross-fitting.html"><a href="cross-fitting.html"><i class="fa fa-check"></i><b>7.11</b> Cross-fitting</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="difference-in-differences.html"><a href="difference-in-differences.html"><i class="fa fa-check"></i><b>8</b> Difference in Differences</a>
<ul>
<li class="chapter" data-level="8.1" data-path="a-quick-introduction.html"><a href="a-quick-introduction.html"><i class="fa fa-check"></i><b>8.1</b> A Quick Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="set-up.html"><a href="set-up.html"><i class="fa fa-check"></i><b>8.2</b> Set up</a></li>
<li class="chapter" data-level="8.3" data-path="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><a href="an-example-evaluating-the-impact-of-medicaid-expansion-on-uninsured-rate.html"><i class="fa fa-check"></i><b>8.3</b> An example: Evaluating the impact of Medicaid expansion on uninsured rate</a></li>
<li class="chapter" data-level="8.4" data-path="naive-estimator.html"><a href="naive-estimator.html"><i class="fa fa-check"></i><b>8.4</b> Naive estimator</a></li>
<li class="chapter" data-level="8.5" data-path="canonical-difference-in-differences-framework.html"><a href="canonical-difference-in-differences-framework.html"><i class="fa fa-check"></i><b>8.5</b> Canonical Difference in Differences Framework</a></li>
<li class="chapter" data-level="8.6" data-path="did-in-multi-period-set-up.html"><a href="did-in-multi-period-set-up.html"><i class="fa fa-check"></i><b>8.6</b> DiD in multi-period set up</a></li>
<li class="chapter" data-level="8.7" data-path="conditional-parallel-trend-assumption.html"><a href="conditional-parallel-trend-assumption.html"><i class="fa fa-check"></i><b>8.7</b> Conditional Parallel Trend Assumption</a></li>
<li class="chapter" data-level="8.8" data-path="some-concerns-with-controls.html"><a href="some-concerns-with-controls.html"><i class="fa fa-check"></i><b>8.8</b> Some concerns with controls</a></li>
<li class="chapter" data-level="8.9" data-path="the-2-times-2-difference-in-differences-estimate.html"><a href="the-2-times-2-difference-in-differences-estimate.html"><i class="fa fa-check"></i><b>8.9</b> The <span class="math inline">\(2 \times 2\)</span> Difference-in-Differences Estimate</a></li>
<li class="chapter" data-level="8.10" data-path="event-study-model.html"><a href="event-study-model.html"><i class="fa fa-check"></i><b>8.10</b> Event study model</a></li>
<li class="chapter" data-level="8.11" data-path="two-way-fixed-effect-twfe-revisited.html"><a href="two-way-fixed-effect-twfe-revisited.html"><i class="fa fa-check"></i><b>8.11</b> Two way fixed effect (TWFE) Revisited</a></li>
<li class="chapter" data-level="8.12" data-path="various-ways-of-estimation.html"><a href="various-ways-of-estimation.html"><i class="fa fa-check"></i><b>8.12</b> Various ways of estimation</a></li>
<li class="chapter" data-level="8.13" data-path="multi-period-multi-group-and-variation-in-treatment-timing.html"><a href="multi-period-multi-group-and-variation-in-treatment-timing.html"><i class="fa fa-check"></i><b>8.13</b> Multi Period, Multi Group and Variation in Treatment Timing</a></li>
<li class="chapter" data-level="8.14" data-path="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><a href="problem-with-twfe-in-multiple-group-with-treatment-timing-variation.html"><i class="fa fa-check"></i><b>8.14</b> Problem with TWFE in Multiple Group with Treatment Timing Variation</a></li>
<li class="chapter" data-level="8.15" data-path="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><a href="what-is-twfe-estimating-when-there-is-treatment-timing-variation.html"><i class="fa fa-check"></i><b>8.15</b> What is TWFE Estimating when there is Treatment Timing Variation?</a></li>
<li class="chapter" data-level="8.16" data-path="assumptions-governing-twfedd-estimate.html"><a href="assumptions-governing-twfedd-estimate.html"><i class="fa fa-check"></i><b>8.16</b> Assumptions governing TWFEDD estimate</a></li>
<li class="chapter" data-level="8.17" data-path="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><a href="how-does-treatment-effect-heterogeneity-in-time-affect-twfe.html"><i class="fa fa-check"></i><b>8.17</b> How Does Treatment Effect Heterogeneity in Time Affect TWFE?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="causal-forest.html"><a href="causal-forest.html"><i class="fa fa-check"></i><b>9</b> Causal Forest</a>
<ul>
<li class="chapter" data-level="9.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="summary-of-grf.html"><a href="summary-of-grf.html"><i class="fa fa-check"></i><b>9.2</b> Summary of GRF</a></li>
<li class="chapter" data-level="9.3" data-path="motivation-for-causal-forests.html"><a href="motivation-for-causal-forests.html"><i class="fa fa-check"></i><b>9.3</b> Motivation for Causal Forests</a></li>
<li class="chapter" data-level="9.4" data-path="causal-forest-1.html"><a href="causal-forest-1.html"><i class="fa fa-check"></i><b>9.4</b> Causal Forest</a></li>
<li class="chapter" data-level="9.5" data-path="an-example-of-causal-forest.html"><a href="an-example-of-causal-forest.html"><i class="fa fa-check"></i><b>9.5</b> An example of causal forest</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>10</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="10.1" data-path="some-ways-to-estimate-cate.html"><a href="some-ways-to-estimate-cate.html"><i class="fa fa-check"></i><b>10.1</b> Some ways to estimate CATE</a></li>
<li class="chapter" data-level="10.2" data-path="estimation-1.html"><a href="estimation-1.html"><i class="fa fa-check"></i><b>10.2</b> Estimation</a></li>
<li class="chapter" data-level="10.3" data-path="some-remarks-and-questions.html"><a href="some-remarks-and-questions.html"><i class="fa fa-check"></i><b>10.3</b> Some Remarks and Questions</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Logistic regression<a href="logistic-regression.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Course:</strong> Causal Inference</p>
<p><strong>Topic:</strong> Logistic Regression</p>
<p>Say, you need to predict the probability of someone being in a good vs. bad health (binary outcome)
given a set of
inputs: i) college education (yes or no); ii) income (high vs. low); iii) insured vs uninsured;
and iv) stress level (continuous variable).</p>
<p>For the sake of simplicity, we are going to assume a super simple DGP as follows:</p>
<ol style="list-style-type: decimal">
<li>College education has a positive effect on health (coefficient = 0.1)</li>
<li>High income has a positive effect on health (coefficient = 0.2)</li>
<li>40 percent more people from higher income households have college education</li>
<li>Insurance has a positive effect on health (coefficient = 0.05)</li>
<li>60 percent more people from low income households are likely to be stressed</li>
<li>Stress has a negative effect on health (coefficient = -1)</li>
</ol>
<p><strong>Note:</strong> We know from the previous lecture that LPM estimates directly gives us the marginal
effects – the coefficients (at least theoretically) are interpreted as the effect of marginal changes
in <span class="math inline">\(X\)</span>s on <span class="math inline">\(y\)</span>. LPM estimates are straight forward and easy to interpret as most often we are
concerned with marginal effects from a policy standpoint.
However, the coefficients pertaining to logistic regression are not marginal effects.
This will be clearer as we proceed.</p>
<p>We looked at the Linear Probability Model (LPM) in the previous lecture – the estimates from a
properly specified model were close to the true parameters.
What if we have to estimate probability
of someone being in good health? In this case, LPM does
not gurantee that the probabilities are restricted between 0 and 1.</p>
<p>Logistic regression is a tool that is used to model binary outcome and used for classification purposes.
It uses a logistic function to restrict
probabilities between values of 0 and 1. So how does it work?</p>
<p>Essentially, the primary goal here is to predict probabilities of a binary event.
The probability is written as a function of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(X\)</span>:
<span class="math display">\[
p = h_\theta(X) = \sigma(\theta X)
\]</span></p>
<p><span class="math inline">\(\theta\)</span> is a vector of true parameters – they govern the DGP, and probabilites
are the function of the true coefficients and inputs.
Specifically, <span class="math inline">\(\sigma(.)\)</span> is the logistic function, defined as:</p>
<p><span class="math display">\[
\sigma(z) = \frac{1}{(1 + exp(-z))}
\]</span></p>
<p>This <span class="math inline">\(\sigma(.)\)</span> is also known as the sigmoid function and <span class="math inline">\(z=\theta X\)</span> is often
known as the logit. The logit is a
linear combination of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(X\)</span>. By nature of the logistic function, the output is restricted
between 0 and 1.<br />
To see the logistic function closely, let’s take a look at the following graph.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="logistic-regression.html#cb76-1" tabindex="-1"></a></span>
<span id="cb76-2"><a href="logistic-regression.html#cb76-2" tabindex="-1"></a><span class="co"># import necessary libraries</span></span>
<span id="cb76-3"><a href="logistic-regression.html#cb76-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np    </span>
<span id="cb76-4"><a href="logistic-regression.html#cb76-4" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb76-5"><a href="logistic-regression.html#cb76-5" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb76-6"><a href="logistic-regression.html#cb76-6" tabindex="-1"></a></span>
<span id="cb76-7"><a href="logistic-regression.html#cb76-7" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb76-8"><a href="logistic-regression.html#cb76-8" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb76-9"><a href="logistic-regression.html#cb76-9" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> add_dummy_feature</span>
<span id="cb76-10"><a href="logistic-regression.html#cb76-10" tabindex="-1"></a></span>
<span id="cb76-11"><a href="logistic-regression.html#cb76-11" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb76-12"><a href="logistic-regression.html#cb76-12" tabindex="-1"></a></span>
<span id="cb76-13"><a href="logistic-regression.html#cb76-13" tabindex="-1"></a><span class="co"># generate numbers from -5 to 5</span></span>
<span id="cb76-14"><a href="logistic-regression.html#cb76-14" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1000</span>)</span>
<span id="cb76-15"><a href="logistic-regression.html#cb76-15" tabindex="-1"></a><span class="bu">print</span>(z[<span class="dv">0</span>:<span class="dv">20</span>])</span>
<span id="cb76-16"><a href="logistic-regression.html#cb76-16" tabindex="-1"></a><span class="co"># compute logistic values (note that these are probabilities)</span></span>
<span id="cb76-17"><a href="logistic-regression.html#cb76-17" tabindex="-1"></a>sigma_z <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb76-18"><a href="logistic-regression.html#cb76-18" tabindex="-1"></a></span>
<span id="cb76-19"><a href="logistic-regression.html#cb76-19" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb76-20"><a href="logistic-regression.html#cb76-20" tabindex="-1"></a>plt.plot(z, sigma_z)</span>
<span id="cb76-21"><a href="logistic-regression.html#cb76-21" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;z&#39;</span>)</span>
<span id="cb76-22"><a href="logistic-regression.html#cb76-22" tabindex="-1"></a>plt.title(<span class="st">&#39;A sigmoid function&#39;</span>)</span>
<span id="cb76-23"><a href="logistic-regression.html#cb76-23" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;probability&#39;</span>)</span>
<span id="cb76-24"><a href="logistic-regression.html#cb76-24" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>[-5.         -4.98998999 -4.97997998 -4.96996997 -4.95995996 -4.94994995
 -4.93993994 -4.92992993 -4.91991992 -4.90990991 -4.8998999  -4.88988989
 -4.87987988 -4.86986987 -4.85985986 -4.84984985 -4.83983984 -4.82982983
 -4.81981982 -4.80980981]</code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_1_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Note that the logistic function is S-shaped – negative logit (z) values will have
probabilities less than 0.5,
whereas the positive z values will have
probablities greater than 0.5. Also, probabilities on the vertical axis are
constrained between 0 and 1, as they should be.</p>
<p>The inputs in the sigmoid function is: <span class="math inline">\(z=\theta X\)</span>, which will help us attain probabilities.
<span class="math inline">\(X\)</span> and <span class="math inline">\(\theta\)</span> are the features (covariates) and the
parameters of interest, respectively.</p>
<p>Using these probability values, one can classify. For example:</p>
<p><span class="math inline">\(y_i = 1\)</span> if <span class="math inline">\(\hat{p_i}\geq 0.5\)</span> or else 0.</p>
<p>Our goal is to come up with the estimates of <span class="math inline">\(\theta\)</span>. After we have <span class="math inline">\(\hat{\theta}\)</span>, we can obtain
probabilities, perform classification based on them, or use probability estimates for downstream analysis.</p>
<p><strong>The Loss Function</strong></p>
<p>To do so, we will start with a loss function. Consider the following:</p>
<p><span class="math inline">\(C = -\log(\hat{p_i})\)</span> if <span class="math inline">\(y_i=1\)</span></p>
<p><span class="math inline">\(C = -\log(1-\hat{p_i})\)</span> if <span class="math inline">\(y_i=0\)</span></p>
<p>Generally speaking, you want the model to come up with higher probabilities for observations with
<span class="math inline">\(y_i=1\)</span> and lower probabilities for <span class="math inline">\(y_i=0\)</span>. With this in mind, consider what might happen
if <span class="math inline">\(\hat{p_i}\)</span> is small vs large (say, 0.05 vs 0.95) when <span class="math inline">\(y_i=1\)</span>.
This will inflate the loss in the former case but reduce it in the latter. The case is
reversed for <span class="math inline">\(y_i=0\)</span>; higher probabilities will yield higher loss; whereas, lower probabilities
will yield lower loss values. So, lower
probabilities are ‘shunned’ for observations with <span class="math inline">\(y_i=1\)</span>, and higher probabilities are penalized
more for observations with <span class="math inline">\(y=0\)</span>.</p>
<p>We put this logic together and come up with the following cross-entropy loss function:</p>
<p><span class="math display">\[
C_{\theta} = -\frac{1}{n} \sum_i^{n} [y_i \times \log(\hat{p_i}) + (1-y_i) \times \log(1-\hat{p_i})]
\]</span></p>
<p>Recall:
<span class="math display">\[
p = \sigma(\theta X) = \frac{1}{(1 + exp(-\theta X))}
\]</span></p>
<p>The <em>objective</em> is to get the estimates of <span class="math inline">\(\theta\)</span> that minimizes the loss function. Turns out that the
loss function above don’t have an analytical or a closed form solution. However, the function is convex, which
means that we can use gradient descent to estimate <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Using Gradient Descent</strong></p>
<p>Let’s first simulate the data following the DGP stated above by using the code below.
Note that the functional that’s used to simulate the
outcome variable (health) will depend on probability values obtained from the
logistic function.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="logistic-regression.html#cb78-1" tabindex="-1"></a></span>
<span id="cb78-2"><a href="logistic-regression.html#cb78-2" tabindex="-1"></a><span class="co"># ----------------------------</span></span>
<span id="cb78-3"><a href="logistic-regression.html#cb78-3" tabindex="-1"></a></span>
<span id="cb78-4"><a href="logistic-regression.html#cb78-4" tabindex="-1"></a><span class="co"># A. Simulate data </span></span>
<span id="cb78-5"><a href="logistic-regression.html#cb78-5" tabindex="-1"></a></span>
<span id="cb78-6"><a href="logistic-regression.html#cb78-6" tabindex="-1"></a><span class="co"># ----------------------------</span></span>
<span id="cb78-7"><a href="logistic-regression.html#cb78-7" tabindex="-1"></a></span>
<span id="cb78-8"><a href="logistic-regression.html#cb78-8" tabindex="-1"></a><span class="co"># 1. College education has a positive effect on health (coefficient = 0.1)</span></span>
<span id="cb78-9"><a href="logistic-regression.html#cb78-9" tabindex="-1"></a><span class="co"># 2. High income has a positive effect on health (coefficient = 0.2)</span></span>
<span id="cb78-10"><a href="logistic-regression.html#cb78-10" tabindex="-1"></a><span class="co"># 3. 40 percent more people from higher income households have college education</span></span>
<span id="cb78-11"><a href="logistic-regression.html#cb78-11" tabindex="-1"></a><span class="co"># 4. Insurance has a positive effect on health (coefficient = 0.05)</span></span>
<span id="cb78-12"><a href="logistic-regression.html#cb78-12" tabindex="-1"></a><span class="co"># 5. Stress has a negative effect on health (coefficient = -1)</span></span>
<span id="cb78-13"><a href="logistic-regression.html#cb78-13" tabindex="-1"></a></span>
<span id="cb78-14"><a href="logistic-regression.html#cb78-14" tabindex="-1"></a><span class="co"># number of obs</span></span>
<span id="cb78-15"><a href="logistic-regression.html#cb78-15" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb78-16"><a href="logistic-regression.html#cb78-16" tabindex="-1"></a></span>
<span id="cb78-17"><a href="logistic-regression.html#cb78-17" tabindex="-1"></a><span class="co"># 1. income </span></span>
<span id="cb78-18"><a href="logistic-regression.html#cb78-18" tabindex="-1"></a>income_log <span class="op">=</span> np.random.lognormal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb78-19"><a href="logistic-regression.html#cb78-19" tabindex="-1"></a>income <span class="op">=</span> income_log <span class="op">*</span> <span class="dv">20000</span></span>
<span id="cb78-20"><a href="logistic-regression.html#cb78-20" tabindex="-1"></a>ln_income <span class="op">=</span> np.log(income)</span>
<span id="cb78-21"><a href="logistic-regression.html#cb78-21" tabindex="-1"></a><span class="co"># categorize high vs low income based on median income</span></span>
<span id="cb78-22"><a href="logistic-regression.html#cb78-22" tabindex="-1"></a>high_income <span class="op">=</span> (income<span class="op">&gt;=</span>np.median(income)).astype(<span class="st">&#39;int&#39;</span>)</span>
<span id="cb78-23"><a href="logistic-regression.html#cb78-23" tabindex="-1"></a>low_income <span class="op">=</span> (income<span class="op">&lt;</span>np.median(income)).astype(<span class="st">&#39;int&#39;</span>)</span>
<span id="cb78-24"><a href="logistic-regression.html#cb78-24" tabindex="-1"></a></span>
<span id="cb78-25"><a href="logistic-regression.html#cb78-25" tabindex="-1"></a><span class="co"># 2. college </span></span>
<span id="cb78-26"><a href="logistic-regression.html#cb78-26" tabindex="-1"></a><span class="kw">def</span> gen_college(prob):</span>
<span id="cb78-27"><a href="logistic-regression.html#cb78-27" tabindex="-1"></a>    col <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, prob, <span class="dv">1</span>)</span>
<span id="cb78-28"><a href="logistic-regression.html#cb78-28" tabindex="-1"></a>    <span class="cf">return</span> col</span>
<span id="cb78-29"><a href="logistic-regression.html#cb78-29" tabindex="-1"></a></span>
<span id="cb78-30"><a href="logistic-regression.html#cb78-30" tabindex="-1"></a>college <span class="op">=</span> []</span>
<span id="cb78-31"><a href="logistic-regression.html#cb78-31" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb78-32"><a href="logistic-regression.html#cb78-32" tabindex="-1"></a>    <span class="co"># 40% more people from high income group will have college degree</span></span>
<span id="cb78-33"><a href="logistic-regression.html#cb78-33" tabindex="-1"></a>    college_i <span class="op">=</span> gen_college(<span class="fl">0.2</span> <span class="op">+</span> <span class="fl">0.4</span><span class="op">*</span>high_income[i])</span>
<span id="cb78-34"><a href="logistic-regression.html#cb78-34" tabindex="-1"></a>    college.append(college_i)</span>
<span id="cb78-35"><a href="logistic-regression.html#cb78-35" tabindex="-1"></a></span>
<span id="cb78-36"><a href="logistic-regression.html#cb78-36" tabindex="-1"></a>college <span class="op">=</span> np.array(college).ravel()</span>
<span id="cb78-37"><a href="logistic-regression.html#cb78-37" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;mean of college: </span><span class="sc">{</span>college<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb78-38"><a href="logistic-regression.html#cb78-38" tabindex="-1"></a></span>
<span id="cb78-39"><a href="logistic-regression.html#cb78-39" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;share college for high income: </span><span class="sc">{</span>np<span class="sc">.</span>mean(college[high_income <span class="op">==</span> <span class="dv">1</span>])<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb78-40"><a href="logistic-regression.html#cb78-40" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;share college for low income: </span><span class="sc">{</span>np<span class="sc">.</span>mean(college[high_income <span class="op">==</span> <span class="dv">0</span>])<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb78-41"><a href="logistic-regression.html#cb78-41" tabindex="-1"></a></span>
<span id="cb78-42"><a href="logistic-regression.html#cb78-42" tabindex="-1"></a><span class="co"># 3. Stress </span></span>
<span id="cb78-43"><a href="logistic-regression.html#cb78-43" tabindex="-1"></a><span class="kw">def</span> gen_stress(prob):</span>
<span id="cb78-44"><a href="logistic-regression.html#cb78-44" tabindex="-1"></a>    p <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, prob, <span class="dv">1</span>)</span>
<span id="cb78-45"><a href="logistic-regression.html#cb78-45" tabindex="-1"></a>    <span class="cf">return</span> p</span>
<span id="cb78-46"><a href="logistic-regression.html#cb78-46" tabindex="-1"></a></span>
<span id="cb78-47"><a href="logistic-regression.html#cb78-47" tabindex="-1"></a>stress <span class="op">=</span> []</span>
<span id="cb78-48"><a href="logistic-regression.html#cb78-48" tabindex="-1"></a></span>
<span id="cb78-49"><a href="logistic-regression.html#cb78-49" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb78-50"><a href="logistic-regression.html#cb78-50" tabindex="-1"></a>    <span class="co"># 60% more people in low income will be stressed</span></span>
<span id="cb78-51"><a href="logistic-regression.html#cb78-51" tabindex="-1"></a>    stress_i <span class="op">=</span> gen_stress(<span class="fl">0.6</span><span class="op">*</span>low_income[i])</span>
<span id="cb78-52"><a href="logistic-regression.html#cb78-52" tabindex="-1"></a>    stress.append(stress_i)</span>
<span id="cb78-53"><a href="logistic-regression.html#cb78-53" tabindex="-1"></a></span>
<span id="cb78-54"><a href="logistic-regression.html#cb78-54" tabindex="-1"></a><span class="co"># a continuous stress variable dependent on income status</span></span>
<span id="cb78-55"><a href="logistic-regression.html#cb78-55" tabindex="-1"></a>stress <span class="op">=</span> np.array(stress).ravel() <span class="op">+</span> np.random.normal(<span class="dv">3</span>, <span class="dv">1</span>, n)<span class="op">*</span>low_income <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, n)</span>
<span id="cb78-56"><a href="logistic-regression.html#cb78-56" tabindex="-1"></a></span>
<span id="cb78-57"><a href="logistic-regression.html#cb78-57" tabindex="-1"></a><span class="co"># histogram of the stress index</span></span>
<span id="cb78-58"><a href="logistic-regression.html#cb78-58" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb78-59"><a href="logistic-regression.html#cb78-59" tabindex="-1"></a>plt.hist(stress, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&quot;steelblue&quot;</span>, edgecolor<span class="op">=</span><span class="st">&quot;black&quot;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb78-60"><a href="logistic-regression.html#cb78-60" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;stress index&#39;</span>)</span>
<span id="cb78-61"><a href="logistic-regression.html#cb78-61" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;frequency&#39;</span>)</span>
<span id="cb78-62"><a href="logistic-regression.html#cb78-62" tabindex="-1"></a>plt.title(<span class="st">&#39;Histogram of stress index&#39;</span>)</span>
<span id="cb78-63"><a href="logistic-regression.html#cb78-63" tabindex="-1"></a>plt.show()</span>
<span id="cb78-64"><a href="logistic-regression.html#cb78-64" tabindex="-1"></a></span>
<span id="cb78-65"><a href="logistic-regression.html#cb78-65" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;average stress index for low income group: </span><span class="sc">{</span>stress[high_income<span class="op">==</span><span class="dv">0</span>]<span class="sc">.</span>mean()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb78-66"><a href="logistic-regression.html#cb78-66" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;average stress index for high income group: </span><span class="sc">{</span>stress[high_income<span class="op">==</span><span class="dv">1</span>]<span class="sc">.</span>mean()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb78-67"><a href="logistic-regression.html#cb78-67" tabindex="-1"></a></span>
<span id="cb78-68"><a href="logistic-regression.html#cb78-68" tabindex="-1"></a><span class="co"># 4. Insurance (exogeneous -- does not depend on other Xs)</span></span>
<span id="cb78-69"><a href="logistic-regression.html#cb78-69" tabindex="-1"></a>insurance <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="fl">0.3</span>, n)</span>
<span id="cb78-70"><a href="logistic-regression.html#cb78-70" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;fraction insured: </span><span class="sc">{</span>insurance<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb78-71"><a href="logistic-regression.html#cb78-71" tabindex="-1"></a></span>
<span id="cb78-72"><a href="logistic-regression.html#cb78-72" tabindex="-1"></a><span class="co"># 5. health (Y variable)</span></span>
<span id="cb78-73"><a href="logistic-regression.html#cb78-73" tabindex="-1"></a><span class="kw">def</span> gen_health(prob):</span>
<span id="cb78-74"><a href="logistic-regression.html#cb78-74" tabindex="-1"></a>    h <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, prob, <span class="dv">1</span>) <span class="co"># these probabilities are going to come from the logistic function</span></span>
<span id="cb78-75"><a href="logistic-regression.html#cb78-75" tabindex="-1"></a>    <span class="cf">return</span> h</span>
<span id="cb78-76"><a href="logistic-regression.html#cb78-76" tabindex="-1"></a></span>
<span id="cb78-77"><a href="logistic-regression.html#cb78-77" tabindex="-1"></a><span class="co"># ----------------------------------------------------</span></span>
<span id="cb78-78"><a href="logistic-regression.html#cb78-78" tabindex="-1"></a></span>
<span id="cb78-79"><a href="logistic-regression.html#cb78-79" tabindex="-1"></a></span>
<span id="cb78-80"><a href="logistic-regression.html#cb78-80" tabindex="-1"></a><span class="co"># Logistic regression using the gradient descent </span></span>
<span id="cb78-81"><a href="logistic-regression.html#cb78-81" tabindex="-1"></a></span>
<span id="cb78-82"><a href="logistic-regression.html#cb78-82" tabindex="-1"></a></span>
<span id="cb78-83"><a href="logistic-regression.html#cb78-83" tabindex="-1"></a><span class="co"># ----------------------------------------------------</span></span>
<span id="cb78-84"><a href="logistic-regression.html#cb78-84" tabindex="-1"></a><span class="co"># define the logistic function</span></span>
<span id="cb78-85"><a href="logistic-regression.html#cb78-85" tabindex="-1"></a><span class="kw">def</span> sigma(<span class="bu">input</span>):</span>
<span id="cb78-86"><a href="logistic-regression.html#cb78-86" tabindex="-1"></a>    logistic <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="bu">input</span>)) </span>
<span id="cb78-87"><a href="logistic-regression.html#cb78-87" tabindex="-1"></a>    <span class="cf">return</span> logistic</span>
<span id="cb78-88"><a href="logistic-regression.html#cb78-88" tabindex="-1"></a></span>
<span id="cb78-89"><a href="logistic-regression.html#cb78-89" tabindex="-1"></a><span class="co"># true thetas governing the DGP</span></span>
<span id="cb78-90"><a href="logistic-regression.html#cb78-90" tabindex="-1"></a>theta_true <span class="op">=</span> np.array([<span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.05</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb78-91"><a href="logistic-regression.html#cb78-91" tabindex="-1"></a>sigma(theta_true)</span>
<span id="cb78-92"><a href="logistic-regression.html#cb78-92" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((college.reshape((n, <span class="dv">1</span>)), </span>
<span id="cb78-93"><a href="logistic-regression.html#cb78-93" tabindex="-1"></a>                    high_income.reshape((n, <span class="dv">1</span>)), </span>
<span id="cb78-94"><a href="logistic-regression.html#cb78-94" tabindex="-1"></a>                    insurance.reshape((n, <span class="dv">1</span>)), </span>
<span id="cb78-95"><a href="logistic-regression.html#cb78-95" tabindex="-1"></a>                    stress.reshape((n, <span class="dv">1</span>))),</span>
<span id="cb78-96"><a href="logistic-regression.html#cb78-96" tabindex="-1"></a>                    axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb78-97"><a href="logistic-regression.html#cb78-97" tabindex="-1"></a>X <span class="op">=</span> add_dummy_feature(X) <span class="co"># this adds 1 in the first column (intercept)</span></span>
<span id="cb78-98"><a href="logistic-regression.html#cb78-98" tabindex="-1"></a>z <span class="op">=</span> X <span class="op">@</span> theta_true.reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb78-99"><a href="logistic-regression.html#cb78-99" tabindex="-1"></a>prob_logit <span class="op">=</span> sigma(z)  <span class="co"># output true probabilities</span></span>
<span id="cb78-100"><a href="logistic-regression.html#cb78-100" tabindex="-1"></a></span>
<span id="cb78-101"><a href="logistic-regression.html#cb78-101" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: PROBABILITIES COME FROM THE LOGISTIC FUNCTION. THIS IS THE KEY TO SIMULATE LOGISTIC REGRESSION.</span></span>
<span id="cb78-102"><a href="logistic-regression.html#cb78-102" tabindex="-1"></a><span class="co"># Step 1: Calculate linear combination (logit): z = X @ theta</span></span>
<span id="cb78-103"><a href="logistic-regression.html#cb78-103" tabindex="-1"></a><span class="co"># Step 2: Transform to probabilities: p = sigma(z) = 1/(1 + exp(-z))</span></span>
<span id="cb78-104"><a href="logistic-regression.html#cb78-104" tabindex="-1"></a><span class="co"># Step 3: Generate binary outcomes using these probabilities using a binomial dist.</span></span>
<span id="cb78-105"><a href="logistic-regression.html#cb78-105" tabindex="-1"></a></span>
<span id="cb78-106"><a href="logistic-regression.html#cb78-106" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb78-107"><a href="logistic-regression.html#cb78-107" tabindex="-1"></a>plt.hist(prob_logit, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&quot;steelblue&quot;</span>, edgecolor<span class="op">=</span><span class="st">&quot;black&quot;</span>)</span>
<span id="cb78-108"><a href="logistic-regression.html#cb78-108" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="dv">0</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb78-109"><a href="logistic-regression.html#cb78-109" tabindex="-1"></a>plt.title(<span class="st">&#39;True probabilities using true theta values&#39;</span>)</span>
<span id="cb78-110"><a href="logistic-regression.html#cb78-110" tabindex="-1"></a>plt.show()</span>
<span id="cb78-111"><a href="logistic-regression.html#cb78-111" tabindex="-1"></a></span>
<span id="cb78-112"><a href="logistic-regression.html#cb78-112" tabindex="-1"></a><span class="co"># generate health using probabilities</span></span>
<span id="cb78-113"><a href="logistic-regression.html#cb78-113" tabindex="-1"></a>health <span class="op">=</span> []</span>
<span id="cb78-114"><a href="logistic-regression.html#cb78-114" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb78-115"><a href="logistic-regression.html#cb78-115" tabindex="-1"></a>    health_i <span class="op">=</span> gen_health(prob_logit[i]) <span class="co"># AGAIN, THESE PROBABILITIES COME FROM THE LOGISTIC FUNCTION</span></span>
<span id="cb78-116"><a href="logistic-regression.html#cb78-116" tabindex="-1"></a>    health.append(health_i)</span>
<span id="cb78-117"><a href="logistic-regression.html#cb78-117" tabindex="-1"></a></span>
<span id="cb78-118"><a href="logistic-regression.html#cb78-118" tabindex="-1"></a>health <span class="op">=</span> np.array(health).ravel()</span></code></pre></div>
<pre><code>mean of college: 0.3981


share college for high income: 0.59654
share college for low income: 0.19966</code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_3_2.png" alt="png" />
<div class="figcaption">png</div>
</div>
<pre><code>average stress index for low income group: 3.5986
average stress index for high income group: 0.0088
fraction insured: 0.29812</code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_3_4.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>Since, this is a simulation, we know the true probabilities generated using the
true coefficients and the DGP. From a practitioner’s standpoint, we won’t know the
true probabilities since we don’t know the true DGP – we have to estimate
them.
Let’s print out our some summary measures on health.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="logistic-regression.html#cb81-1" tabindex="-1"></a><span class="co">#print(f&quot;y variable: {health} \n&quot;)</span></span>
<span id="cb81-2"><a href="logistic-regression.html#cb81-2" tabindex="-1"></a><span class="co">#print(f&quot;X matrix: {X}&quot;)</span></span>
<span id="cb81-3"><a href="logistic-regression.html#cb81-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;fraction with good health: </span><span class="sc">{</span>health<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb81-4"><a href="logistic-regression.html#cb81-4" tabindex="-1"></a></span>
<span id="cb81-5"><a href="logistic-regression.html#cb81-5" tabindex="-1"></a><span class="co"># create a stress band around the mean for no college, low income and uninsured</span></span>
<span id="cb81-6"><a href="logistic-regression.html#cb81-6" tabindex="-1"></a>mean_stress_baseline <span class="op">=</span> stress[(college<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> (high_income<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> (insurance<span class="op">==</span><span class="dv">0</span>)].mean()</span>
<span id="cb81-7"><a href="logistic-regression.html#cb81-7" tabindex="-1"></a>stress_tolerance <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># within ±0.5 of mean</span></span>
<span id="cb81-8"><a href="logistic-regression.html#cb81-8" tabindex="-1"></a>stress_band <span class="op">=</span> (np.<span class="bu">abs</span>(stress <span class="op">-</span> mean_stress_baseline) <span class="op">&lt;=</span> stress_tolerance)</span>
<span id="cb81-9"><a href="logistic-regression.html#cb81-9" tabindex="-1"></a></span>
<span id="cb81-10"><a href="logistic-regression.html#cb81-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;fraction with good health among no school, low income, and uninsured: </span><span class="sc">{</span>np<span class="sc">.</span>mean(health[(college<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> </span>
<span id="cb81-11"><a href="logistic-regression.html#cb81-11" tabindex="-1"></a>                                                                                               (high_income<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> </span>
<span id="cb81-12"><a href="logistic-regression.html#cb81-12" tabindex="-1"></a>                                                                                               (insurance<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> </span>
<span id="cb81-13"><a href="logistic-regression.html#cb81-13" tabindex="-1"></a>                                                                                               (stress_band)])<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>fraction with good health: 0.34883
fraction with good health among no school, low income, and uninsured: 0.0382</code></pre>
<p>The true <span class="math inline">\(\theta\)</span> values are <span class="math inline">\([\theta_0=0.3, \theta_1=0.1, \theta_2=0.2, \theta_3=0.05, \theta_4=-1]\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>0.3 is the intercept coefficient, representing people in no college,
low income, and uninsured group.</p></li>
<li><p>0.1 corresponds to college coefficient.</p></li>
<li><p>0.2 corresponds to high income coefficient.</p></li>
<li><p>0.05 corresponds to insurance coefficient.</p></li>
<li><p>-1 corresponds to stress coefficient.</p></li>
</ol>
<p>Note that 3.61 percent of people who have no schooling, are of low income, are uninsured and around the mean stress
index are in good health. This pertains to true <span class="math inline">\(\theta\)</span> of 0.3. Let’s convert this value
into probability using:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="logistic-regression.html#cb83-1" tabindex="-1"></a>p_gh <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span><span class="fl">0.3</span> <span class="op">+</span> np.mean(stress[(college<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> (high_income<span class="op">==</span><span class="dv">0</span>) <span class="op">&amp;</span> (insurance<span class="op">==</span><span class="dv">0</span>)])))</span>
<span id="cb83-2"><a href="logistic-regression.html#cb83-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;the conversion of theta = 0.3 + mean stress value to prob: </span><span class="sc">{</span>p_gh<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>the conversion of theta = 0.3 + mean stress value to prob: 0.0355 </code></pre>
<p>Notice that according to the DGP, around 3.55 percent of people in the population with no college,
low income, uninsured, and of the mean stress value are in good health.
This is close to what we have in our sample. Hence,
it is important to recognize that <span class="math inline">\(\theta\)</span> values are coefficients and in the case of logistic
regression; they are different from probabilities.</p>
<p><strong>Gradient Descent</strong></p>
<p>Let’s move on to the gradient descent and its usage in estimating <span class="math inline">\(\theta\)</span>.</p>
<p>Simply put, gradient is a vector of the partial derivatives of the loss function with respect to each
<span class="math inline">\(\theta\)</span> stacked together.</p>
<p><span class="math display">\[
\text{gradient} = \begin{bmatrix} \frac{\partial C}{\partial \theta_0} \\ \frac{\partial C}{\partial \theta_1} \\ \frac{\partial C}{\partial \theta_2} \\ \frac{\partial C}{\partial \theta_3} \end{bmatrix}
\]</span></p>
<p>Before we get to the gradient of the logistic kind, let’s stack the loss function using matrices:</p>
<p><span class="math display">\[C = -\frac{1}{n} [Y^{t} \log(p) + (1-Y^{t}) \log(1- p)]\]</span></p>
<p>Replacing <span class="math inline">\(p= \sigma(\theta X)\)</span>, we have:</p>
<p><span class="math display">\[C = -\frac{1}{n} [Y^{t} \log(\sigma(X\theta)) + (1-Y^{t}) \log(1-\sigma(X\theta)]\]</span></p>
<p><span class="math inline">\(C\)</span> will be a scalar.
Next, get <span class="math inline">\(\frac{\partial C}{\partial \theta}\)</span>.</p>
<p>But first, here are the dimensions of terms in the RHS:</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(X: (n\times 5)\)</span></p></li>
<li><p><span class="math inline">\(Y^{T}: (1\times n)\)</span></p></li>
<li><p><span class="math inline">\(\theta : (5\times 1)\)</span></p></li>
<li><p><span class="math inline">\(X\theta : (n\times 1)\)</span></p></li>
<li><p><span class="math inline">\(Y^{t} \log(\sigma(X\theta)): scalar\)</span></p></li>
</ol>
<p>Taking the partial derivative of the newly formatted cost function <span class="math inline">\(C\)</span> with
respect to <span class="math inline">\(\theta\)</span>, you get the gradient vector
as follows:</p>
<p><span class="math inline">\(\frac{\partial C}{\partial \theta} = \frac{1}{n} X^{T}(\sigma(X \theta) - Y)\)</span>.</p>
<p>where, <span class="math inline">\(X^{T}\)</span> is a <span class="math inline">\(5\times n\)</span> matrix and <span class="math inline">\((X^{T}\sigma(X \theta) - Y)\)</span> is a <span class="math inline">\(n \times 1\)</span> matrix.
I solved for the partial using
the brute force chain rule. One thing to note while solving is a small trick below:</p>
<p><span class="math inline">\(\frac{exp(\theta X)}{1 + exp(\theta X)} = \frac{1 + exp(\theta X) -1}{1 + exp(\theta X)}\)</span>. This results to:
<span class="math inline">\(1 - \frac{1}{1 + exp(\theta X)} = 1 - \sigma(\theta X)\)</span>.</p>
<p>This is getting into minute little details. You can escape this
and just take the word for the gradient or you could try it all out. Upto you!</p>
<p>Now that we are through with all this, the gradient descent algorithm is straight forward.</p>
<p><strong>Gradient Descent Algorithm</strong></p>
<ol style="list-style-type: decimal">
<li><p>Initialize the <span class="math inline">\(\theta_{gd}\)</span> values. I’ve used values from the normal distribution.</p></li>
<li><p>Initialize the learning rate – <span class="math inline">\(\eta\)</span> and the number of interations (epochs). We set <span class="math inline">\(\eta = 0.5\)</span> and epochs=50.</p></li>
<li><p>Compute the gradient. Call this <span class="math inline">\(gd_i\)</span>.</p></li>
<li><p>Adjust theta using the gradient and the learning rate as: <span class="math inline">\(\theta_{gd} = \theta_{gd} - \eta \times gd_i\)</span>. Note that
we have to move against the gradient; hence, the negative.</p></li>
<li><p>Iterate steps 3 and 4 for <span class="math inline">\(iter=epochs\)</span> number of times or until the algorithm converges.</p></li>
</ol>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="logistic-regression.html#cb85-1" tabindex="-1"></a><span class="co"># Gradient Descent </span></span>
<span id="cb85-2"><a href="logistic-regression.html#cb85-2" tabindex="-1"></a>theta_gd <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">5</span>)    <span class="co"># initial theta values from the normal dist.</span></span>
<span id="cb85-3"><a href="logistic-regression.html#cb85-3" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">1e-15</span>                         <span class="co"># to prevent overflow coming from logit values close to 0.</span></span>
<span id="cb85-4"><a href="logistic-regression.html#cb85-4" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">50</span>                           <span class="co"># number of iterations</span></span>
<span id="cb85-5"><a href="logistic-regression.html#cb85-5" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.5</span>                               <span class="co"># learning rate</span></span>
<span id="cb85-6"><a href="logistic-regression.html#cb85-6" tabindex="-1"></a>loss <span class="op">=</span> []</span>
<span id="cb85-7"><a href="logistic-regression.html#cb85-7" tabindex="-1"></a></span>
<span id="cb85-8"><a href="logistic-regression.html#cb85-8" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb85-9"><a href="logistic-regression.html#cb85-9" tabindex="-1"></a></span>
<span id="cb85-10"><a href="logistic-regression.html#cb85-10" tabindex="-1"></a>    z <span class="op">=</span> np.clip(X <span class="op">@</span> theta_gd.reshape((<span class="dv">5</span>, <span class="dv">1</span>)), <span class="op">-</span><span class="dv">500</span>, <span class="dv">500</span>)</span>
<span id="cb85-11"><a href="logistic-regression.html#cb85-11" tabindex="-1"></a>    gradient_i <span class="op">=</span> ((sigma(z) <span class="op">-</span> health.reshape((n, <span class="dv">1</span>))).transpose() <span class="op">@</span> X) <span class="op">/</span> n <span class="co"># caculate the gradient</span></span>
<span id="cb85-12"><a href="logistic-regression.html#cb85-12" tabindex="-1"></a>    theta_gd <span class="op">=</span> theta_gd <span class="op">-</span> eta<span class="op">*</span>gradient_i                                   <span class="co"># adjust theta by moving opposite to the gradient</span></span>
<span id="cb85-13"><a href="logistic-regression.html#cb85-13" tabindex="-1"></a>    loss_i <span class="op">=</span> np.mean(health<span class="op">*</span>(<span class="op">-</span>np.log(sigma(z)<span class="op">+</span>epsilon).ravel()) <span class="op">+</span>          <span class="co"># calculate loss</span></span>
<span id="cb85-14"><a href="logistic-regression.html#cb85-14" tabindex="-1"></a>                    (<span class="dv">1</span><span class="op">-</span>health)<span class="op">*</span>(<span class="op">-</span>np.log(<span class="dv">1</span><span class="op">-</span>sigma(z)<span class="op">+</span>epsilon).ravel()))</span>
<span id="cb85-15"><a href="logistic-regression.html#cb85-15" tabindex="-1"></a>    loss.append(loss_i)                                                     <span class="co"># append loss</span></span>
<span id="cb85-16"><a href="logistic-regression.html#cb85-16" tabindex="-1"></a></span>
<span id="cb85-17"><a href="logistic-regression.html#cb85-17" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;theta estimates from gradient descent: </span><span class="sc">{</span>theta_gd<span class="sc">.</span><span class="bu">round</span>(<span class="dv">4</span>)<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb85-18"><a href="logistic-regression.html#cb85-18" tabindex="-1"></a></span>
<span id="cb85-19"><a href="logistic-regression.html#cb85-19" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb85-20"><a href="logistic-regression.html#cb85-20" tabindex="-1"></a>plt.plot(np.linspace(<span class="dv">1</span>, epochs, epochs), np.array(loss).ravel())</span>
<span id="cb85-21"><a href="logistic-regression.html#cb85-21" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;epochs&#39;</span>)</span>
<span id="cb85-22"><a href="logistic-regression.html#cb85-22" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;cross-entropy loss&#39;</span>)</span>
<span id="cb85-23"><a href="logistic-regression.html#cb85-23" tabindex="-1"></a>plt.title(<span class="st">&quot;Loss with respect to iterations&quot;</span>)</span>
<span id="cb85-24"><a href="logistic-regression.html#cb85-24" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>theta estimates from gradient descent: [[ 0.06   -0.0542  0.5818 -0.0198 -0.9164]] 
 </code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_9_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>We’ve now estimated the <span class="math inline">\(\theta\)</span> using gradient descent. Let’s check our results using the
in-built library in sklearn that estimates the Logistic Regression.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="logistic-regression.html#cb87-1" tabindex="-1"></a><span class="co"># compare estimates from sklearn</span></span>
<span id="cb87-2"><a href="logistic-regression.html#cb87-2" tabindex="-1"></a>mod <span class="op">=</span> LogisticRegression(max_iter<span class="op">=</span>epochs, fit_intercept<span class="op">=</span><span class="va">False</span>, C<span class="op">=</span>np.inf)</span>
<span id="cb87-3"><a href="logistic-regression.html#cb87-3" tabindex="-1"></a>mod_fit <span class="op">=</span> mod.fit(X, health)</span>
<span id="cb87-4"><a href="logistic-regression.html#cb87-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Estimates from sklearn: </span><span class="sc">{</span>mod_fit<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb87-5"><a href="logistic-regression.html#cb87-5" tabindex="-1"></a></span>
<span id="cb87-6"><a href="logistic-regression.html#cb87-6" tabindex="-1"></a>results <span class="op">=</span> {</span>
<span id="cb87-7"><a href="logistic-regression.html#cb87-7" tabindex="-1"></a>            <span class="st">&quot;GD&quot;</span>: theta_gd.ravel(),</span>
<span id="cb87-8"><a href="logistic-regression.html#cb87-8" tabindex="-1"></a>            <span class="st">&quot;SK&quot;</span>: mod_fit.coef_.ravel()</span>
<span id="cb87-9"><a href="logistic-regression.html#cb87-9" tabindex="-1"></a>}</span>
<span id="cb87-10"><a href="logistic-regression.html#cb87-10" tabindex="-1"></a></span>
<span id="cb87-11"><a href="logistic-regression.html#cb87-11" tabindex="-1"></a>pd.DataFrame(results)</span></code></pre></div>
<pre><code>Estimates from sklearn: [[ 0.29600819  0.09481312  0.22279611  0.03175326 -0.98693222]]


/home/vinish/Dropbox/Machine Learning/myenv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1170: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters
  warnings.warn(</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
GD
</th>
<th>
SK
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.060008
</td>
<td>
0.296008
</td>
</tr>
<tr>
<th>
1
</th>
<td>
-0.054172
</td>
<td>
0.094813
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.581845
</td>
<td>
0.222796
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-0.019761
</td>
<td>
0.031753
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-0.916444
</td>
<td>
-0.986932
</td>
</tr>
</tbody>
</table>
</div>
<p>Let’s estimate the model using the LPM – note that this is a wrong functional form at use.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="logistic-regression.html#cb89-1" tabindex="-1"></a><span class="co"># linear regression</span></span>
<span id="cb89-2"><a href="logistic-regression.html#cb89-2" tabindex="-1"></a>mod_linear <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">False</span>) <span class="co"># we dont want to double fit the intercept; X already contains it </span></span>
<span id="cb89-3"><a href="logistic-regression.html#cb89-3" tabindex="-1"></a>mod_linear <span class="op">=</span> mod_linear.fit(X, health)</span>
<span id="cb89-4"><a href="logistic-regression.html#cb89-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Estimates from linear reg: </span><span class="sc">{</span>mod_linear<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>Estimates from linear reg: [ 0.44482771  0.01377842  0.16374733  0.00466073 -0.10242624]</code></pre>
<p>The estimates from the gradient descent and sklearn are virtually similar. Note that <span class="math inline">\(\theta\)</span> estimates
aren’t marginal effects as they are in the LPM setup. Recall, in the case of logistic regression:
<span class="math inline">\(\hat{p} = \frac{1}{(1 + exp(-\theta X))}\)</span>. Hence, we need to translate <span class="math inline">\(\theta\)</span> into marginal effects
before comparing them with LPM’s estimates. Calculation of marginal effect needs to be with respect to a benchmark.
We consider person A: with no college, low income, uninsured, and stress level around the mean (for the group with
no college, uninsured, and low income) as this benchmark person and the marginal effects
are computed with respect to this person.</p>
<p>The following code translates <span class="math inline">\(\theta\)</span> into marginal effect.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="logistic-regression.html#cb91-1" tabindex="-1"></a><span class="kw">def</span> fun_me(theta_vals, person):</span>
<span id="cb91-2"><a href="logistic-regression.html#cb91-2" tabindex="-1"></a>    logit <span class="op">=</span> (theta_vals <span class="op">@</span> person).ravel()</span>
<span id="cb91-3"><a href="logistic-regression.html#cb91-3" tabindex="-1"></a>    p <span class="op">=</span> sigma(logit)</span>
<span id="cb91-4"><a href="logistic-regression.html#cb91-4" tabindex="-1"></a>    <span class="cf">return</span> p</span>
<span id="cb91-5"><a href="logistic-regression.html#cb91-5" tabindex="-1"></a></span>
<span id="cb91-6"><a href="logistic-regression.html#cb91-6" tabindex="-1"></a><span class="co"># create person A: without college, low income, and uninsured   </span></span>
<span id="cb91-7"><a href="logistic-regression.html#cb91-7" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: This will be our benchmark person.</span></span>
<span id="cb91-8"><a href="logistic-regression.html#cb91-8" tabindex="-1"></a>person_A <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, mean_stress_baseline]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb91-9"><a href="logistic-regression.html#cb91-9" tabindex="-1"></a>prob_health_A <span class="op">=</span> fun_me(theta_gd, person_A)</span>
<span id="cb91-10"><a href="logistic-regression.html#cb91-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The probability that person A is in good health is:</span><span class="sc">{</span>prob_health_A<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb91-11"><a href="logistic-regression.html#cb91-11" tabindex="-1"></a></span>
<span id="cb91-12"><a href="logistic-regression.html#cb91-12" tabindex="-1"></a><span class="co"># Person B: with college but low income and uninsured</span></span>
<span id="cb91-13"><a href="logistic-regression.html#cb91-13" tabindex="-1"></a>person_B <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, mean_stress_baseline]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb91-14"><a href="logistic-regression.html#cb91-14" tabindex="-1"></a>prob_health_B <span class="op">=</span> fun_me(theta_gd, person_B)</span>
<span id="cb91-15"><a href="logistic-regression.html#cb91-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;The probability that person B is in good health is: </span><span class="sc">{</span>prob_health_B<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb91-16"><a href="logistic-regression.html#cb91-16" tabindex="-1"></a></span>
<span id="cb91-17"><a href="logistic-regression.html#cb91-17" tabindex="-1"></a><span class="co"># Person C: with high income but without college and uninsured</span></span>
<span id="cb91-18"><a href="logistic-regression.html#cb91-18" tabindex="-1"></a>person_C <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, mean_stress_baseline]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb91-19"><a href="logistic-regression.html#cb91-19" tabindex="-1"></a>prob_health_C <span class="op">=</span> fun_me(theta_gd, person_C)</span>
<span id="cb91-20"><a href="logistic-regression.html#cb91-20" tabindex="-1"></a><span class="bu">print</span>(prob_health_C)</span>
<span id="cb91-21"><a href="logistic-regression.html#cb91-21" tabindex="-1"></a></span>
<span id="cb91-22"><a href="logistic-regression.html#cb91-22" tabindex="-1"></a><span class="co"># Person D: with insurance but without college and low income</span></span>
<span id="cb91-23"><a href="logistic-regression.html#cb91-23" tabindex="-1"></a>person_D <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, mean_stress_baseline]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb91-24"><a href="logistic-regression.html#cb91-24" tabindex="-1"></a>prob_health_D <span class="op">=</span> fun_me(theta_gd, person_D)</span>
<span id="cb91-25"><a href="logistic-regression.html#cb91-25" tabindex="-1"></a><span class="bu">print</span>(prob_health_D)</span>
<span id="cb91-26"><a href="logistic-regression.html#cb91-26" tabindex="-1"></a></span>
<span id="cb91-27"><a href="logistic-regression.html#cb91-27" tabindex="-1"></a><span class="co"># Person E: Same as Person A but one unit increase in stress for marginal effects</span></span>
<span id="cb91-28"><a href="logistic-regression.html#cb91-28" tabindex="-1"></a>person_E <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, mean_stress_baseline <span class="op">+</span> <span class="dv">1</span>]).reshape((<span class="dv">5</span>, <span class="dv">1</span>))</span>
<span id="cb91-29"><a href="logistic-regression.html#cb91-29" tabindex="-1"></a>prob_health_E <span class="op">=</span> fun_me(theta_gd, person_E)</span>
<span id="cb91-30"><a href="logistic-regression.html#cb91-30" tabindex="-1"></a><span class="bu">print</span>(prob_health_E)</span></code></pre></div>
<pre><code>The probability that person A is in good health is:[0.0376653] 

The probability that person B is in good health is: [0.03575017] 

[0.06545006]
[0.03695553]
[0.01541215]</code></pre>
<p>Since we are using Person A as the benchmark, we can compute marginal probabilities simply by
substracting probabilities.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="logistic-regression.html#cb93-1" tabindex="-1"></a>me_B_A <span class="op">=</span> prob_health_B <span class="op">-</span> prob_health_A </span>
<span id="cb93-2"><a href="logistic-regression.html#cb93-2" tabindex="-1"></a>me_C_A <span class="op">=</span> prob_health_C <span class="op">-</span> prob_health_A </span>
<span id="cb93-3"><a href="logistic-regression.html#cb93-3" tabindex="-1"></a>me_D_A <span class="op">=</span> prob_health_D <span class="op">-</span> prob_health_A</span>
<span id="cb93-4"><a href="logistic-regression.html#cb93-4" tabindex="-1"></a>me_E_A <span class="op">=</span> prob_health_E <span class="op">-</span> prob_health_A</span>
<span id="cb93-5"><a href="logistic-regression.html#cb93-5" tabindex="-1"></a></span>
<span id="cb93-6"><a href="logistic-regression.html#cb93-6" tabindex="-1"></a>me <span class="op">=</span> np.array([prob_health_A, me_B_A, me_C_A, me_D_A, me_E_A])</span>
<span id="cb93-7"><a href="logistic-regression.html#cb93-7" tabindex="-1"></a></span>
<span id="cb93-8"><a href="logistic-regression.html#cb93-8" tabindex="-1"></a>results_me_lpm <span class="op">=</span> {</span>
<span id="cb93-9"><a href="logistic-regression.html#cb93-9" tabindex="-1"></a>                    <span class="st">&quot;ME:logistic&quot;</span>: me.ravel().<span class="bu">round</span>(<span class="dv">3</span>),</span>
<span id="cb93-10"><a href="logistic-regression.html#cb93-10" tabindex="-1"></a>                    <span class="st">&quot;ME:LPM&quot;</span>: mod_linear.coef_.ravel().<span class="bu">round</span>(<span class="dv">3</span>)</span>
<span id="cb93-11"><a href="logistic-regression.html#cb93-11" tabindex="-1"></a>}</span>
<span id="cb93-12"><a href="logistic-regression.html#cb93-12" tabindex="-1"></a></span>
<span id="cb93-13"><a href="logistic-regression.html#cb93-13" tabindex="-1"></a>pd.DataFrame(results_me_lpm)</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
ME:logistic
</th>
<th>
ME:LPM
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.038
</td>
<td>
0.445
</td>
</tr>
<tr>
<th>
1
</th>
<td>
-0.002
</td>
<td>
0.014
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.028
</td>
<td>
0.164
</td>
</tr>
<tr>
<th>
3
</th>
<td>
-0.001
</td>
<td>
0.005
</td>
</tr>
<tr>
<th>
4
</th>
<td>
-0.022
</td>
<td>
-0.102
</td>
</tr>
</tbody>
</table>
</div>
<p>The marginal effects from LPM and logistic regression are shown in the table.
Let’s take a look at predicted probabilities from both LPM and logistic regression models:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="logistic-regression.html#cb94-1" tabindex="-1"></a><span class="co"># prediction from LPM</span></span>
<span id="cb94-2"><a href="logistic-regression.html#cb94-2" tabindex="-1"></a>prob_linear <span class="op">=</span> mod_linear.predict(X) <span class="co"># LPM directly gives probabilites</span></span>
<span id="cb94-3"><a href="logistic-regression.html#cb94-3" tabindex="-1"></a><span class="bu">print</span>(prob_linear) </span>
<span id="cb94-4"><a href="logistic-regression.html#cb94-4" tabindex="-1"></a></span>
<span id="cb94-5"><a href="logistic-regression.html#cb94-5" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb94-6"><a href="logistic-regression.html#cb94-6" tabindex="-1"></a>plt.hist(prob_linear, bins <span class="op">=</span> <span class="dv">30</span>, color<span class="op">=</span><span class="st">&quot;red&quot;</span>, edgecolor<span class="op">=</span><span class="st">&quot;black&quot;</span>, alpha<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb94-7"><a href="logistic-regression.html#cb94-7" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;probability&#39;</span>)</span>
<span id="cb94-8"><a href="logistic-regression.html#cb94-8" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;frequency&#39;</span>)</span>
<span id="cb94-9"><a href="logistic-regression.html#cb94-9" tabindex="-1"></a>plt.title(<span class="st">&#39;Predicted probabilities from LPM&#39;</span>)</span>
<span id="cb94-10"><a href="logistic-regression.html#cb94-10" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">&quot;--&quot;</span>)</span>
<span id="cb94-11"><a href="logistic-regression.html#cb94-11" tabindex="-1"></a>plt.show()</span>
<span id="cb94-12"><a href="logistic-regression.html#cb94-12" tabindex="-1"></a></span>
<span id="cb94-13"><a href="logistic-regression.html#cb94-13" tabindex="-1"></a><span class="co"># predictions from logistic</span></span>
<span id="cb94-14"><a href="logistic-regression.html#cb94-14" tabindex="-1"></a>prob_logit_predict <span class="op">=</span> mod_fit.predict_proba(X) <span class="co"># need to call .predict_proba() to get probabilities from logistic model</span></span>
<span id="cb94-15"><a href="logistic-regression.html#cb94-15" tabindex="-1"></a></span>
<span id="cb94-16"><a href="logistic-regression.html#cb94-16" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">5</span>))</span>
<span id="cb94-17"><a href="logistic-regression.html#cb94-17" tabindex="-1"></a>plt.hist(prob_logit_predict[:,<span class="dv">1</span>], bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">&quot;steelblue&quot;</span>, edgecolor<span class="op">=</span><span class="st">&quot;black&quot;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb94-18"><a href="logistic-regression.html#cb94-18" tabindex="-1"></a><span class="co">#plt.hist(prob_logit, bins=30, color=&quot;red&quot;, edgecolor=&quot;black&quot;, alpha=0.3)</span></span>
<span id="cb94-19"><a href="logistic-regression.html#cb94-19" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;probability&#39;</span>)</span>
<span id="cb94-20"><a href="logistic-regression.html#cb94-20" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;frequency&#39;</span>)</span>
<span id="cb94-21"><a href="logistic-regression.html#cb94-21" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>)</span>
<span id="cb94-22"><a href="logistic-regression.html#cb94-22" tabindex="-1"></a>plt.title(<span class="st">&#39;Predicted probabilities from Logistic Regression&#39;</span>)</span>
<span id="cb94-23"><a href="logistic-regression.html#cb94-23" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>[ 0.73310663 -0.06638304  0.21153432 ...  0.69453787  0.76647622
  0.6886302 ]</code></pre>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_19_1.png" alt="png" />
<div class="figcaption">png</div>
</div>
<div class="float">
<img src="Lecture2_Logistic_files/Lecture2_Logistic_19_2.png" alt="png" />
<div class="figcaption">png</div>
</div>
<p>We see that the predicted probabilities from LPM are negative (this can’t happen), whereas those from the logistic
model closely mimic the actual probabilities. Hence, if the goal is to attain probabilities then logistic regression
is clearly better than LPM.</p>
<p><strong>LPM vs Logistic Regression</strong></p>
<p>From a practitioner’s perspective, one can get by using LPM if the goal is to infer causality alone and you
aren’t concerned about predicting probabilities. It is simple, easy to interpret, and will require low computational power.
It does mean that you should, but you can get by. However, if the goal is to predict, say probability of the binary outcome,
then LPM is a no-go.</p>
<p>In the world of causality, the importance of estimating the probability of someone being treated (vs untreated) cannot be overstated.
We know this as propensity scores. Logistic regression can be a good starting model while estimating propensity scores.</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-and-gradient-descent.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="causal-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  },
  "toolbar": {
    "position": "static"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
