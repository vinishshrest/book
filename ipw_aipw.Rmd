---
title: "IPW and AIPW"
output:
  html_document:
    css: custom.css
    includes:
      in_header: header.html
header-includes:
   - \usepackage{amsmath}
---



# IPW and AIPW

```{r, echo = FALSE, include = FALSE}
#################################
#
# Author: VS
# Last Revised: Jan 18, 2024
# Keywords: Heterogeneous Treatment Effects
# using Lasso, GRF
#
#################################

rm(list = ls())

# making table
library("kableExtra")

# Helper packages
library(dplyr) # for data wrangling
library(ggplot2) # for graphics
library(splines)

# Modeling packages 
library(ranger)
library(caret) # for automating the tuning process
library(grf)
library(glmnet) # for implementing regularized regression
```


The target is to estimate the average treatment effect (ATE): 

\begin{equation}
\label{eq:ATE}
ATE = E[Y_i(1) - Y_i(0)]  (\#eq:ate)
\end{equation}

Note that using the following two assumptions: 

i) $W_i \perp \{Y_i(0), \; Y_i(1)\}$ (independence assumption)

ii) $Y_i(W) = Y_i$ (SUTVA)

the ATE estimate $\hat{\tau}$ can be written as the difference-in-means estimator: 

\begin{equation}
\label{eq:ATE_estimator}
\hat{\tau} = \frac{1}{N_T} \sum_{W_i = 1} Y_i - \frac{1}{N_C} \sum_{i \in W_i = 0} Y_i 
\end{equation}

where $N_T$ and $N_C$ are the number of treated and control units, respectively. 


In the previous lecture, we disscussed randomized control trial as an ideal approach to estimate ATE. 
In a randomized controlled trial each unit has an equal probability of receiving the treatment. This means the following: 

\begin{equation}
P(W_i = 1 \; | \;  Y_i(0), \;  Y_i(1),  \;  n_T) = \frac{n_T}{n}, \; \; i = \{1, ...., n\} (\#eq:ptreat)
\end{equation}



In equation \@ref(eq:ptreat), $n_T$ refers to the number of units that receives the treatment.^[Although it is generally recommended to assign half of the sample to the treatment group and the other half to the control group, this is not a strict requirement.] 
In an easy to understand set-up, if a researcher wants $P(W_i = 1) = 0.5$ (unit is equally likely to be treated or untreated), a coin flip can feasibly be used as a mechanism to assign treatment.^[Of course, this is quicky going to be inefficient as the sample size increases. In general, treatment assignment is determinted by a statistical process via a software. For example, if a researcher wants about one-third of the sample treated then a bernoulli trial with the probability of success of 0.33 can be used.] 


Although randomized controlled trials (RCTs) are often considered the gold standard in causal inference, they cannot always be used due to ethical, moral, and monetary reasons. Returning to the example we used in the previous chapter, it is not ethical to demarcate who can attend the tutoring session versus who cannot. In real-world scenarios, tutoring sessions are typically voluntary. Students who regularly attend these sessions may have different baseline (pre-treatment) characteristics compared to those who do not attend. These differences can introduce biases that complicate causal inference in observational studies.


To proceed further in observational setting (without using RCTs), we require more knowledge about the treatment assignment. In other words, we need to understand which variables determine who attends the tutoring sessions. This information is crucial for identifying potential confounders and for applying methods that can help estimate causal effects in observational settings. *In causal inference, confounders are variables that are associated with both the treatment and the outcome. They can introduce bias in the estimation of the causal effect of the treatment on the outcome by providing alternative explanations for any observed relationships.* For example, say you are trying to evaluate the efficacy of a new drug on blood pressure level. If smokers are more likey to get treated and if they tend to have higher blood pressure to begin with, the treatment effects are likely to be understated.  


This brings us to the **unconfoundedness** assumption. 


**Unconfoundedness**: The treatment assignment is as good as random once we control for $X$s. 

\begin{equation}
\{W_i \perp \{Y_i(0), \; Y_i(1)\} | X_i \} \; for \; all \; x \in \chi. (\#eq:uncon)
\end{equation}   

As with the tutoring example, the independence assumption (discussed in the previous chapter) is highly unlikely to hold in observational settings. Let's consider the following scenarios:

a. Out of the ten states that are yet to expand Medicaid, eight fall in South. Medicaid expansion is not random.
b. Cigarette taxes are higher in states with higher anti-smoking sentiments. 
c. Infrastructure development, such as construction of roads, schools, hopitals, are demand-driven. 
d. The list goes on ..


However, if we manage to observe all the $X$s (covariates) that influence the treatment, we can invoke unconfoundedness for causal inference.

## A simple example 

Say, you are interested in evaluating the effect of tutoring program initiated following the first exam on grades at an introductory level course. For simplicity, the possible grades are **A** and **B**. However, students who received B on their first exam are more likely to attend the tutoring session. In other words, $P(W_i = 1 | Y_{iFE} = A) < P(W_i = 1 | Y_{iFE} = B)$ ($Y_{iFE}$ is read as unit $i's$ grade in the first exam). In this case, the treatment assignment is correlated with the past grade, which can predict the grade on the second exam. In other words, if you did well in the first exam, you are likely to perform well in the second exam and so on. Hence, using equation (2) to estimate effects of the tutoring program will result in biased estimate. 

Since we know that the probability of treatment is influenced by the grade on the first exam, we can estimate the conditional average treatment effect (CATE) and average them using weights to form an estimate of ATE. Let's take a look at the data. 

```{r}

# function to report grade breakdown by the first exam grade (A and B)
grade_mat  <- function(grade_vec){
    grade  <- matrix(0, nrow =2, ncol = 3)
    grade[, 1]  <- c("Treat", "Control")
    grade[ ,2]  <- c(grade_vec[1], grade_vec[2])
    grade[ ,3]  <- c(grade_vec[3], grade_vec[4])
    return(grade)
}

# Y_iFS == A
grade  <- grade_mat(c(5, 9, 2, 4))
colnames(grade)  <- c(" ", "A (2nd Exam)", "B (2nd Exam)")

# Se
grade  %>% kable()  %>% 
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left")  %>% 
    add_header_above(c("Table 1." = 1, "Grade in the 2nd exam | 1st exam = A" = 2))

grade  <- grade_mat(c(15, 1, 5, 4))
colnames(grade)  <- c(" ", "A (2nd Exam)", "B (2nd Exam)")

grade  %>% kable()  %>% 
    kable_styling(bootstrap_options = "striped", full_width = F, position = "left")  %>% 
    add_header_above(c("Table 2." = 1, "Grade in the 2nd exam | 1st exam = B" = 2))
```


$~$
$~$

**Estimation**

$\hat{\tau}_{FE=A} = \frac{5}{7} - \frac{9}{13} = 2.1 \; pp$

$\hat{\tau}_{FE=B} = \frac{15}{20} - \frac{1}{5} = 55 \; pp$

$\hat{\tau}_{AGG} = \frac{20}{45} \hat{\tau}_{FE=A} - \frac{25}{45} \hat{\tau}_{FE=B} = 31.48 \; pp$. 

The first two are CATEs for the group that recived A and B in the first exam. The assumption is that once conditioned on the grade in the first exam, treatment (who attends vs. who doesn't) is random. This allows valid estimation of within group causal effects, which are then averaged to form ATE using appropriate weights on the third line. This simple example using the discrete feature space (grade in the first exam can be A or B) provides intuition that if variables influencing the treatment assignment are observed then ATE estimate can be uncovered by taking weighted average of CATE estimates (these are also group-wise ATE).^[In this case, CATEs are different across the two sub-groups. Sometimes the core interest of analysis can be uncovering the heterogeneous treatment effects, which motivates estimation and inference on CATEs across two or more sub-groups.]


## Aggregated Estimator 

As we saw in Section 1, aggregated estimator can be used in when we know what exact variables are determining the treatment assignment. We will consider the discrete case, when a discrete covariate determines the probability of treatment for simplicity. Note that students who received B on their first exam are more likely to attend the tutoring session in our **simple example**. So grade in the first exam (A or B for simplicity) determines the treatment here. 

**Setup.** $Y_i$ is the outcome variable; final exam score. $W_i$ is the treatment variable; whether a student attended the tutoring session. $X_i$ is the covariate; a student's grade on the first exam. We want to evaluate the effects of attending a tutoring program on a student's exam score.  

What we would want to do is to condition on the first exam's grade, estimate the treatment effects, and aggregate it. 

The aggregated estimator is given as: 

\begin{align}

\hat{\tau}_{AGG} = \frac{n_{A}}{n} \bigg[\frac{1}{n_{A1}} \underbrace{\sum}_{\substack{i \in A \\ \textrm{W = 1}}} Y_i -   
                    \frac{1}{n_{A0}} \underbrace{\sum}_{\substack{i \in A \\ \textrm{W = 0}}} Y_i \bigg] + 
\frac{n_{B}}{n} \bigg[\frac{1}{n_{B1}} \underbrace{\sum}_{\substack{i \in B \\ \textrm{W = 1}}} Y_i -   
                    \frac{1}{n_{B0}} \underbrace{\sum}_{\substack{i \in B \\ \textrm{W = 0}}} Y_i \bigg] (\#eq:AGG0)

\end{align}

Note that the first block is the treatment effect for those who received A on their first exam, whereas the second block represents 
treatment effect for those who received B. After a few steps of simple algebra, equation \@ref{eq:AGG0} can be written as: 

\begin{equation}
= \frac{1}{n} \bigg[ \frac{1}{\frac{n_{A1}}{n_{A}}} \underbrace{\sum}_{\substack{i \in A}} Y_i \times W_i -   
                    \frac{1}{\frac{n_{A0}}{n_{A}}} \underbrace{\sum}_{\substack{i \in A}} Y_i \times (1 - W_i) \bigg] + \\
\frac{1}{n} \bigg[ \frac{1}{\frac{n_{B1}}{n_{B}}} \underbrace{\sum}_{\substack{i \in B}} Y_i \times W_i -   
                    \frac{1}{\frac{n_{B0}}{n_{B}}} \underbrace{\sum}_{\substack{i \in B}} Y_i \times (1 - W_i) \bigg] (\#eq:AGG)
\end{equation}

Equation \@ref(eq:AGG) looks super complicated, buts its not. Let's breakdown the components of it: 

1. $\frac{n_{A1}}{n_A}:$ Represents the fraction of treated individuals who received $A$ on the first exam. 

2. $\frac{n_{A0}}{n_A}:$ Represents the fraction of untreated individuals who received $A$ on the first exam. 

3. $\frac{n_{B1}}{n_B}:$ Represents the fraction of treated individuals who received $B$ on the first exam. 

4. $\frac{n_{B0}}{n_B}:$ Represents the fraction of untreated individuals who received $B$ on the first exam. 


Note that $\frac{n_{A1}}{n_A} = e(X = A)$ and $\frac{n_{A0}}{n_A} = 1 - e(X = A)$ and the same goes with the segment composed of those who received B on their first exam. 

Equation \@ref(eq:AGG) can be further simplified as: 

\begin{equation}
\hat{\tau}_{AGG} = \frac{1}{n}\bigg[ \sum \frac{Y_i \times W_i}{e(X)} - \sum \frac{Y_i \times (1-W_i)}{1 - e(X)} \bigg] (\#eq:AGG2)
\end{equation}

Equation \@ref{eq:AGG2} takes the form of an **inverse probability-weighted estimator**.
We'll find out that Aggregate Estimator $(\hat{\tau}_{AGG})$ is a special case of Inverse Probability Weighting later on in this section.

```{r}
n <- 2000
p <- 10
true_effect  <-  10

set.seed  <- 12570
agg.means  <- replicate(1000, {
X <- matrix(rnorm(n * p), n, p)
X.test <- matrix(0, 101, p)
X.test[, 1] <- seq(-2, 2, length.out = 101)
prob  <- 1 / (1 + exp(- (X[, 1] < 1 + rnorm(n))))
W <- rbinom(n, 1, prob)
Y <- 60 + true_effect * W + 5 * pmax(X[, 1], 1) + rnorm(n)
group  <- ifelse( X[, 1] < 1, "A", "B" )

# estimates for those receiving A and B on the first exam
att.A  <- mean(Y[group == "A" & W == 1]) - mean(Y[group == "A" & W == 0])
att.B  <- mean(Y[group == "B" & W == 1]) - mean(Y[group == "B" & W == 0])

prop.A  <- mean(group == "A")
prop.B  <- mean(group == "B")

prop.A * att.A + prop.B * att.B

}
)

# plot(X[, 1], X[, 2], col = as.factor(W))

hist(agg.means, freq = F, main = "", col= rgb(0, 0, 1, 1/8), xlab = "ATT estimates", las = 1)
abline(v = true_effect, lwd = 3, lty = 2)
legend("topright", "True effect", lwd = 3, lty = 2, bty = "n")

paste("mean of AGG estimates: ", round(mean(agg.means), 3))
paste("standard error of AGG estimates: ", round(sd(agg.means), 3))

```

Let's compare this to a naive estimator. 

```{r}
set.seed  <- 12570
naive.means  <- replicate(1000, {

X <- matrix(rnorm(n * p), n, p)
X.test <- matrix(0, 101, p)
X.test[, 1] <- seq(-2, 2, length.out = 101)

prob  <- 1 / (1 + exp(- (X[, 1] < 1 + rnorm(n))))
W <- rbinom(n, 1, prob)
Y <- 60 + true_effect * W  + 5 * pmax(X[, 1], 1) + rnorm(n)

# naive estimates that does not take voluntary selection into account
mean(Y[ W == 1]) - mean(Y[ W == 0])

}
)

# plot(X[, 1], X[, 2], col = as.factor(W))

hist(naive.means, freq = F, main = "", col= rgb(0, 0, 1, 1/8), xlab = "ATT naive estimates", las = 1)
abline(v = true_effect, lwd = 3, lty = 2)
legend("topright", "True effect", lwd = 3, lty = 2, bty = "n")

paste("mean of naive estimates: ", round(mean(naive.means), 3))
paste("standard error of naive estimates: ", round(sd(naive.means), 3))
```



## Propensity score 

Previously we discussed the setting of a discrete feature in which case we estimate group-wise ATEs and use the weighted average to obtain an overall ATE estimate. When there are many features (covariates), this approach is prone to the *curse of dimensionality*.^[As the number of covariates increases the domain space shrinks quite rapidly making it infeasible to estimate ATE within the given domain due to thinning out data.] Moreover, if features are continuous, we won't be able to estimate ATE at each value of $x \in \chi$ due to lack of enough sample size. Instead of estimating group-wise ATE and averaging them, we would want to use a more indirect approach. This is when propensity score comes in. 

The implicit assumption is that we have collected enough features (discrete, continuous, interaction terms, higher degree polynomials) to back unconfoundedness. This again means that the treatment assignment is as good as random after controlling for $X_i$. More formally, this us back to equation \@ref(eq:uncon). But in actuality we are not interested in splitting groups to estimate group-wise treatment effects in the case when covariates are continuous and there are many characteristics determining the treatment assignment.   

**Propensity score: $e(x)$.** The probability of being treated given a set of covariates $X$s. 

\begin{equation}
e(x) = P(W_i = 1 | X_i = x) (#eq:prop)
\end{equation}

The key property of the propensity score is that it balances units in the treatment and control groups. If unconfoundedness assumption holds, we can write the following: 

\begin{equation}
W_i \perp \{Y_i(0), \; Y_i(1)\} | \; e(X_i) (\#eq:pconf) 
\end{equation}

What equation  \@ref(eq:pconf) says is that instead of controlling for $X$ one can control for the probability of treatment $(e(X))$ to establish the desired property that the treatment is as good as random. The propensity scores are mainly used for balancing purposes. 


One straight-forward implication of equation \@ref(eq:pconf) is that if we partition observations into groups with similar propensity score then we can estimate group-wise treatment effects and aggregate them to form an estimate for ATE. This can be done using the propensity score stratification method. The argument here is that when units with similar propensity scores are compared, the covariates are approximately balanced, mimicking a randomized experiment. 

## Estimation of propensity score

- Propensity scores can be estimated using various statistical or machine learning models. 

- We will first estimate propensity score using a logistic regression model, where the treatment assignment $W$ is regressed on the covariates $X$.

- Next, we will estimate propensity score using random forest model built within the GRF framework in Athey et al. 

**Logistic Regression** 

Using a linear regression framework to predict probabilities when the outcome is binary $\{0, \; 1\}$ falls short since the predicted values can go beyond 0 and 1. Many models contain 
values within the range of 0 and 1, which can be used to model a binary response. The logistic regression uses a logistic function given as: 


\begin{equation}
p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + .... \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + .... \beta_p X_p}} (\#eq:logit)  
\end{equation}

It is easy to see that $lim_{a \rightarrow - \inf}[\frac{e^a}{1+e^a}] = 0$ and $lim_{a \rightarrow  \inf}[\frac{e^a}{1+e^a}] = 1$. Equation \@ref{eq:logit} can be transformed using the *logit transformation* given as: 

\begin{equation}
g(X) = ln[\frac{p(X)}{1-p(X)}] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + .... \beta_p X_p
\end{equation}

We want to fit a logistic regression in order to predict the probability. For now, we will use simulated data. 


```{r}

# helper packages
library(dplyr)  # data wrangling 
library(ggplot2) # plots
library(rsample) # data splitting
library(tidyr) # for reshaping, pivot_wider

# Modeling package 
library(caret) # for logistic regression modeling 

# Model interpretability 
library(vip)


set.seed(194) # for replicability

# Generate simulated Data 

n <- 2000 # number of obsevations
p <- 10 # number of covariates
X <- matrix(rnorm(n * p), n, p) # data matrix
true_effect  <- 2.5

W <- rbinom(n, 1, 0.1 + 0.4 * (X[, 1] > 0) + 0.2 * (X[, 2] > 0))
prob  <- 0.1 + 0.4 * (X[, 1] > 0) + 0.2 * (X[, 2] > 0)  # oracle propensity score

Y <- true_effect * W + X[, 2] + pmax(X[, 1], 0) + rnorm(n)
#plot(X[, 1], X[, 2], col = as.factor(W))

dat  <- data.frame(cbind(W, Y, X))
colnames(dat)  <- c("W", "Y", paste0("X", seq(1, 10))) 
dat  <- dat  %>% 
        mutate(W = as.factor(W))

# create 70% training and 30% test data 
churn_split  <- initial_split(dat, prop = 0.7)
dat_train  <- training(churn_split)
dat_test  <- testing(churn_split)

# dimension of training and testing data
print(dim(dat_train))
print(dim(dat_test))


# let's compare two different models

# using X1 as the predictor
cv_model1  <- train(
     W ~ X1 + X2 + X3 + X4, 
    data = dat_train, 
    method = "glm", 
    family = "binomial", 
    trControl = trainControl(method = "cv", number = 10)
)

# misses out on X1
cv_model2  <- train(
     W ~ X2 + X3 + X4, 
    data = dat_train, 
    method = "glm", 
    family = "binomial", 
    trControl = trainControl(method = "cv", number = 10)
)


# print the sample performance measures 

sum_performance  <-  summary(
    resamples(
        list(
            model1  <- cv_model1, 
            model2  <- cv_model2
        )
    )
)

sum_performance$statistics$Accuracy

# use the confusion matrix 

# predict class 

threshold  <- 0.5
pred_prob  <- predict(cv_model1, dat_train, type = "prob")
pred_class_manual  <-  rep(0, 1400)
pred_class_manual[pred_prob[, 2] >= 0.5]  <- 1

pred_class  <- predict(cv_model1, dat_train)

# print the confusion matrix 
confusionMatrix(
    data = relevel(pred_class, ref = "1"), # predictions
    reference = relevel(dat_train$W, ref = "1") # reference or the true value
)

# if predict all yes still get an accuracy of 0.5936
table(dat_train$W)  %>% prop.table()

```

Looking at the confusion matrix, the values on the downward diagonal ([1, 1] and [2, 2] in matrix) are correctly idenfified by the model, while the upward diagonal values ([2, 1] and [1, 2]) are incorrectly classified. If all of the observations were assigned the value of 0, the accuracy would still be 0.5936%. This is termed as the *no information rate*. The model performs quite well in predicting True Negatives (classify as 0, when the value is actually 0). However, it does not perform so well in classifying the True Positives -- more than 50% of the positive cases are classified as negative. 


Next, two measures of importance are *sensitivity* and *specificity*. The sensitivity measure tracks the true positive rate from the model, while the specificity measure tracks the true negative rate.

- $sensitivity = \frac{True \; positives}{True \; positives + False \; negatives} = 0.8790$. 

- $specificity = \frac{True \; negatives}{True \; negatives \; + \; False \; positives} = \frac{604}{604 + 85} = 0.8766$. 

**How are the observations classified?**
A threshold value is used to transform the raw prediction of probabilities into classification such that $P(Y_{i} > p_{threshold})=1.$ The implicit $p_{threshold}$ used is 0.5. Varying the threshold from 0 to 1, one can calculate the relationship between the False Positive Rate (the prediction is positive when in actual the outcome is negative) and True Positive Rate at each threshold value. If the threshold value $(p_{threshold})$ is 1, then all observations are classified as 0, which means that the False Positive Rate is 0 but so is the True Positive Rate. Similarly, if the threshold is 0, then both True and False positive rates are 1. This gives the Receiver Operating Characteristic (ROC). 

```{r, fig.cap="ROC", fig.align = "center"}
library(ROCR)

# compute probabilities 
m1_prob  <- predict(cv_model1, dat_train, type = "prob")[, 2]
m2_prob  <- predict(cv_model2, dat_train, type = "prob")[, 2]

# AUC metrics 
perf1  <- prediction(m1_prob, dat_train$W)  %>% 
            performance(measure = "tpr", x.measure = "fpr")

perf2  <- prediction(m2_prob, dat_train$W)  %>% 
            performance(measure = "tpr", x.measure = "fpr")

# plot ROC curves
plot(perf1, col = "red")
plot(perf2, add = TRUE, col = "green") 
legend(0.8, 0.2, legend = c("cv_model1", "cv_model2"), lty = c(1,1),
            col = c("red", "green"), cex = 0.6)
```

The figure above plots the ROC for two models that we tested using cross-validation. The cv_model2 produces a diagonal line, which means that this model is as good as a random guess. Next, cv_model1 performs a whole lot better since a large gains in True positive rate can be achieved with a relatively small increase in False positive rate at the start. The ROC curve pertaining to cv_model1 helps pick a threshold to balance the sensitivity (True Positive Rate) and specificity (1 - False Positive Rate).  


The histogram of the estimated propensity scores using the logistic regression is as:  

```{r}
hist(pred_prob[, 2], main = "Histogram of P(W=1|X)
      \n using Logistic Regression", xlab = "probabilities")
```

Now, let's take a look at the confusion matrix using the test data. 

```{r}
pred_class_test  <- predict(cv_model1, dat_test)

# print the confusion matrix this time for the test sample 
confusionMatrix(
    data = relevel(pred_class_test, ref = "1"), # classification from the prediction
    reference = relevel(dat_test$W, ref = "1") # ground truth 
)
```

The measures of accuracy, sensitivity, and specificity are similar for both the training and testing sample.

## Using cross-fitting to predict propensity score

Here, we will be using 10-fold cross-folding to predict propensity score. 

```{r}

fun_probit_predict <- function(predictfold){
    # @Arg predictfold: number of the fold to avoid for model traning 
    # but used for prediction
    cv_model1  <- train(
                        W ~ X1 + X2 + X3 + X4, 
                        data = dat[-predictfold, ], 
                        method = "glm", 
                        family = "binomial", 
                        trControl = trainControl(method = "cv", number = 10)
                        )

    predict_logit  <- predict(cv_model1, dat[predictfold, ], type = "prob")
    return(predict_logit[, 2])
    
}


##############################
#
# cross-fitting
#
##############################

k  <- 10 # number of folds
len  <-  nrow(dat)

ind  <-  sample(1:len, replace = FALSE, size = len)
fold  <- cut(1:len, breaks = k, labels = FALSE) # create 10 folds

fold  <-  fold[ind] # randomly allocate the folds by ind

# container to store the predicted values

store  <- c()
true_index  <- c()

# do the cross-fitting and store
for(i in 1:k){
    # which(fold == i) is used as an index, if 8th observation receives the 1st fold for the first time, 
    # then the 1st prediction value corresponds to the 8th obs 
    store_new  <- fun_probit_predict(predictfold = which(fold == i)) 
    store_new  <- as.numeric(as.character(store_new))
    true_index_new  <- which(fold == i)
    store  <- c(store, store_new)
    true_index  <- c(true_index, true_index_new)
}

# create a dataframe with index that maps the predictions with the actual data
store  <-  data.frame(pscore = store, index = true_index)

# sort by index
store  <-  store[order(store[, 2]), ]

# propensity score
dat  <- dat  %>% 
            mutate(pscore = store$pscore)

# histogram of propensity score
hist(dat$pscore, main = "propensity score \n from cross-fitting")

```

## Propensity score stratification

Propensity scores are super important as they can be used in various different approaches to enchance the validity of causal inference in observational settings. These include but are not limited to inverse probability weighting, matching estimates, weight adjustments in regression (for better balancing), trimming, and propensity score stratification. These methods will be discussed in detail as we move on with the course. First, let's take a look at propensity score stratification to get a gist of how propensity scores contribute in comparing treatment units with control units. 

The simple idea is given by the cliché  that we want to compare oranges with oranges and not apples. To bring focus back into our context, it simply means that it is no good comparing a treated unit with an extremely high probability of receiving the treatment with a control unit with super low probability of receiving the treatment. But what if (yes, what if) we compare units with similar treatment probabilities? 

Let's run a quick thought experiment. We run the logistic regression and estimate the propensity score. Say, we have two units, each from the treatment and control group, with the propensity score of 0.6. The assumption here is, conditional on the similar propensity score, the treatment assignment is random. This follows from the unconfoundedness assumption: $Y_i^{0}, \; Y_i^{1} \; \perp \; W_i \; | X_i$.

Propensity score stratification divides the estimates of propensity scores into several segments and estimates the ATE within each segment. Finally, these segment-specific ATE estimates are averaged to obtain the overall estimate of ATE.

**Steps for ATE estimation using propensity score stratification**

1. Order observations according to their estimated propensity score. 

$\hat{e}(X)_{i1}, \; \hat{e}(X)_{i2}, ... \; \hat{e}(X)_{iN}$

2. Form $J$ strata of equal size and take the simple difference in mean between the treated and control units within each strata. These are $\hat{\tau}_j$ for $j = \{1, \; 2, \; ..., \; N\}$.  

3. Form the ATE, 

$\hat{\tau}_{Strat} = \frac{1}{J} \sum_{j = 1}^{J} \hat{\tau}_j$

Here, $\hat{\tau}_{Strat}$ is consistent for $\tau$, meaning that $\hat{\tau}_{Strat} \rightarrow_p \tau$ given that $\hat{e}(x)$ is consistent for $e(x)$ and the number of strata grows appropriately with $N$. However, one needs to set the number of strata, which can be a bit ad-hoc.  

Demo of propensity score stratification
```{r}

# order data by the propensity score: low to high
dat  <- dat[order(dat$pscore), ]

# cut to form ventiles
strata  <- cut(dat$pscore, breaks = quantile(dat$pscore, seq(0, 1, 0.05)), labels = 1:20, include.lowest = TRUE)

dat  <-  dat  %>%  
            mutate(strata = strata)

# compare across strata
dat_sum  <- dat  %>%  
                group_by(W, strata)  %>%  
                summarize(mean_Y = mean(Y))  %>%  
                pivot_wider(names_from = W, values_from = mean_Y)

colnames(dat_sum)  <-  c("strata", "mean_control", "mean_treat")
dat_sum  <- dat_sum  %>%  
                mutate(diff = mean_treat - mean_control)

print(paste("ATE Estimation from propensity score stratification is: ", mean(dat_sum$diff), sep = ""))

print(paste("raw difference is :", mean(dat$Y[dat$W == 1]) - mean(dat$Y[dat$W == 0]), sep = ""))

print(paste("And the true treatment effect is :", true_effect, sep = ""))
```

We see that the estimate from stratification gets closer to the true effect compared to the mean difference estimator. Looks like given that we know and observe what variables determine the treatment assignment, propensity score stratification approach performs well in estimating the ATE.



## Inverse Probability Weighting (IPW)

A more natural way to exploit the condition of unconfoundedness is to weight observations by their propensity score, which is known as the inverse probability weighting. As before $\hat{e}(x)$ is defined as an estimated propensity score. 

\begin{equation}
\hat{\tau}_{IPW} = \frac{1}{N}\sum_{i = 1}^{N} \Bigg(\frac{Y_i . W_i}{\hat{e}(X_i)} - \frac{Y_i . (1-W_i)}{1 - \hat{e}(X_i)}\Bigg) (\#eq:IPW)
\end{equation}

Intuitively, observations with high propensity score within the treated group are weighted down, while observations with higher propensity score in the control group are weighted more. In this way, propensity score is used to balance the differences in covariates across the treatment and control groups. Note that the validity of $\hat{\tau}$ still hinges on the unconfoundedness assumption. Any inference that you make is only good if your assumption holds.  

**Limitation of IPW Estimate.** One way to analyze the accuracy of $\hat{\tau}_{IPW}$ is to compare it with the oracle IPW estimate, $\hat{\tau}_{IPW}^{*}$. The oracle estimate is obtained from the known propensity score. Briefly, comparison between $\hat{\tau}_{IPW}^{*}$ and $\hat{\tau}_{AGG}$ suggests that the oracle IPW under-performs $\hat{\tau}_{AGG}$. In other words, the variance of the oracle estimate is larger than that of $\hat{\tau}_{AGG}$. 

Algorithmically, we can form *score* as:

$(\frac{Y_i \times W_i}{\hat{e}(X_i)} - \frac{Y_i \times (1-W_i)}{1 - \hat{e}(X_i)})$

The mean of it results to $\hat{\tau}$ and the standard error of the estimate is simply $\frac{\hat{\sigma}_{score}}{\sqrt{N}}$. 


**Estimating IPW.** In the example below we will simulate a dataset where the treatment assignment is made to be correlated with the outcome. This means that the independence assumption does not hold. However, since this is a simulated data, we know exactly what covariates influence the treatment assignment. Hence, we can invoke the unconfoundedness assumption. We estimate the propensity score using random forest based on *honest splitting*. For this, we use GRF package from \cite{athey2019}.

Note that $e(x)$ is estimated via cross-fitting.

1. The data is divided into $K$-folds. 

2. For each fold $k$, model building is administered using $-k$ folds. 

3. Using Step 2, predictions are generated for units in the $k^{th}$ fold.  

4. Steps 2 and 3 are repeated until all $K$ folds are exhausted.

**Estimation**

The following example uses 10 fold cross-fitting.

```{r}
#################################
# Author: VS
# Last Revised: Jan 16, 2024
# Keywords: IPW, AIPW, GRF
#
#
#
#################################


set.seed(194)

# Generate Data 
n <- 2000
p <- 10
true_effect  <-  15

X <- matrix(rnorm(n * p), n, p)
X.test <- matrix(0, 101, p)
X.test[, 1] <- seq(-2, 2, length.out = 101)
prob  <- 1 / (1 + exp(- (X[, 1] > 1 + rnorm(n))))
W <- rbinom(n, 1, prob)
Y <- 60 + true_effect * W + 5 * pmax(X[, 1], 0) + rnorm(n)
plot(X[, 1], X[, 2], col = as.factor(W))

#paste0("average treatment effect is: ", round(mean(pmax(X[, 1], 0)), 3))


#################################
#################################
#
# Inverse Probability Weighting
#
#################################
#################################

# use the random forest to get the propensity score 
dat  <- data.frame(W, X, Y)
n_features  <- length(setdiff(names(dat), "W"))

# A. ranger (probability tree)
rf1_ranger  <- ranger(
    W ~ ., 
    data = dat, 
    mtry = min(ceiling(sqrt(n_features) + 20), n_features),  
    num.trees = 2000, 
    probability =  TRUE
)
# OOB predictions from ranger
p.ranger  <- rf1_ranger$predictions[, 1]  

# B. probability tree using GRF

# cross-fitting index 
K  <- 10
ind  <- sample(1:length(W), replace = FALSE, size = length(W))
folds  <- cut(1:length(W), breaks = K, labels = FALSE)
index  <- matrix(0, nrow = length(ind) / K, ncol = K)
for(f in 1:K){
    index[, f]  <- ind[which(folds == f)]
}

# Build RF using GRF P(W = 1 | X)
fun.rf.grf  <- function(X, W, predictkfold){
    rf_grf  <- regression_forest(X, W, tune.parameters = "all")
    p.grf  <- predict(rf_grf, predictkfold)$predictions
    return(p.grf)
}

# storing 
predict.mat  <- matrix(0, nrow = nrow(index), ncol = K)
tauk  <- rep(0, K)
tauk_oracle  <- rep(0, K)
weighttau  <- rep(0, K)
score  <-  list()
score_oracle  <- list()

# for each fold i use other folds for estimation 
for(i in seq(1:K)){
    predict.mat[, i]  <- fun.rf.grf(X = X[c(index[, -i]), ], W = W[index[, -i]], predictkfold = X[c(index[, i]), ])
    # fold-specific treatment effect
    score[[i]]  <- ((W[index[, i]] * Y[index[, i]]) / (predict.mat[, i])) - 
                (((1 - W[index[, i]]) * Y[index[, i]]) / (1 - predict.mat[, i]))

    tauk[i]  <- mean(score[[i]])
}

# ipw using oracle propensity score and propensity score estimated from grf 
alpha  <-  0.05 # 5 percent level of significance
#ipw.ranger  <- mean(((W * Y) / (p.ranger)) - (((1 - W) * Y) / (1 - p.ranger))) 
ipw.grf  <- mean(unlist(score))
score_oracle  <- ((W * Y) / (prob)) - 
                ((1 - W) * Y / (1 - prob))
ipw.oracle  <- mean(score_oracle)

sd.ipw  <-  sd(unlist(score))
sd.oracle  <-  sd(score_oracle)
ll  <- ipw.grf - (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2)
ul  <- ipw.grf + (sd.ipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2)
ll_oracle  <- ipw.oracle - (sd.oracle / sqrt(length(score_oracle))) * qnorm(1 - alpha/2)
ul_oracle  <- ipw.oracle + (sd.oracle / sqrt(length(score_oracle))) * qnorm(1 - alpha/2)

result.ipw  <- c("IPW estimate" = round(ipw.grf, 3), "se" = round(sd.ipw / (sqrt(length(W))), 3),
                 "lower bound" = round(ll, 3), "upper bound" = round(ul, 3))

result.oracle.ipw  <- c("IPW Oracle estimate" = round(ipw.oracle, 3), "se" = round(sd.oracle / (sqrt(length(W))), 3),
                 "lower bound" = round(ll_oracle, 3), "upper bound" = round(ul_oracle, 3))                 

print(result.ipw)
print(result.oracle.ipw)
```

What? Despite having true propensity score, the Oracle IPW underperforms in accuracy compared to the IPW estimate with unknown propensity score. Why is it so?

## Comparing IPW with Aggregated Estimate 

We know that the probablity of the treatment increases with $X1$, as shown below. 

```{r}
plot(X[, 1], prob)
par(new = T)
abline(v = 0, col = "red", lty = "dashed")

```

Next, we would want to divide $X1$ into segments such that the probability of treatment remains more or less similar in each segment. Since we generated the data ourselves, we know that the probability of treatment increases for observations with $X1 > 0$. However, lets take 10 segments, which cuts the distribution of $X1$ in decile. 

```{r}
quant  <- quantile(X[, 1], p = seq(0, 1, 0.1))
group  <- cut(X[, 1], quant, include.lowest = TRUE)
dat$group  <- group
```

Next, we want to estimate the treatment effects for each segment and then aggregate it.

```{r}


dat_sum   <- data.frame(dat  %>% 
                mutate(status = ifelse(W == 1, "treatment", "control"))  %>% 
                group_by(group)  %>% 
                summarize(num_treat = sum(W), n_group = n()))  %>% 
                mutate(prop_treat = num_treat / n_group) %>%  
                dplyr::select(c(group, prop_treat))


dat   <- dat  %>%  
            merge(dat_sum, by = "group", all.x = T)  %>%  
            mutate(score = Y * ((W / prop_treat) - ((1 - W) / (1 - prop_treat))) 
            )


paste("aggregated treatment effect is: ", round(mean(dat$score), 3))
paste("se of aggregated treatment effect is: ", round(sd(dat$score) / sqrt(length(W)), 3))

```

Now, let's estimate the oracle IPW with known propensity score.

```{r}
score_oracle  <- ((W * Y) / (prob)) - 
                ((1 - W) * Y / (1 - prob))
ipw.oracle  <- mean(score_oracle)
se.oracle  <- sd(score_oracle) / sqrt(length(score_oracle))

paste("oracle IPW estimate: ", round(ipw.oracle, 3))
paste("standard error: ", round(se.oracle, 3))
```


## AIPW and Estimation

Augmented Inverse Probability Weighting (AIPW) provides a robust way to estimate ATE by alleviating the limitation of IPW estimate. 

Following the IPW approach, estimation of ATE is given in equation (6). The other approach to estimate $\tau$ is to think of it from the conditional response approach. Write $\mu_{w}(x) = E[Y_i| \; X_i = x, W_i = w]$. Then: 

$\tau(x) = E[Y_i| \; X_i = x, W_i = 1] - E[Y_i| \; X_i = x, W_i = 0]$


This is the regression outcome approach, where $\tau = E[\mu_{1}(x) - \mu_{0}(x)]$. The consistent estimator can be formed by using: $\hat{\tau}(x) = N^{-1} \sum_{i = 1}^{N} \mu_{1}(X_i) - \mu_{0}(X_i)$. 

AIPW approach combines both IPW approach as well as regression outcome approach to estimate $\tau$. 

$\hat{\tau}_{AIPW} = \frac{1}{N} \sum_{i = 1}^{N} (\mu_{1}(X_i) - \mu_{0}(X_i) + 
\frac{(Y_i - \hat{\mu}_1(X_i)). W_i}{\hat{e}(X_i)} - \frac{(Y_i - \hat{\mu}_0(X_i)). (1-W_i)}{1 - \hat{e}(X_i)})$


ML approach using cross-fitting is used to estimate both $\hat{e}(x)$ and $\hat{\mu}_{w}(x)$. Following the cross-fitting structure, we can formally write the estimate for $\tau$ as: 

$\hat{\tau}_{AIPW} = \lowerbracket{\frac{1}{N} \sum_{i = 1}^{N} (\mu_{1}^{-k(i)}(X_i) - \mu_{0}^{-k(i)}(X_i)}_{consistent \; estimate \; of \; \tau} + 
\frac{(Y_i - \hat{\mu}_1^{-k(i)}(X_i)). W_i}{\hat{e}^{-k(i)}(X_i)} - \frac{(Y_i - \hat{\mu}_0^{-k(i)}(X_i)). (1-W_i)}{1 - \hat{e}^{-k(i)}(X_i)})$

The AIPW approach can be thought of estimating ATE taking the difference across conditional responses. Next, the residuals are adjusted using weights  given by the propensity score. There are two attractive features of AIPW estimate. First, $\hat{\tau}_{AIPW} is consistent as long as $\hat{e}(x)$ or $\hat{\mu}_{w}(x)$ is consistent. This is because $E[(Y_i - \hat{\mu}_{W_i}(X_i)) \approx 0$. Second, $\hat{\tau}_{AIPW}$ is a good approximation to oracle $\hat{\tau}_{AIPW}^{*}$ as long as $\hat{\mu}(.)$ and $\hat{e}(.)$ are reasonably accurate. If one estimate is highly accurate, then it can compensate lack of accuracy on the other estimate. If both $\hat{\mu}(.)$ and $\hat{e}(.)$ are $\sqrt{n}$-consistent^[This means that $\hat{\mu}(.)$ converges to $\hat{\mu}$ at the ], then the following holds. 

$\sqrt{n}(\hat{\tau}_{AIPW} - \hat{\tau}_{AIPW}^{*}) \rightarrow_p 0$.

```{r}

#######################
#
# Augmented IPW (aipw)
#
#######################

#n_features2  <- length(setdiff(names(dat2), "Y"))
# ranger
#funrf_ranger  <- function(dat){
#    rf2  <- ranger(
#        Y ~ ., 
#        data = dat, 
#        mtry = min(ceiling(sqrt(n_features) + 20), n_features),  
#        respect.unordered.factors = "order", 
#        seed = 123, 
#        num.trees = 2000
#    )
#    return(rf2)
#}

# storing
predict.mat2a  <- matrix(0, nrow = nrow(index), ncol = K)
predict.mat2b  <- predict.mat2a
aipwK  <- rep(0, K)
weightaipK  <- rep(nrow(index) / length(index), K)

for(i in seq(1:K)){
    # E(Y | X, W = 1) using cross-fitting
    predict.mat2a[, i]  <- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], 
                                    predictkfold = cbind(X[c(index[, i]), ], 1))
    # E(Y | X, W = 0) using cross-fitting
    predict.mat2b[, i]  <- fun.rf.grf(X = cbind(X[c(index[, -i]), ], W[index[, -i]]), W = Y[index[, -i]], 
                                    predictkfold = cbind(X[c(index[, i]), ], 0))                                
    noise  <-  ((W[index[, i]] * (Y[index[, i]] - predict.mat2a[, i])) / (predict.mat[, i])) 
                - 
                (((1 - W[index[, i]]) * (Y[index[, i]] - predict.mat2b)) / (1 - predict.mat[, i]))
    score[[i]]  <-  predict.mat2a[, i] - predict.mat2b[, i] + noise
    aipwK[i]  <- mean(score[[i]])
}

aipw.grf  <- weighted.mean(aipwK, weights = weightaipK)
sd.aipw  <- sd(unlist(score)) 
ll  <-  aipw.grf - (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2)
ul  <-  aipw.grf + (sd.aipw / sqrt(length(unlist(score)))) * qnorm(1 - alpha/2)

result.aipw  <- c("AIPW Est." = round(aipw.grf, 3), "se" = round(sd.aipw/(sqrt(length(W))), 3), 
                    "lower bound" = round(ll, 3), "upper bound" = round(ul, 3))

######################
# grf
######################

# Train a causal forest 
tau.forest  <-  causal_forest(X, Y, W)

# Estimate the conditional average treatment effect on the full sample (CATE).
grf_ate  <- average_treatment_effect(tau.forest, target.sample = "all")
grf_att  <- average_treatment_effect(tau.forest, target.sample = "treated")


##  PRINT ALL

#print(paste0("average treatment effect is: ", round(mean(pmax(X[, 1], 0)), 3)))
print(paste("treatment effects according to naive estimator:", round(mean(Y[which(W == 1)]) - mean(Y[which( W == 0)]), 3), sep = " "))
print(paste("treatment effects according to IPW using", K, "fold cross-fittin:", round(ipw.grf, 3), sep = " "))
print(paste("treatment effects according to IPW oracle:", round(ipw.oracle, 3), sep = " "))
print(paste("treatment effects according to AIPW using", K, "fold cross-fitting:", round(aipw.grf, 3), sep = " "))
print(paste("treatment effects according to GRF:", round(grf_ate[[1]], 3), sep = " "))

print(result.ipw)
print(result.aipw)
print(grf_ate)


```

## Assessing Balance
```{r}

##########################################
#
#
# Assessing Balance
#
#
##########################################
XX  <- X[c(index), ]
YY  <- Y[c(index)]
WW  <- W[c(index)]
e.hat  <- c(predict.mat)

# unadjusted
means.treat  <- apply(XX[WW == 1, ], 2, mean)
means.control  <- apply(XX[WW == 0, ], 2, mean)
abs.mean.diff  <- abs(means.treat - means.control)

var.treat  <- apply(XX[WW == 1, ], 2, var)
var.control  <- apply(XX[WW == 0, ], 2, var)
std  <- sqrt(var.treat + var.control)

# adjusted

means.treat.adj  <- apply(XX*WW / e.hat, 2, mean)
means.control.adj  <- apply(XX*(1 - WW) / (1 - e.hat), 2, mean)
abs.mean.diff.adj  <- abs(means.treat.adj - means.control.adj)

var.treat.adj  <- apply(XX * WW / e.hat, 2, var)
var.control.adj  <- apply(XX * (1 - WW) / (1 - e.hat), 2, var)
std.adj  <- sqrt(var.treat.adj + var.control.adj)

# plot unadjusted and adjusted differences
par(oma=c(0,4,0,0))
plot(-2, xaxt="n", yaxt="n", xlab="", ylab="", xlim=c(-.01, 1.01), ylim=c(0, ncol(XX)+1), main="")
axis(side=1, at=c(-1, 0, 1), las=1)
lines(abs.mean.diff / std, seq(1, ncol(XX)), type="p", col="blue", pch=19)
lines(abs.mean.diff.adj / std.adj, seq(1, ncol(XX)), type="p", col="orange", pch=19)
legend("topright", c("Unadjusted", "Adjusted"), col=c("blue", "orange"), pch=19)
abline(v = seq(0, 1, by=.25), lty = 2, col = "grey", lwd=.5)
abline(h = 1:ncol(XX),  lty = 2, col = "grey", lwd=.5)
mtext(paste0("X", seq(1, ncol(XX))), side=2, cex=0.7, at=1:ncol(XX), padj=.4, adj=1, col="black", las=1, line=.3)
abline(v = 0)

hist(e.hat, breaks = 100, freq = FALSE)

```

## Cross-fitting

**What is cross-fitting?**

1. Divide the data into K folds randomly. 
2. Train the model using $-k$ folds (all folds except the $k^{th}$ one).
3. Generate a fit of *fold k* on the model trained using $-k$ folds
4. Repeat steps 2 and 3 to generate fit for all $K$ number of folds.   

This is illustrated using the figure below. The data is randomly divided into 5 folds (segments). This is an example of a five-fold cross-fitting. In the first round, the blue segments are used for model building, while responses are constructed for observations in the green segment of the data. Next, we move into the second round and so on; again the blue segments are used for model building and responses are constructed for the green segment. In this way, each observation is used for model building.


```{r}
# cross-fitting illustration
colorcode <- diag(5) # this creates a coding
colorcode <- c(colorcode)

# Create data for the boxes
boxes <- data.frame(
  x = rep(seq(2, 10, 2), 5),
  y = rep(seq(5, 1, by = -1), each = 5),
  label = rep(paste("fold", seq(1, 5), sep = " "), 5), 
  colorcode = colorcode
)

boxes <- boxes  %>% 
            mutate(fill = ifelse(colorcode == 1, "lightgreen", "lightblue"))  %>% 
            dplyr::select(-c(colorcode))
  


# Create the plot
ggplot() +
    geom_rect(data = boxes, aes(xmin = x , xmax = x + 2, ymin = y - 0.3, ymax = y + 0.5, fill = fill), 
            color = "black", alpha = 0.5) +   xlim(0, 14) +
    ylim(-1, 6) + 
    theme_void() +
    scale_fill_identity() +
    annotate("text", x = c(seq(3, 11, 2), rep(0.5, 5)), y = c(rep(0.3, 5), seq(5, 1, -1)), label = c(paste("fold", seq(1, 5, 1), sep = " "), paste("round", seq(1, 5, 1), sep = " ")), color = rep(c("red", "black"), each = 5)
    )
```

**What does it do?**

Simply put, cross-fitting assures that the same observations are not used for modeling building as well as 
to estimate the response (e.g., predictions). In this way, we would want to alleviate concerns of over-fitting. 


