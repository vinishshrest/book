{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb6c59a",
   "metadata": {},
   "source": [
    "Logistic regression \n",
    "\n",
    "**Course:** Causal Inference\n",
    "\n",
    "**Topic:** Logistic Regression\n",
    "\n",
    "Say, you need to predict the probability of someone being in a good vs. bad health (binary outcome)\n",
    "given a set of \n",
    "inputs: i) college education (yes or no); ii) income (high vs. low); iii) insured vs uninsured; \n",
    "and iv) stress level (continuous variable). \n",
    "\n",
    "For the sake of simplicity, we are going to assume a super simple DGP as follows: \n",
    "\n",
    "1. College education has a positive effect on health (coefficient = 0.1)\n",
    "2. High income has a positive effect on health (coefficient = 0.2)\n",
    "3. 40 percent more people from higher income households have college education\n",
    "4. Insurance has a positive effect on health (coefficient = 0.05)\n",
    "5. 60 percent more people from low income households are likely to be stressed\n",
    "6. Stress has a negative effect on health (coefficient = -1)\n",
    "\n",
    "**Note:** We know from the previous lecture that LPM estimates directly gives us the marginal\n",
    "effects -- the coefficients (at least theoretically) are interpreted as the effect of marginal changes \n",
    "in $X$s on $y$. LPM estimates are straight forward and easy to interpret as most often we are \n",
    "concerned with marginal effects from a policy standpoint.\n",
    "However, the coefficients pertaining to logistic regression are not marginal effects.\n",
    "This will be clearer as we proceed.\n",
    "\n",
    "We looked at the Linear Probability Model (LPM) in the previous lecture -- the estimates from a \n",
    "properly specified model were close to the true parameters. \n",
    "What if we have to estimate probability \n",
    "of someone being in good health? In this case, LPM does \n",
    "not gurantee that the probabilities are restricted between 0 and 1. \n",
    "\n",
    "Logistic regression is a tool that is used to model binary outcome and used for classification purposes. \n",
    "It uses a logistic function to restrict \n",
    "probabilities between values of 0 and 1. So how does it work?\n",
    "\n",
    "Essentially, the primary goal here is to predict probabilities of a binary event. \n",
    "The true probability is written as a function of $\\theta$ and $X$:\n",
    "$$\n",
    "p = h_\\theta(X) = \\sigma(\\theta X)\n",
    "$$\n",
    "\n",
    "$\\theta$ is a vector of true parameters -- they govern the DGP, and probabilites \n",
    "are the function of the true coefficients and inputs. \n",
    "Specifically, $\\sigma(.)$ is the logistic function, defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{(1 + exp(-z))} \n",
    "$$\n",
    "\n",
    "This $\\sigma(.)$ is also known as the sigmoid function and $z=\\theta X$ is often \n",
    "known as the logit. The logit is a \n",
    "linear combination of $\\theta$ and $X$. By nature of the logistic function, the output is restricted\n",
    "between 0 and 1.  \n",
    "To see the logistic function closely, let's take a look  at the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e77b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import necessary libraries\n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# generate numbers from -5 to 5\n",
    "z = np.linspace(-5, 5, 1000)\n",
    "print(z[0:20])\n",
    "# compute logistic values (note that these are probabilities)\n",
    "sigma_z = 1/(1 + np.exp(-z))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(z, sigma_z)\n",
    "plt.xlabel('z')\n",
    "plt.title('A sigmoid function')\n",
    "plt.ylabel('probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfa0c8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Note that the logistic function is S-shaped -- negative logit (z) values will have \n",
    "probabilities less than 0.5, \n",
    "whereas the positive z values will have \n",
    "probablities greater than 0.5. Also, probabilities on the vertical axis are \n",
    "constrained between 0 and 1, as they should be. \n",
    " \n",
    "The inputs in the sigmoid function is: $z=\\theta X$, which will help us attain probabilities. \n",
    "$X$ and $\\theta$ are the features (covariates) and the \n",
    "parameters of interest, respectively.\n",
    " \n",
    "Using these probability values, one can classify. For example:\n",
    "\n",
    "$y_i = 1$ if $\\hat{p_i}\\geq 0.5$ or else 0.    \n",
    "\n",
    "Our goal is to come up with the estimates of $\\theta$. After we have $\\hat{\\theta}$, we can obtain\n",
    "probabilities, perform classification based on them, or use probability estimates for downstream analysis.\n",
    "\n",
    "**The Loss Function**\n",
    "\n",
    "To do so, we will start with a loss function. Consider the following:\n",
    "\n",
    "$C = -\\log(\\hat{p_i})$ if $y_i=1$\n",
    "\n",
    "$C = -\\log(1-\\hat{p_i})$ if $y_i=0$\n",
    "\n",
    "Generally speaking, you want the model to come up with higher probabilities for observations with \n",
    "$y_i=1$ and lower probabilities for $y_i=0$. With this in mind, consider what might happen\n",
    "if $\\hat{p_i}$ is small vs large (say, 0.05 vs 0.95) when $y_i=1$. \n",
    "This will inflate the loss in the former case but reduce it in the latter. The case is \n",
    "reversed for $y_i=0$; higher probabilities will yield higher loss; whereas, lower probabilities \n",
    "will yield lower loss values. So, lower \n",
    "probabilities are 'shunned' for observations with $y_i=1$, and higher probabilities are penalized \n",
    "more for observations with $y=0$. \n",
    "\n",
    "We put this logic together and come up with the following cross-entropy loss function:\n",
    "\n",
    "$$\n",
    "C_{\\theta} = -\\frac{1}{n} \\sum_i^{n} [y_i \\times \\log(\\hat{p_i}) + (1-y_i) \\times \\log(1-\\hat{p_i})]\n",
    "$$\n",
    "\n",
    "Recall: \n",
    "$$\n",
    "p = \\sigma(\\theta X) = \\frac{1}{(1 + exp(-\\theta X))}\n",
    "$$ \n",
    "\n",
    "The *objective* is to get the estimates of $\\theta$ that minimizes the loss function. Turns out that the \n",
    "loss function above don't have an analytical or a closed form solution. However, the function is convex, which\n",
    "means that we can use gradient descent to estimate $\\theta$. \n",
    "\n",
    "\n",
    "**Using Gradient Descent**\n",
    "\n",
    "Let's first simulate the data following the DGP mentioned above. \n",
    "Note that the functional that's used to simulate the\n",
    "outcome variable (health) will depend on probability values obtained from the \n",
    "logistic function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c03ee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "\n",
    "# A. Simulate data \n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# 1. College education has a positive effect on health (coefficient = 0.1)\n",
    "# 2. High income has a positive effect on health (coefficient = 0.2)\n",
    "# 3. 40 percent more people from higher income households have college education\n",
    "# 4. Insurance has a positive effect on health (coefficient = 0.05)\n",
    "# 5. Stress has a negative effect on health (coefficient = -1)\n",
    "\n",
    "# number of obs\n",
    "n = 100000\n",
    "\n",
    "# 1. income \n",
    "income_log = np.random.lognormal(0, 1, n)\n",
    "income = income_log * 20000\n",
    "ln_income = np.log(income)\n",
    "# categorize high vs low income based on median income\n",
    "high_income = (income>=np.median(income)).astype('int')\n",
    "low_income = (income<np.median(income)).astype('int')\n",
    "\n",
    "# 2. college \n",
    "def gen_college(prob):\n",
    "    col = np.random.binomial(1, prob, 1)\n",
    "    return col\n",
    "\n",
    "college = []\n",
    "for i in range(n):\n",
    "    # 40% more people from high income group will have college degree\n",
    "    college_i = gen_college(0.2 + 0.4*high_income[i])\n",
    "    college.append(college_i)\n",
    "\n",
    "college = np.array(college).ravel()\n",
    "print(f\"mean of college: {college.mean()}\")\n",
    "\n",
    "print(f\"share college for high income: {np.mean(college[high_income == 1])}\")\n",
    "print(f\"share college for low income: {np.mean(college[high_income == 0])}\")\n",
    "\n",
    "# 3. Stress \n",
    "def gen_stress(prob):\n",
    "    p = np.random.binomial(1, prob, 1)\n",
    "    return p\n",
    "\n",
    "stress = []\n",
    "\n",
    "for i in range(n):\n",
    "    # 60% more people in low income will be stressed\n",
    "    stress_i = gen_stress(0.6*low_income[i])\n",
    "    stress.append(stress_i)\n",
    "\n",
    "# a continuous stress variable dependent on income status\n",
    "stress = np.array(stress).ravel() + np.random.normal(3, 1, n)*low_income + np.random.normal(0, 1, n)\n",
    "\n",
    "# histogram of the stress index\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(stress, bins=30, color=\"steelblue\", edgecolor=\"black\", alpha=0.3)\n",
    "plt.xlabel('stress index')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Histogram of stress index')\n",
    "plt.show()\n",
    "\n",
    "print(f\"average stress index for low income group: {stress[high_income==0].mean().round(4)}\")\n",
    "print(f\"average stress index for high income group: {stress[high_income==1].mean().round(4)}\")\n",
    "\n",
    "# 4. Insurance (exogeneous -- does not depend on other Xs)\n",
    "insurance = np.random.binomial(1, 0.3, n)\n",
    "print(f\"fraction insured: {insurance.mean()}\")\n",
    "\n",
    "# 5. health (Y variable)\n",
    "def gen_health(prob):\n",
    "    h = np.random.binomial(1, prob, 1) # these probabilities are going to come from the logistic function\n",
    "    return h\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# Logistic regression using the gradient descent \n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# define the logistic function\n",
    "def sigma(input):\n",
    "    logistic = 1/(1 + np.exp(-input)) \n",
    "    return logistic\n",
    "\n",
    "# true thetas governing the DGP\n",
    "theta_true = np.array([0.3, 0.1, 0.2, 0.05, -1])\n",
    "sigma(theta_true)\n",
    "X = np.concatenate((college.reshape((n, 1)), \n",
    "                    high_income.reshape((n, 1)), \n",
    "                    insurance.reshape((n, 1)), \n",
    "                    stress.reshape((n, 1))),\n",
    "                    axis=1)\n",
    "X = add_dummy_feature(X) # this adds 1 in the first column (intercept)\n",
    "z = X @ theta_true.reshape((5, 1))\n",
    "prob_logit = sigma(z)  # output true probabilities\n",
    "\n",
    "# NOTE: PROBABILITIES COME FROM THE LOGISTIC FUNCTION. THIS IS THE KEY TO SIMULATE LOGISTIC REGRESSION.\n",
    "# Step 1: Calculate linear combination (logit): z = X @ theta\n",
    "# Step 2: Transform to probabilities: p = sigma(z) = 1/(1 + exp(-z))\n",
    "# Step 3: Generate binary outcomes using these probabilities using a binomial dist.\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(prob_logit, bins=30, color=\"steelblue\", edgecolor=\"black\")\n",
    "plt.grid(True, alpha=0, linestyle='--')\n",
    "plt.title('True probabilities using true theta values')\n",
    "plt.show()\n",
    "\n",
    "# generate health using probabilities\n",
    "health = []\n",
    "for i in range(n):\n",
    "    health_i = gen_health(prob_logit[i]) # AGAIN, THESE PROBABILITIES COME FROM THE LOGISTIC FUNCTION\n",
    "    health.append(health_i)\n",
    "\n",
    "health = np.array(health).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7c5e6",
   "metadata": {},
   "source": [
    "Since, this is a simulation, we know the true probabilities generated using the \n",
    "true coefficients and the DGP. From a practitioner's standpoint, we won't know the \n",
    "true probabilities in non-experimental settings, since we don't know the true DGP to begin with. We have to estimate \n",
    "them.\n",
    "Let's print out our some summary measures on health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1265d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"y variable: {health} \\n\")\n",
    "#print(f\"X matrix: {X}\")\n",
    "print(f\"fraction with good health: {health.mean()}\")\n",
    "\n",
    "# create a stress band around the mean for no college, low income and uninsured\n",
    "mean_stress_baseline = stress[(college==0) & (high_income==0) & (insurance==0)].mean()\n",
    "stress_tolerance = 0.5  # within Â±0.5 of mean\n",
    "stress_band = (np.abs(stress - mean_stress_baseline) <= stress_tolerance)\n",
    "\n",
    "print(f\"fraction with good health among no school, low income, and uninsured: {np.mean(health[(college==0) & \n",
    "                                                                                               (high_income==0) & \n",
    "                                                                                               (insurance==0) & \n",
    "                                                                                               (stress_band)]).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a7767",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The true $\\theta$ values are $[\\theta_0=0.3, \\theta_1=0.1, \n",
    "\\theta_2=0.2, \\theta_3=0.05, \\theta_4=-1]$. \n",
    "\n",
    "a. 0.3 is the intercept coefficient, representing people in no college, \n",
    "low income, and uninsured group.\n",
    "\n",
    "b. 0.1 corresponds to college coefficient.\n",
    "\n",
    "c. 0.2 corresponds to high income coefficient.\n",
    "\n",
    "d. 0.05 corresponds to insurance coefficient.\n",
    "\n",
    "e. -1 corresponds to stress coefficient.\n",
    " \n",
    "Note that 3.61 percent of people who have no schooling, are of low income, are uninsured and around the mean stress \n",
    "index are in good health. This pertains to true $\\theta$ of 0.3. Let's convert this value \n",
    "into probability using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gh = 1/(1 + np.exp(-0.3 + np.mean(stress[(college==0) & (high_income==0) & (insurance==0)])))\n",
    "print(f\"the conversion of theta = 0.3 + mean stress value to prob: {p_gh.round(4)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04e7e7",
   "metadata": {},
   "source": [
    "Notice that according to the DGP, around 3.55 percent of people in the population with no college,\n",
    "low income, uninsured, and of the mean stress value are in good health. \n",
    "This is close to what we have in our sample. Hence,\n",
    "it is important to recognize that $\\theta$ values are coefficients and in the case of logistic \n",
    "regression; they are different from probabilities.\n",
    "\n",
    "**Gradient Descent**\n",
    "\n",
    "Let's move on to the gradient descent and its usage in estimating $\\theta$.\n",
    "\n",
    "Simply put, gradient is a vector of the partial derivatives of the loss function with respect to each \n",
    "$\\theta$ stacked together.\n",
    "\n",
    "$$\n",
    "\\text{gradient} = \\begin{bmatrix} \\frac{\\partial C}{\\partial \\theta_0} \\\\ \\frac{\\partial C}{\\partial \\theta_1} \\\\ \\frac{\\partial C}{\\partial \\theta_2} \\\\ \\frac{\\partial C}{\\partial \\theta_3} \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Before we get to the gradient of the logistic kind, let's stack the loss function using matrices:\n",
    "\n",
    "$$C_{\\theta} = -\\frac{1}{n} [Y^{t} \\log(p) + (1-Y^{t}) \\log(1- p)]$$\n",
    "\n",
    "Replacing $p= \\sigma(\\theta X)$, we have:  \n",
    "\n",
    "$$C_{\\theta} = -\\frac{1}{n} [Y^{t} \\log(\\sigma(X\\theta)) + (1-Y^{t}) \\log(1-\\sigma(X\\theta)]$$\n",
    "\n",
    "$C$ will be a scalar.\n",
    "Next, get $\\frac{\\partial C}{\\partial \\theta}$.\n",
    "\n",
    "But first, here are the dimensions of terms in the RHS:\n",
    "\n",
    "a. $X: (n\\times 5)$\n",
    "\n",
    "b. $Y^{T}: (1\\times n)$\n",
    "\n",
    "c. $\\theta : (5\\times 1)$\n",
    "\n",
    "d. $X\\theta : (n\\times 1)$\n",
    "\n",
    "e. $Y^{t} \\log(\\sigma(X\\theta)): scalar$\n",
    "\n",
    "Taking the partial derivative of the newly formatted cost function $C$ with \n",
    "respect to $\\theta$, you get the gradient vector\n",
    "as follows:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial \\theta} = \\frac{1}{n} X^{T}(\\sigma(X \\theta) - Y)$. \n",
    "\n",
    "where, $X^{T}$ is a $5\\times n$ matrix and $(X^{T}\\sigma(X \\theta) - Y)$ is a $n \\times 1$ matrix. \n",
    "I solved for the partial using \n",
    "the brute force chain rule. One thing to note while solving is a small trick below:\n",
    "\n",
    "$\\frac{exp(\\theta X)}{1 + exp(\\theta X)} =  \\frac{1 + exp(\\theta X) -1}{1 + exp(\\theta X)}$. This results to:\n",
    "$1 - \\frac{1}{1 + exp(\\theta X)} = 1 - \\sigma(\\theta X)$. \n",
    "\n",
    "This is getting into minute little details. You can escape this \n",
    "and just take the word for the gradient or you could try it all out. Upto you! \n",
    "\n",
    "Now that we are through with all this, the gradient descent algorithm is straight forward.\n",
    "\n",
    "**Gradient Descent Algorithm**\n",
    "\n",
    "1. Initialize the $\\theta_{gd}$ values. I've used values from the normal distribution. \n",
    "\n",
    "2. Initialize the learning rate -- $\\eta$ and the number of interations (epochs). We set $\\eta = 0.5$ and epochs=50.\n",
    "\n",
    "3. Compute the gradient. Call this $gd_i$.    \n",
    "\n",
    "4. Adjust $\\theta$ using the gradient and the learning rate as: $\\theta_{gd} = \\theta_{gd} - \\eta \\times gd_i$. Note that \n",
    "we have to move against the gradient; hence, the negative.\n",
    "\n",
    "5. Iterate steps 3 and 4 for $iter=epochs$ number of times or until the algorithm converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf749f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Gradient Descent \n",
    "theta_gd = np.random.normal(0, 1, 5)    # initial theta values from the normal dist.\n",
    "epsilon = 1e-15                         # to prevent overflow coming from logit values close to 0.\n",
    "epochs = 50                           # number of iterations\n",
    "eta = 0.5                               # learning rate\n",
    "loss = []\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    z = np.clip(X @ theta_gd.reshape((5, 1)), -500, 500)\n",
    "    gradient_i = ((sigma(z) - health.reshape((n, 1))).transpose() @ X) / n # caculate the gradient\n",
    "    theta_gd = theta_gd - eta*gradient_i                                   # adjust theta by moving opposite to the gradient\n",
    "    loss_i = np.mean(health*(-np.log(sigma(z)+epsilon).ravel()) +          # calculate loss\n",
    "                    (1-health)*(-np.log(1-sigma(z)+epsilon).ravel()))\n",
    "    loss.append(loss_i)                                                     # append loss\n",
    "\n",
    "print(f\"theta estimates from gradient descent: {theta_gd.round(4)} \\n \\n\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.linspace(1, epochs, epochs), np.array(loss).ravel())\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cross-entropy loss')\n",
    "plt.title(\"Loss with respect to iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8dd56b",
   "metadata": {},
   "source": [
    "We've now estimated the $\\theta$ using gradient descent. Let's verify our results using the \n",
    "in-built library in sklearn that estimates the Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fbf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare estimates from sklearn\n",
    "mod = LogisticRegression(max_iter=epochs, fit_intercept=False, C=np.inf)\n",
    "mod_fit = mod.fit(X, health)\n",
    "print(f\"Estimates from sklearn: {mod_fit.coef_}\")\n",
    "\n",
    "results = {\n",
    "            \"GD\": theta_gd.ravel().round(3),\n",
    "            \"SK\": mod_fit.coef_.ravel().round(3)\n",
    "}\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e46457",
   "metadata": {},
   "source": [
    "\n",
    "Let's estimate the model using the LPM -- note that this is a wrong functional form at use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "mod_linear = LinearRegression(fit_intercept=False) # we dont want to double fit the intercept; X already contains it \n",
    "mod_linear = mod_linear.fit(X, health)\n",
    "print(f\"Estimates from linear reg: {mod_linear.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9ac6b",
   "metadata": {},
   "source": [
    "The estimates from the gradient descent and sklearn are virtually similar. Note that the interpretation of $\\theta$ estimates \n",
    "are not equivalent to marginal effects as they are in the LPM set up. Recall, in the case of logistic regression: \n",
    "$\\hat{p} = \\frac{1}{(1 + exp(-\\theta X))}$. Hence, we need to translate $\\theta$ into marginal effects \n",
    "before comparing them with LPM's estimates. Calculation of marginal effect needs to be with respect to a given benchmark.\n",
    "There is nothing wrong with creating a benchmark.\n",
    "Say, we consider person A: with no college, low income, uninsured, and stress level around the mean (for the group with \n",
    "no college, uninsured, and low income) as this benchmark person and the marginal effects \n",
    "are computed with respect to this person. \n",
    " \n",
    "The following code translates $\\theta$ into marginal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290347c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def fun_me(theta_vals, person):\n",
    "    logit = (theta_vals @ person).ravel()\n",
    "    p = sigma(logit)\n",
    "    return p\n",
    "\n",
    "# create person A: without college, low income, and uninsured   \n",
    "# NOTE: This will be our benchmark person.\n",
    "person_A = np.array([1, 0, 0, 0, mean_stress_baseline]).reshape((5, 1))\n",
    "prob_health_A = fun_me(theta_gd, person_A)\n",
    "print(f\"The probability that person A is in good health is:{prob_health_A} \\n\")\n",
    "\n",
    "# Person B: with college but low income and uninsured\n",
    "person_B = np.array([1, 1, 0, 0, mean_stress_baseline]).reshape((5, 1))\n",
    "prob_health_B = fun_me(theta_gd, person_B)\n",
    "print(f\"The probability that person B is in good health is: {prob_health_B} \\n\")\n",
    "\n",
    "# Person C: with high income but without college and uninsured\n",
    "person_C = np.array([1, 0, 1, 0, mean_stress_baseline]).reshape((5, 1))\n",
    "prob_health_C = fun_me(theta_gd, person_C)\n",
    "print(prob_health_C)\n",
    "\n",
    "# Person D: with insurance but without college and low income\n",
    "person_D = np.array([1, 0, 0, 1, mean_stress_baseline]).reshape((5, 1))\n",
    "prob_health_D = fun_me(theta_gd, person_D)\n",
    "print(prob_health_D)\n",
    "\n",
    "# Person E: Same as Person A but one unit increase in stress for marginal effects\n",
    "person_E = np.array([1, 0, 0, 0, mean_stress_baseline + 1]).reshape((5, 1))\n",
    "prob_health_E = fun_me(theta_gd, person_E)\n",
    "print(prob_health_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b4454",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "Since we are using Person A as the benchmark, we can compute marginal probabilities simply by \n",
    "substracting probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923afd4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "me_B_A = prob_health_B - prob_health_A \n",
    "me_C_A = prob_health_C - prob_health_A \n",
    "me_D_A = prob_health_D - prob_health_A\n",
    "me_E_A = prob_health_E - prob_health_A\n",
    "\n",
    "me = np.array([prob_health_A, me_B_A, me_C_A, me_D_A, me_E_A])\n",
    "\n",
    "results_me_lpm = {\n",
    "                    \"ME:logistic\": me.ravel().round(3),\n",
    "                    \"ME:LPM\": mod_linear.coef_.ravel().round(3)\n",
    "}\n",
    "\n",
    "pd.DataFrame(results_me_lpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da36fa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The marginal effects from LPM and logistic regression are shown in the table. \n",
    "Let's take a look at predicted probabilities from both LPM and logistic regression models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6f836",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# prediction from LPM\n",
    "prob_linear = mod_linear.predict(X) # LPM directly gives probabilites\n",
    "print(prob_linear) \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(prob_linear, bins = 30, color=\"red\", edgecolor=\"black\", alpha=0)\n",
    "plt.xlabel('probability')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Predicted probabilities from LPM')\n",
    "plt.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# predictions from logistic\n",
    "prob_logit_predict = mod_fit.predict_proba(X) # need to call .predict_proba() to get probabilities from logistic model\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(prob_logit_predict[:,1], bins=30, color=\"steelblue\", edgecolor=\"black\", alpha=0.3)\n",
    "#plt.hist(prob_logit, bins=30, color=\"red\", edgecolor=\"black\", alpha=0.3)\n",
    "plt.xlabel('probability')\n",
    "plt.ylabel('frequency')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.title('Predicted probabilities from Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f581f47",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We see that the predicted probabilities from LPM are negative (this can't happen), whereas those from the logistic \n",
    "model closely mimic the actual probabilities. Hence, if the goal is to attain probabilities then logistic regression \n",
    "is clearly better than LPM.\n",
    "\n",
    "\n",
    "**LPM vs Logistic Regression**\n",
    "\n",
    "From a practitioner's perspective, one can get by using LPM if the goal is to infer causality alone and you \n",
    "aren't concerned about predicting probabilities. It is simple, easy to interpret, and will require low computational power. \n",
    "It does mean that you should, but you can get by. However, if the goal is to predict, say probability of the binary outcome, \n",
    "then LPM is a no-go. \n",
    "\n",
    "In the world of causality, the importance of estimating the probability of someone being treated (vs untreated) cannot be overstated. \n",
    "We know this as propensity scores. Logistic regression can be a good starting model while estimating propensity scores.  "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
