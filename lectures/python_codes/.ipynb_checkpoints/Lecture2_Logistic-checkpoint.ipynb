{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5fa217",
   "metadata": {},
   "source": [
    "Logistic regression \n",
    "\n",
    "**Course:** Causal Inference\n",
    "\n",
    "**Topic:** Logistic Regression\n",
    "\n",
    "Say, you need to predict the probability of someone being in a good vs. bad health given a set of \n",
    "inputs: i) college education (yes or no); ii) income (high vs. low); and iii) insured vs uninsured.\n",
    "\n",
    "For the sake of simplicity, we are going to assume a super simple DGP as follows: \n",
    "\n",
    "1. College education boosts health by 10 percent.\n",
    "2. Income boosts health by 20 percent.\n",
    "3. 40 percent more people from higher income households have college education.\n",
    "4. Having insurance boosts health by 5 percent.\n",
    "\n",
    "We can use the Linear Probability Model (LPM), as we did in the previous lecture -- the estimates from a \n",
    "properly specified model will be close to the true parameters. What if we have to estimate probability \n",
    "of someone being in good health? In this case, LPM does \n",
    "not gurantee that the probabilities are restricted between 0 and 1. \n",
    "\n",
    "Logistical regression is a tool that is used to model binary outcome. It uses a logistic function to restrict \n",
    "probabilities between values of 0 and 1. So how does it work?\n",
    "\n",
    "Let's first write probabilities as follows:\n",
    "$$\n",
    "p = h_\\theta(X) = \\sigma(\\theta X)\n",
    "$$\n",
    "\n",
    "here, $\\sigma(.)$ is the logistic function, defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{(1 + exp(-z))} \n",
    "$$\n",
    "\n",
    "This $\\sigma()$ is also known as the sigmoid function. To see this, let's take a look \n",
    "at the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import necessary libraries\n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# generate numbers from -5 to 5\n",
    "z = np.linspace(-5, 5, 1000)\n",
    "print(z)\n",
    "# compute logistic values (note that these are probabilities)\n",
    "sigma_z = 1/(1 + np.exp(-z))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(z, sigma_z)\n",
    "plt.xlabel('z')\n",
    "plt.title('A sigmoid function')\n",
    "plt.ylabel('probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2b775",
   "metadata": {},
   "source": [
    "Note that the graph is S-shaped -- negative z values will have probabilities less than 0, whereas the positive z values will have \n",
    "probablities greater than 0. The probabilities on the vertical axis are constrained between 0 and 1. \n",
    " \n",
    "The input of the sigmoid function is: $z=\\theta X$, which will help us attain probabilities. $X$ and $\\theta$ are the inputs and the \n",
    "parameters of interest, respectively.\n",
    " \n",
    "Using these probability values, one can classify. For example:\n",
    "\n",
    "$y_i = 1$ if $\\hat{p}\\geq 0.5$ or else 0.    \n",
    "\n",
    "Our goal is to come up with the estimates of $\\theta$.\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "To do so, we will start with a loss function. Consider the following:\n",
    "\n",
    "$C = -\\log(\\hat{p_i})$ if $y_i=1$\n",
    "\n",
    "$C = -\\log(1-\\hat{p_i})$ if $y_i=0$\n",
    "\n",
    "Generally speaking, you want the model to come up with higher probabilities for observations with \n",
    "$y_i=1$ and lower probabilities for $y_i=0$. With this in mind, consider what might happen\n",
    "if $\\hat{p}$ is small vs large (say, 0.05 vs 0.95) when $y=1$. \n",
    "This will inflate the loss in the former case but reduce it in the latter. The case is \n",
    "reversed for $y=0$; higher probabilities will yield higher loss, whereas lower probabilities \n",
    "will yield lower loss values. So, lower \n",
    "probabilities are 'shunned' for observations with $y=1$, and higher probabilities are penalized \n",
    "more for observations with $y=0$. \n",
    "\n",
    "We put this logic together and come up with the following loss function:\n",
    "\n",
    "$$\n",
    "C = -\\frac{1}{n} \\sum_i^{n} [y_i \\times \\log(\\hat{p_i}) + (1-y_i) \\times \\log(1-\\hat{p_i})]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc92afe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "\n",
    "# A. Simulate data \n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# The pseudo DGP takes the following form:\n",
    "\n",
    "\n",
    "# number of obs\n",
    "n = 100000\n",
    "\n",
    "\n",
    "# 1. income \n",
    "income_log = np.random.lognormal(0, 1, n)\n",
    "income = income_log * 20000\n",
    "ln_income = np.log(income)\n",
    "high_income = (income>=np.median(income)).astype('int')\n",
    "\n",
    "# 2. college \n",
    "def gen_college(prob):\n",
    "\n",
    "    col = np.random.binomial(1, prob, 1)\n",
    "    return col\n",
    "\n",
    "college = []\n",
    "for i in range(n):\n",
    "\n",
    "    college_i = gen_college(0.2 + 0.4*high_income[i])\n",
    "    college.append(college_i)\n",
    "\n",
    "college = np.array(college).ravel()\n",
    "print(f\"mean of college: {college.mean()}\")\n",
    "\n",
    "print(f\"share college for high income: {np.mean(college[high_income == 1])}\")\n",
    "print(f\"share college for low income: {np.mean(college[high_income == 0])}\")\n",
    "\n",
    "# 3. Insurance (exogeneous)\n",
    "insurance = np.random.binomial(1, 0.3, n)\n",
    "print(f\"fraction insured: {insurance.mean()}\")\n",
    "\n",
    "\n",
    "# 4. health\n",
    "def gen_health(prob):\n",
    "\n",
    "    h = np.random.binomial(1, prob, 1)\n",
    "    return h\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# Logistic regression using the gradient descent \n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# define the logistic function\n",
    "def sigma(input):\n",
    "    logistic = 1/(1 + np.exp(-input)) \n",
    "    return logistic\n",
    "\n",
    "# true thetas \n",
    "theta_true = np.array([0.3, 0.1, 0.2, 0.05])\n",
    "X = np.concatenate((college.reshape((n, 1)), \n",
    "                    high_income.reshape((n, 1)), \n",
    "                    insurance.reshape((n, 1))), \n",
    "                    axis=1)\n",
    "X = add_dummy_feature(X)\n",
    "z = X @ theta_true.reshape((4, 1))\n",
    "prob_logit = sigma(z)  # output probabilities\n",
    "\n",
    "# generate health using probablities\n",
    "health = []\n",
    "for i in range(n):\n",
    "    health_i = gen_health(prob_logit[i])\n",
    "    health.append(health_i)\n",
    "\n",
    "health = np.array(health).ravel()\n",
    "print(f\"fraction with good health: {health.mean()}\")\n",
    "print(f\"fraction with good health among no school, low income, and uninsured: {np.mean(health[((college==0) & (high_income==0) & (insurance==0))]).round(4)}\")\n",
    "\n",
    "# Gradient Descent \n",
    "theta_gd = np.random.normal(0, 1, 4)  # initial theta values\n",
    "epsilon = 1e-15                    # to prevent overflow\n",
    "epochs = 1000                       # number of iterations\n",
    "eta = 0.5                    # learning rate\n",
    "loss = []\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    z = np.clip(X @ theta_gd.reshape((4, 1)), -500, 500)\n",
    "    gradient_i = ((sigma(z) - health.reshape((n, 1))).transpose() @ X) / n \n",
    "    theta_gd = theta_gd - eta*gradient_i\n",
    "    loss_i = np.mean(health*(-np.log(sigma(z)+epsilon).ravel()) + \n",
    "                    (1-health)*(-np.log(1-sigma(z)+epsilon).ravel()))\n",
    "    loss.append(loss_i)\n",
    "\n",
    "# compare estimates from sklearn\n",
    "mod = LogisticRegression(max_iter=epochs, fit_intercept=False, penalty=None)\n",
    "mod_fit = mod.fit(X, health)\n",
    "print(f\"Estimates from sklearn: {mod_fit.coef_}\")\n",
    "\n",
    "# linear regression\n",
    "mod_linear = LinearRegression(fit_intercept=False)\n",
    "mod_linear = mod_linear.fit(X, health)\n",
    "print(f\"Estimates from linear reg: {mod_linear.coef_}\")\n",
    "\n",
    "results = {\n",
    "            \"Gradient Descent\": theta_gd.ravel(),\n",
    "            \"sklearn\": mod_fit.coef_.ravel(),\n",
    "            \"Linear Reg\": mod_linear.coef_.ravel()\n",
    "}\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
